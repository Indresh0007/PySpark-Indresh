{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH4/oRrVUadvPwBtekBZKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indresh0007/PySpark-Indresh/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtlchvLUL_kI",
        "outputId": "3ad16720-016f-4ea0-938d-68e1ce1ba36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.4.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# STEP 2: Download and Extract Spark 3.4.1 from reliable mirror\n",
        "!wget -q -O spark.tgz https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Remove any existing Spark folder and extract fresh\n",
        "!rm -rf spark-3.4.1-bin-hadoop3\n",
        "!tar -xvzf spark.tgz > /dev/null\n",
        "\n",
        "# STEP 3: Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# STEP 4: Install findspark and initialize\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "ZloXzCU6MCmD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/content/data\", exist_ok=True)\n",
        "os.makedirs(\"/content/data/parquet\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Vbzm8wtiNXPy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "RpoZzoliNZa0",
        "outputId": "cd128f6f-c06e-44ce-835c-bc92cf6c1be9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-934e9f4f-e3b5-4829-9202-8ed571e391be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-934e9f4f-e3b5-4829-9202-8ed571e391be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving customers.csv to customers.csv\n",
            "Saving employees.csv to employees.csv\n",
            "Saving offices.csv to offices.csv\n",
            "Saving orderdetails.csv to orderdetails.csv\n",
            "Saving orders.csv to orders.csv\n",
            "Saving payments.csv to payments.csv\n",
            "Saving productlines.csv to productlines.csv\n",
            "Saving products.csv to products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "for file in uploaded.keys():\n",
        "    shutil.move(file, f\"/content/data/{file}\")\n"
      ],
      "metadata": {
        "id": "90PeLIpdNjlV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrderDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class OrderDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Order Data Ingestion\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        // Example: customers.csv schema\n",
        "        StructType customersSchema = new StructType()\n",
        "            .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "            .add(\"customerName\", DataTypes.StringType)\n",
        "            .add(\"contactLastName\", DataTypes.StringType)\n",
        "            .add(\"contactFirstName\", DataTypes.StringType)\n",
        "            .add(\"phone\", DataTypes.StringType)\n",
        "            .add(\"addressLine1\", DataTypes.StringType)\n",
        "            .add(\"addressLine2\", DataTypes.StringType)\n",
        "            .add(\"city\", DataTypes.StringType)\n",
        "            .add(\"state\", DataTypes.StringType)\n",
        "            .add(\"postalCode\", DataTypes.StringType)\n",
        "            .add(\"country\", DataTypes.StringType)\n",
        "            .add(\"salesRepEmployeeNumber\", DataTypes.IntegerType)\n",
        "            .add(\"creditLimit\", DataTypes.DoubleType);\n",
        "\n",
        "        Dataset<Row> customersDF = spark.read()\n",
        "            .option(\"header\", true)\n",
        "            .schema(customersSchema)\n",
        "            .csv(\"data/customers.csv\");\n",
        "\n",
        "        customersDF.write()\n",
        "            .mode(\"overwrite\")\n",
        "            .parquet(\"data/parquet/customers.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Customers Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPq72WuvMTPk",
        "outputId": "576b64a5-8409-48b1-e68a-1d81b1f2f3c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting OrderDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcaWp6SM-xc",
        "outputId": "eb76634b-8f30-402b-9c5b-41d9f298b0b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:09:18 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:09:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:09:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:09:19 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:09:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:09:19 INFO SparkContext: Submitted application: Order Data Ingestion\n",
            "25/08/06 07:09:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:09:19 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:09:19 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:09:19 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:09:19 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:09:19 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:09:19 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:09:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:09:19 INFO Utils: Successfully started service 'sparkDriver' on port 43587.\n",
            "25/08/06 07:09:19 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:09:19 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:09:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:09:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:09:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:09:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e701911a-ba34-48a9-9297-a32aeb665d5a\n",
            "25/08/06 07:09:19 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:09:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:09:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:09:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:09:20 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:09:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:09:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34519.\n",
            "25/08/06 07:09:20 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:34519\n",
            "25/08/06 07:09:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:09:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 34519, None)\n",
            "25/08/06 07:09:20 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:34519 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 34519, None)\n",
            "25/08/06 07:09:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 34519, None)\n",
            "25/08/06 07:09:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 34519, None)\n",
            "25/08/06 07:09:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:09:21 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:09:22 INFO InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:09:26 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:09:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:09:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:09:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:09:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:09:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:09:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:09:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:09:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:09:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:09:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:09:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:34519 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:09:28 INFO SparkContext: Created broadcast 0 from parquet at OrderDataIngestion.java:34\n",
            "25/08/06 07:09:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:09:28 INFO SparkContext: Starting job: parquet at OrderDataIngestion.java:34\n",
            "25/08/06 07:09:28 INFO DAGScheduler: Got job 0 (parquet at OrderDataIngestion.java:34) with 1 output partitions\n",
            "25/08/06 07:09:28 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at OrderDataIngestion.java:34)\n",
            "25/08/06 07:09:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:09:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:09:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at OrderDataIngestion.java:34), which has no missing parents\n",
            "25/08/06 07:09:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 213.1 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:09:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 77.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:09:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:34519 (size: 77.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:09:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:09:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at OrderDataIngestion.java:34) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:09:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:09:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:09:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:09:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:09:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:09:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:09:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:09:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:09:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:09:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:09:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:09:29 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:09:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional int32 salesRepEmployeeNumber;\n",
            "  optional double creditLimit;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:09:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:09:30 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:09:31 INFO CodeGenerator: Code generated in 595.508545 ms\n",
            "25/08/06 07:09:32 INFO FileOutputCommitter: Saved output of task 'attempt_20250806070928945499350408850681_0000_m_000000_0' to file:/content/data/parquet/customers.parquet/_temporary/0/task_20250806070928945499350408850681_0000_m_000000\n",
            "25/08/06 07:09:32 INFO SparkHadoopMapRedUtil: attempt_20250806070928945499350408850681_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:09:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2545 bytes result sent to driver\n",
            "25/08/06 07:09:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2794 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:09:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:09:32 INFO DAGScheduler: ResultStage 0 (parquet at OrderDataIngestion.java:34) finished in 3.575 s\n",
            "25/08/06 07:09:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:09:32 INFO DAGScheduler: Job 0 finished: parquet at OrderDataIngestion.java:34, took 3.706771 s\n",
            "25/08/06 07:09:32 INFO FileFormatWriter: Start to commit write Job 2f82b09a-5f8d-49c2-a168-02033baa67b6.\n",
            "25/08/06 07:09:32 INFO FileFormatWriter: Write Job 2f82b09a-5f8d-49c2-a168-02033baa67b6 committed. Elapsed time: 38 ms.\n",
            "25/08/06 07:09:32 INFO FileFormatWriter: Finished processing stats for write job 2f82b09a-5f8d-49c2-a168-02033baa67b6.\n",
            "✅ Customers Parquet File Created Successfully.\n",
            "25/08/06 07:09:32 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:09:32 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:09:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:09:32 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:09:32 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:09:32 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:09:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:09:32 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:09:32 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:09:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-96df44f7-0579-48ac-bf4e-b69203a19bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrdersDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OrdersDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Orders Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> ordersDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/orders.csv\");\n",
        "\n",
        "        ordersDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/orders.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Orders Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikRpI4qINH8l",
        "outputId": "366326d4-a99a-4920-d1b4-c4c34f3f53d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OrdersDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" OrdersDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" OrdersDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhYcScAfN70p",
        "outputId": "f37dd9ae-4079-4c04-d67e-690c0d363947"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:10:52 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:10:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:10:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:10:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:10:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:10:53 INFO SparkContext: Submitted application: Orders Data Ingestion\n",
            "25/08/06 07:10:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:10:53 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:10:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:10:53 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:10:53 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:10:53 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:10:53 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:10:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:10:54 INFO Utils: Successfully started service 'sparkDriver' on port 38931.\n",
            "25/08/06 07:10:54 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:10:54 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:10:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:10:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:10:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:10:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9555241f-9bc2-428a-9188-6ccf97ba970f\n",
            "25/08/06 07:10:54 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:10:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:10:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:10:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:10:55 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:10:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:10:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38845.\n",
            "25/08/06 07:10:55 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:38845\n",
            "25/08/06 07:10:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:10:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 38845, None)\n",
            "25/08/06 07:10:55 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:38845 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 38845, None)\n",
            "25/08/06 07:10:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 38845, None)\n",
            "25/08/06 07:10:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 38845, None)\n",
            "25/08/06 07:10:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:10:55 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:10:57 INFO InMemoryFileIndex: It took 82 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:10:57 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:11:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:11:01 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:11:02 INFO CodeGenerator: Code generated in 461.283922 ms\n",
            "25/08/06 07:11:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:11:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:11:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:38845 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:11:02 INFO SparkContext: Created broadcast 0 from csv at OrdersDataIngestion.java:13\n",
            "25/08/06 07:11:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:11:02 INFO SparkContext: Starting job: csv at OrdersDataIngestion.java:13\n",
            "25/08/06 07:11:02 INFO DAGScheduler: Got job 0 (csv at OrdersDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:11:02 INFO DAGScheduler: Final stage: ResultStage 0 (csv at OrdersDataIngestion.java:13)\n",
            "25/08/06 07:11:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:11:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:11:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at OrdersDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:11:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:11:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:11:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:38845 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:11:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:11:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at OrdersDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:11:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:11:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) \n",
            "25/08/06 07:11:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:11:03 INFO FileScanRDD: Reading File path: file:///content/data/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 07:11:03 INFO CodeGenerator: Code generated in 61.01543 ms\n",
            "25/08/06 07:11:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1679 bytes result sent to driver\n",
            "25/08/06 07:11:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 817 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:11:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:11:04 INFO DAGScheduler: ResultStage 0 (csv at OrdersDataIngestion.java:13) finished in 1.301 s\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:11:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Job 0 finished: csv at OrdersDataIngestion.java:13, took 1.472340 s\n",
            "25/08/06 07:11:04 INFO CodeGenerator: Code generated in 49.642888 ms\n",
            "25/08/06 07:11:04 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:11:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:11:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:11:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:11:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:38845 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:11:04 INFO SparkContext: Created broadcast 2 from csv at OrdersDataIngestion.java:13\n",
            "25/08/06 07:11:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:11:04 INFO SparkContext: Starting job: csv at OrdersDataIngestion.java:13\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Got job 1 (csv at OrdersDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Final stage: ResultStage 1 (csv at OrdersDataIngestion.java:13)\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at OrdersDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:11:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:11:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:11:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:38845 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:11:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:11:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at OrdersDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:11:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:11:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) \n",
            "25/08/06 07:11:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:11:05 INFO FileScanRDD: Reading File path: file:///content/data/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 07:11:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver\n",
            "25/08/06 07:11:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 340 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:11:05 INFO DAGScheduler: ResultStage 1 (csv at OrdersDataIngestion.java:13) finished in 0.490 s\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:11:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:11:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Job 1 finished: csv at OrdersDataIngestion.java:13, took 0.508645 s\n",
            "25/08/06 07:11:05 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:11:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:11:05 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:11:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:11:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:11:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:11:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:11:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:11:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:11:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:11:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:11:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:38845 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:11:05 INFO SparkContext: Created broadcast 4 from parquet at OrdersDataIngestion.java:17\n",
            "25/08/06 07:11:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:11:05 INFO SparkContext: Starting job: parquet at OrdersDataIngestion.java:17\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Got job 2 (parquet at OrdersDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrdersDataIngestion.java:17)\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:11:05 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at OrdersDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:11:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.6 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:11:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:11:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:38845 (size: 76.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:11:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:11:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at OrdersDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:11:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:11:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) \n",
            "25/08/06 07:11:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:11:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:11:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:11:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:11:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:11:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:11:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:11:06 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:11:06 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:11:06 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:11:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"requiredDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"shippedDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"status\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"comments\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional int32 orderDate (DATE);\n",
            "  optional int32 requiredDate (DATE);\n",
            "  optional int32 shippedDate (DATE);\n",
            "  optional binary status (STRING);\n",
            "  optional binary comments (STRING);\n",
            "  optional int32 customerNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:11:06 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:11:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:38845 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:11:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:38845 in memory (size: 34.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:11:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:38845 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:11:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:38845 in memory (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:11:06 INFO FileScanRDD: Reading File path: file:///content/data/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 07:11:06 INFO CodeGenerator: Code generated in 58.117876 ms\n",
            "25/08/06 07:11:07 INFO FileOutputCommitter: Saved output of task 'attempt_20250806071105762574849540884155_0002_m_000000_2' to file:/content/data/parquet/orders.parquet/_temporary/0/task_20250806071105762574849540884155_0002_m_000000\n",
            "25/08/06 07:11:07 INFO SparkHadoopMapRedUtil: attempt_20250806071105762574849540884155_0002_m_000000_2: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:11:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:11:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1754 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:11:07 INFO DAGScheduler: ResultStage 2 (parquet at OrdersDataIngestion.java:17) finished in 1.824 s\n",
            "25/08/06 07:11:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:11:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:11:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:11:07 INFO DAGScheduler: Job 2 finished: parquet at OrdersDataIngestion.java:17, took 1.837981 s\n",
            "25/08/06 07:11:07 INFO FileFormatWriter: Start to commit write Job 18e9aeb1-3d37-4372-9dc1-a4f7dc524f82.\n",
            "25/08/06 07:11:07 INFO FileFormatWriter: Write Job 18e9aeb1-3d37-4372-9dc1-a4f7dc524f82 committed. Elapsed time: 32 ms.\n",
            "25/08/06 07:11:07 INFO FileFormatWriter: Finished processing stats for write job 18e9aeb1-3d37-4372-9dc1-a4f7dc524f82.\n",
            "✅ Orders Parquet File Created Successfully.\n",
            "25/08/06 07:11:07 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:11:07 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:11:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:11:07 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:11:07 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:11:07 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:11:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:11:07 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:11:07 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:11:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-136273d6-97c8-41e7-afad-def42555d44c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrderDetailsDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OrderDetailsDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Order Details Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orderDetailsDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/orderdetails.csv\");\n",
        "\n",
        "        orderDetailsDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Order Details Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6BzU_2uN9ec",
        "outputId": "881903c4-c1c1-4baa-a5af-adbcdcc44b24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OrderDetailsDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDetailsDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDetailsDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dstwB8f-ORGp",
        "outputId": "c70c0efb-aac5-4131-b4a2-23cbfcf52094"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:12:20 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:12:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:12:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:12:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:12:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:12:21 INFO SparkContext: Submitted application: Order Details Data Ingestion\n",
            "25/08/06 07:12:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:12:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:12:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:12:21 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:12:21 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:12:21 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:12:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:12:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:12:21 INFO Utils: Successfully started service 'sparkDriver' on port 35703.\n",
            "25/08/06 07:12:22 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:12:22 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:12:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:12:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:12:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:12:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3af6119b-1a64-4f26-af0f-4e470c62ca59\n",
            "25/08/06 07:12:22 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:12:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:12:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:12:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:12:23 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:12:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:12:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41687.\n",
            "25/08/06 07:12:23 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:41687\n",
            "25/08/06 07:12:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:12:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 41687, None)\n",
            "25/08/06 07:12:23 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:41687 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 41687, None)\n",
            "25/08/06 07:12:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 41687, None)\n",
            "25/08/06 07:12:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 41687, None)\n",
            "25/08/06 07:12:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:12:23 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:12:25 INFO InMemoryFileIndex: It took 123 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:12:26 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:12:31 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:12:31 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:12:32 INFO CodeGenerator: Code generated in 436.746468 ms\n",
            "25/08/06 07:12:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:12:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:12:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:41687 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:12:33 INFO SparkContext: Created broadcast 0 from csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:12:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:12:33 INFO SparkContext: Starting job: csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Got job 0 (csv at OrderDetailsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Final stage: ResultStage 0 (csv at OrderDetailsDataIngestion.java:13)\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at OrderDetailsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:12:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:12:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:12:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:41687 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:12:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:12:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at OrderDetailsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:12:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:12:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:12:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:12:34 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:12:34 INFO CodeGenerator: Code generated in 29.747778 ms\n",
            "25/08/06 07:12:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver\n",
            "25/08/06 07:12:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 526 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:12:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:12:34 INFO DAGScheduler: ResultStage 0 (csv at OrderDetailsDataIngestion.java:13) finished in 0.857 s\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:12:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Job 0 finished: csv at OrderDetailsDataIngestion.java:13, took 0.985097 s\n",
            "25/08/06 07:12:34 INFO CodeGenerator: Code generated in 41.123984 ms\n",
            "25/08/06 07:12:34 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:12:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:12:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:12:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:12:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:41687 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:34 INFO SparkContext: Created broadcast 2 from csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:12:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:12:34 INFO SparkContext: Starting job: csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Got job 1 (csv at OrderDetailsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Final stage: ResultStage 1 (csv at OrderDetailsDataIngestion.java:13)\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at OrderDetailsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:12:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:12:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:12:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:41687 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:12:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at OrderDetailsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:12:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:12:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:12:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:12:35 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:12:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1676 bytes result sent to driver\n",
            "25/08/06 07:12:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 365 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:12:35 INFO DAGScheduler: ResultStage 1 (csv at OrderDetailsDataIngestion.java:13) finished in 0.501 s\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:12:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:12:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Job 1 finished: csv at OrderDetailsDataIngestion.java:13, took 0.516891 s\n",
            "25/08/06 07:12:35 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:12:35 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:12:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:12:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:12:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:12:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:12:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:12:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:12:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:12:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:12:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:12:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:41687 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:35 INFO SparkContext: Created broadcast 4 from parquet at OrderDetailsDataIngestion.java:17\n",
            "25/08/06 07:12:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:12:35 INFO SparkContext: Starting job: parquet at OrderDetailsDataIngestion.java:17\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Got job 2 (parquet at OrderDetailsDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrderDetailsDataIngestion.java:17)\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at OrderDetailsDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:12:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:41687 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:12:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:12:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:41687 (size: 76.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:12:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:12:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at OrderDetailsDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:12:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:12:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:12:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:12:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:41687 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:12:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:41687 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:41687 in memory (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:12:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:12:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:12:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:12:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:12:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:12:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:12:36 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:12:36 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:12:36 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:12:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "  optional double priceEach;\n",
            "  optional int32 orderLineNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:12:36 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:12:36 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:12:36 INFO CodeGenerator: Code generated in 39.978975 ms\n",
            "25/08/06 07:12:37 INFO FileOutputCommitter: Saved output of task 'attempt_202508060712354578939859661488504_0002_m_000000_2' to file:/content/data/parquet/orderdetails.parquet/_temporary/0/task_202508060712354578939859661488504_0002_m_000000\n",
            "25/08/06 07:12:37 INFO SparkHadoopMapRedUtil: attempt_202508060712354578939859661488504_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:12:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:12:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1665 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:12:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:12:37 INFO DAGScheduler: ResultStage 2 (parquet at OrderDetailsDataIngestion.java:17) finished in 1.773 s\n",
            "25/08/06 07:12:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:12:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:12:37 INFO DAGScheduler: Job 2 finished: parquet at OrderDetailsDataIngestion.java:17, took 1.848832 s\n",
            "25/08/06 07:12:37 INFO FileFormatWriter: Start to commit write Job bed6a56e-79f2-4826-829b-331d4e3e9233.\n",
            "25/08/06 07:12:37 INFO FileFormatWriter: Write Job bed6a56e-79f2-4826-829b-331d4e3e9233 committed. Elapsed time: 33 ms.\n",
            "25/08/06 07:12:37 INFO FileFormatWriter: Finished processing stats for write job bed6a56e-79f2-4826-829b-331d4e3e9233.\n",
            "✅ Order Details Parquet File Created Successfully.\n",
            "25/08/06 07:12:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:12:37 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:12:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:12:37 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:12:37 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:12:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:12:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:12:37 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:12:37 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:12:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b5f40f8-cde2-4f2d-87bd-92ac616ec343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EmployeesDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class EmployeesDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Employees Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> employeesDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/employees.csv\");\n",
        "\n",
        "        employeesDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/employees.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Employees Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16dHi3PyOS0g",
        "outputId": "81eb7fdc-45dd-4298-821f-26b9d5b79160"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EmployeesDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" EmployeesDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" EmployeesDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S9B_Q_-Oed9",
        "outputId": "1a2720b4-88e9-4836-c26d-c54a8f75c209"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:13:19 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:13:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:13:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:13:19 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:13:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:13:19 INFO SparkContext: Submitted application: Employees Data Ingestion\n",
            "25/08/06 07:13:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:13:19 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:13:19 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:13:19 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:13:19 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:13:19 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:13:19 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:13:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:13:20 INFO Utils: Successfully started service 'sparkDriver' on port 38377.\n",
            "25/08/06 07:13:20 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:13:20 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:13:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:13:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:13:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:13:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-04babcfd-076a-4878-a630-e45025521257\n",
            "25/08/06 07:13:20 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:13:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:13:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:13:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:13:21 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:13:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:13:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42551.\n",
            "25/08/06 07:13:21 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:42551\n",
            "25/08/06 07:13:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:13:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 42551, None)\n",
            "25/08/06 07:13:21 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:42551 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 42551, None)\n",
            "25/08/06 07:13:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 42551, None)\n",
            "25/08/06 07:13:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 42551, None)\n",
            "25/08/06 07:13:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:13:21 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:13:23 INFO InMemoryFileIndex: It took 96 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:13:23 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:13:27 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:13:27 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:13:28 INFO CodeGenerator: Code generated in 471.956435 ms\n",
            "25/08/06 07:13:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:13:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:13:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:42551 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:13:29 INFO SparkContext: Created broadcast 0 from csv at EmployeesDataIngestion.java:13\n",
            "25/08/06 07:13:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:13:29 INFO SparkContext: Starting job: csv at EmployeesDataIngestion.java:13\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Got job 0 (csv at EmployeesDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Final stage: ResultStage 0 (csv at EmployeesDataIngestion.java:13)\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at EmployeesDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:13:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:13:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:13:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:42551 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:13:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:13:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at EmployeesDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:13:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:13:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:13:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:13:30 INFO FileScanRDD: Reading File path: file:///content/data/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 07:13:30 INFO CodeGenerator: Code generated in 49.990987 ms\n",
            "25/08/06 07:13:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1689 bytes result sent to driver\n",
            "25/08/06 07:13:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 627 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:13:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:13:30 INFO DAGScheduler: ResultStage 0 (csv at EmployeesDataIngestion.java:13) finished in 1.079 s\n",
            "25/08/06 07:13:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:13:30 INFO DAGScheduler: Job 0 finished: csv at EmployeesDataIngestion.java:13, took 1.306845 s\n",
            "25/08/06 07:13:30 INFO CodeGenerator: Code generated in 53.041893 ms\n",
            "25/08/06 07:13:31 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:13:31 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:13:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:13:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:13:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:42551 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:13:31 INFO SparkContext: Created broadcast 2 from csv at EmployeesDataIngestion.java:13\n",
            "25/08/06 07:13:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:13:31 INFO SparkContext: Starting job: csv at EmployeesDataIngestion.java:13\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Got job 1 (csv at EmployeesDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Final stage: ResultStage 1 (csv at EmployeesDataIngestion.java:13)\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at EmployeesDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:13:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:13:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:13:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:42551 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:13:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at EmployeesDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:13:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:13:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:13:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:13:31 INFO FileScanRDD: Reading File path: file:///content/data/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 07:13:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1694 bytes result sent to driver\n",
            "25/08/06 07:13:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 229 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:13:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:13:31 INFO DAGScheduler: ResultStage 1 (csv at EmployeesDataIngestion.java:13) finished in 0.374 s\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:13:31 INFO DAGScheduler: Job 1 finished: csv at EmployeesDataIngestion.java:13, took 0.387387 s\n",
            "25/08/06 07:13:31 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:13:31 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:13:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:13:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:13:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:13:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:13:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:13:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:13:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:13:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:42551 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:13:32 INFO SparkContext: Created broadcast 4 from parquet at EmployeesDataIngestion.java:17\n",
            "25/08/06 07:13:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:13:32 INFO SparkContext: Starting job: parquet at EmployeesDataIngestion.java:17\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Got job 2 (parquet at EmployeesDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at EmployeesDataIngestion.java:17)\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at EmployeesDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:13:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.9 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:13:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:42551 (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:13:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:13:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at EmployeesDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:13:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:13:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:13:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:13:32 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:13:32 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:13:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:13:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"firstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"extension\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"email\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"reportsTo\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"jobTitle\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 employeeNumber;\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary firstName (STRING);\n",
            "  optional binary extension (STRING);\n",
            "  optional binary email (STRING);\n",
            "  optional int32 officeCode;\n",
            "  optional double reportsTo;\n",
            "  optional binary jobTitle (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:42551 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:13:32 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:42551 in memory (size: 34.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:42551 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:13:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:42551 in memory (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:13:33 INFO FileScanRDD: Reading File path: file:///content/data/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 07:13:33 INFO CodeGenerator: Code generated in 56.325745 ms\n",
            "25/08/06 07:13:33 INFO FileOutputCommitter: Saved output of task 'attempt_202508060713324895169703766427792_0002_m_000000_2' to file:/content/data/parquet/employees.parquet/_temporary/0/task_202508060713324895169703766427792_0002_m_000000\n",
            "25/08/06 07:13:33 INFO SparkHadoopMapRedUtil: attempt_202508060713324895169703766427792_0002_m_000000_2: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:13:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:13:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1543 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:13:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:13:33 INFO DAGScheduler: ResultStage 2 (parquet at EmployeesDataIngestion.java:17) finished in 1.615 s\n",
            "25/08/06 07:13:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:13:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:13:33 INFO DAGScheduler: Job 2 finished: parquet at EmployeesDataIngestion.java:17, took 1.626979 s\n",
            "25/08/06 07:13:33 INFO FileFormatWriter: Start to commit write Job da77f3c3-ea2c-4fbe-a3d8-b988ab854a83.\n",
            "25/08/06 07:13:33 INFO FileFormatWriter: Write Job da77f3c3-ea2c-4fbe-a3d8-b988ab854a83 committed. Elapsed time: 25 ms.\n",
            "25/08/06 07:13:33 INFO FileFormatWriter: Finished processing stats for write job da77f3c3-ea2c-4fbe-a3d8-b988ab854a83.\n",
            "✅ Employees Parquet File Created Successfully.\n",
            "25/08/06 07:13:33 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:13:33 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:13:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:13:33 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:13:33 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:13:33 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:13:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:13:33 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:13:33 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:13:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-7201a758-0577-4b58-8d0d-165bc9495965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CustomersDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class CustomersDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Customers Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> customersDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/customers.csv\");\n",
        "\n",
        "        customersDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/customers.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Customers Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwUNy2nvOhMx",
        "outputId": "a991588a-e138-40b6-90f1-39418cf3f360"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CustomersDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" CustomersDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" CustomersDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F40TD4vMOtJ-",
        "outputId": "bf62ced8-7716-4763-ed4a-91262c1280e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:14:12 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:14:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:14:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:14:13 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:14:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:14:13 INFO SparkContext: Submitted application: Customers Data Ingestion\n",
            "25/08/06 07:14:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:14:13 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:14:13 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:14:13 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:14:13 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:14:13 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:14:13 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:14:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:14:13 INFO Utils: Successfully started service 'sparkDriver' on port 34685.\n",
            "25/08/06 07:14:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:14:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:14:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:14:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:14:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:14:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f637f3e-fc31-46f6-94c4-03086a0aeec6\n",
            "25/08/06 07:14:14 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:14:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:14:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:14:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:14:14 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:14:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:14:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41773.\n",
            "25/08/06 07:14:14 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:41773\n",
            "25/08/06 07:14:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:14:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 41773, None)\n",
            "25/08/06 07:14:14 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:41773 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 41773, None)\n",
            "25/08/06 07:14:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 41773, None)\n",
            "25/08/06 07:14:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 41773, None)\n",
            "25/08/06 07:14:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:14:15 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:14:17 INFO InMemoryFileIndex: It took 183 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:14:17 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:14:22 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:14:22 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:14:23 INFO CodeGenerator: Code generated in 416.196035 ms\n",
            "25/08/06 07:14:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:14:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:14:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:41773 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:14:23 INFO SparkContext: Created broadcast 0 from csv at CustomersDataIngestion.java:13\n",
            "25/08/06 07:14:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:14:23 INFO SparkContext: Starting job: csv at CustomersDataIngestion.java:13\n",
            "25/08/06 07:14:23 INFO DAGScheduler: Got job 0 (csv at CustomersDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:14:23 INFO DAGScheduler: Final stage: ResultStage 0 (csv at CustomersDataIngestion.java:13)\n",
            "25/08/06 07:14:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:14:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:14:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at CustomersDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:14:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:14:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:14:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:41773 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:14:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at CustomersDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:14:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:14:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:14:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:14:24 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:14:24 INFO CodeGenerator: Code generated in 50.5983 ms\n",
            "25/08/06 07:14:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1743 bytes result sent to driver\n",
            "25/08/06 07:14:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 473 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:14:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:14:24 INFO DAGScheduler: ResultStage 0 (csv at CustomersDataIngestion.java:13) finished in 0.750 s\n",
            "25/08/06 07:14:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:14:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:14:24 INFO DAGScheduler: Job 0 finished: csv at CustomersDataIngestion.java:13, took 0.854617 s\n",
            "25/08/06 07:14:24 INFO CodeGenerator: Code generated in 16.932582 ms\n",
            "25/08/06 07:14:24 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:14:24 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:14:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:14:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:14:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:41773 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:14:24 INFO SparkContext: Created broadcast 2 from csv at CustomersDataIngestion.java:13\n",
            "25/08/06 07:14:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:14:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:41773 in memory (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:14:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:41773 in memory (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:14:25 INFO SparkContext: Starting job: csv at CustomersDataIngestion.java:13\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Got job 1 (csv at CustomersDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Final stage: ResultStage 1 (csv at CustomersDataIngestion.java:13)\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at CustomersDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.0 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:14:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:41773 (size: 12.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:14:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at CustomersDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:14:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:14:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:14:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:14:25 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:14:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1719 bytes result sent to driver\n",
            "25/08/06 07:14:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 189 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:14:25 INFO DAGScheduler: ResultStage 1 (csv at CustomersDataIngestion.java:13) finished in 0.284 s\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:14:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:14:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Job 1 finished: csv at CustomersDataIngestion.java:13, took 0.297048 s\n",
            "25/08/06 07:14:25 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:14:25 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:14:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:14:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:14:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:14:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:41773 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:14:25 INFO SparkContext: Created broadcast 4 from parquet at CustomersDataIngestion.java:17\n",
            "25/08/06 07:14:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:14:25 INFO SparkContext: Starting job: parquet at CustomersDataIngestion.java:17\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Got job 2 (parquet at CustomersDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at CustomersDataIngestion.java:17)\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at CustomersDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 213.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:14:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.0 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:14:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:41773 (size: 77.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:14:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at CustomersDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:14:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:14:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:14:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:14:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:14:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:14:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:14:25 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:14:26 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:14:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:14:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional double salesRepEmployeeNumber;\n",
            "  optional double creditLimit;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:14:26 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:14:26 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:14:26 INFO CodeGenerator: Code generated in 73.856138 ms\n",
            "25/08/06 07:14:27 INFO FileOutputCommitter: Saved output of task 'attempt_202508060714258689340732524864131_0002_m_000000_2' to file:/content/data/parquet/customers.parquet/_temporary/0/task_202508060714258689340732524864131_0002_m_000000\n",
            "25/08/06 07:14:27 INFO SparkHadoopMapRedUtil: attempt_202508060714258689340732524864131_0002_m_000000_2: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 07:14:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver\n",
            "25/08/06 07:14:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1432 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:14:27 INFO DAGScheduler: ResultStage 2 (parquet at CustomersDataIngestion.java:17) finished in 1.496 s\n",
            "25/08/06 07:14:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:14:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:14:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:14:27 INFO DAGScheduler: Job 2 finished: parquet at CustomersDataIngestion.java:17, took 1.509622 s\n",
            "25/08/06 07:14:27 INFO FileFormatWriter: Start to commit write Job 8f471199-d7ed-49c9-89ba-00d5b6a28c36.\n",
            "25/08/06 07:14:27 INFO FileFormatWriter: Write Job 8f471199-d7ed-49c9-89ba-00d5b6a28c36 committed. Elapsed time: 31 ms.\n",
            "25/08/06 07:14:27 INFO FileFormatWriter: Finished processing stats for write job 8f471199-d7ed-49c9-89ba-00d5b6a28c36.\n",
            "✅ Customers Parquet File Created Successfully.\n",
            "25/08/06 07:14:27 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:14:27 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:14:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:14:27 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:14:27 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:14:27 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:14:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:14:27 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:14:27 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:14:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-9cd807dd-c082-40bb-8a5d-9e77e964a6bf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrderDetailsDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OrderDetailsDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"OrderDetails Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orderDetailsDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/orderdetails.csv\");\n",
        "\n",
        "        orderDetailsDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ OrderDetails Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jog1odu0OuiT",
        "outputId": "4697c665-72d3-4d63-c421-7f2449461f5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting OrderDetailsDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDetailsDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" OrderDetailsDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3764fZK6O4n5",
        "outputId": "506ffbb6-d47f-435b-c5fb-1c853c35795a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:15:00 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:15:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:15:00 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:15:00 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:15:00 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:15:00 INFO SparkContext: Submitted application: OrderDetails Data Ingestion\n",
            "25/08/06 07:15:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:15:00 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:15:00 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:15:00 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:15:00 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:15:00 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:15:00 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:15:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:15:01 INFO Utils: Successfully started service 'sparkDriver' on port 33817.\n",
            "25/08/06 07:15:01 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:15:01 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:15:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:15:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:15:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:15:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7f03f512-5acd-423c-a9e2-37764187fa3e\n",
            "25/08/06 07:15:01 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:15:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:15:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:15:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:15:02 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:15:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:15:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45971.\n",
            "25/08/06 07:15:02 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:45971\n",
            "25/08/06 07:15:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:15:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 45971, None)\n",
            "25/08/06 07:15:02 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:45971 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 45971, None)\n",
            "25/08/06 07:15:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 45971, None)\n",
            "25/08/06 07:15:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 45971, None)\n",
            "25/08/06 07:15:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:15:03 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:15:04 INFO InMemoryFileIndex: It took 105 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:15:04 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:15:09 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:09 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:15:10 INFO CodeGenerator: Code generated in 354.014261 ms\n",
            "25/08/06 07:15:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:45971 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:15:10 INFO SparkContext: Created broadcast 0 from csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:15:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:11 INFO SparkContext: Starting job: csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Got job 0 (csv at OrderDetailsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Final stage: ResultStage 0 (csv at OrderDetailsDataIngestion.java:13)\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at OrderDetailsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:15:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:45971 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:15:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at OrderDetailsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:15:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:15:11 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:15:11 INFO CodeGenerator: Code generated in 35.98292 ms\n",
            "25/08/06 07:15:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver\n",
            "25/08/06 07:15:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 466 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:15:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:15:12 INFO DAGScheduler: ResultStage 0 (csv at OrderDetailsDataIngestion.java:13) finished in 0.705 s\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Job 0 finished: csv at OrderDetailsDataIngestion.java:13, took 0.851604 s\n",
            "25/08/06 07:15:12 INFO CodeGenerator: Code generated in 28.063502 ms\n",
            "25/08/06 07:15:12 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:15:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:15:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:45971 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:12 INFO SparkContext: Created broadcast 2 from csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:15:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:12 INFO SparkContext: Starting job: csv at OrderDetailsDataIngestion.java:13\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Got job 1 (csv at OrderDetailsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Final stage: ResultStage 1 (csv at OrderDetailsDataIngestion.java:13)\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at OrderDetailsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:15:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:45971 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at OrderDetailsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:15:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:15:12 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:15:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1676 bytes result sent to driver\n",
            "25/08/06 07:15:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 372 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:15:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:15:12 INFO DAGScheduler: ResultStage 1 (csv at OrderDetailsDataIngestion.java:13) finished in 0.466 s\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:15:12 INFO DAGScheduler: Job 1 finished: csv at OrderDetailsDataIngestion.java:13, took 0.476238 s\n",
            "25/08/06 07:15:13 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:15:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:15:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:15:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:45971 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:13 INFO SparkContext: Created broadcast 4 from parquet at OrderDetailsDataIngestion.java:17\n",
            "25/08/06 07:15:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:13 INFO SparkContext: Starting job: parquet at OrderDetailsDataIngestion.java:17\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Got job 2 (parquet at OrderDetailsDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrderDetailsDataIngestion.java:17)\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at OrderDetailsDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:15:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.2 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:15:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:15:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:45971 (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:15:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at OrderDetailsDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:15:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:13 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:15:13 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:15:13 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:15:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "  optional double priceEach;\n",
            "  optional int32 orderLineNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:15:13 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:15:14 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:45971 in memory (size: 34.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:15:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:45971 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:15:14 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:15:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:45971 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:45971 in memory (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:14 INFO CodeGenerator: Code generated in 27.342083 ms\n",
            "25/08/06 07:15:14 INFO FileOutputCommitter: Saved output of task 'attempt_202508060715136248949434032462044_0002_m_000000_2' to file:/content/data/parquet/orderdetails.parquet/_temporary/0/task_202508060715136248949434032462044_0002_m_000000\n",
            "25/08/06 07:15:14 INFO SparkHadoopMapRedUtil: attempt_202508060715136248949434032462044_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:15:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:15:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1466 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:15:14 INFO DAGScheduler: ResultStage 2 (parquet at OrderDetailsDataIngestion.java:17) finished in 1.554 s\n",
            "25/08/06 07:15:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:15:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:15:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:15:14 INFO DAGScheduler: Job 2 finished: parquet at OrderDetailsDataIngestion.java:17, took 1.570572 s\n",
            "25/08/06 07:15:14 INFO FileFormatWriter: Start to commit write Job 1def3c01-acaa-4cff-ab65-343f12d4f336.\n",
            "25/08/06 07:15:14 INFO FileFormatWriter: Write Job 1def3c01-acaa-4cff-ab65-343f12d4f336 committed. Elapsed time: 32 ms.\n",
            "25/08/06 07:15:14 INFO FileFormatWriter: Finished processing stats for write job 1def3c01-acaa-4cff-ab65-343f12d4f336.\n",
            "✅ OrderDetails Parquet File Created Successfully.\n",
            "25/08/06 07:15:14 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:15:14 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:15:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:15:14 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:15:14 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:15:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:15:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:15:14 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:15:14 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:15:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-778aede7-6829-4a9f-b073-954b2778f71a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile PaymentsDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class PaymentsDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Payments Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> paymentsDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/payments.csv\");\n",
        "\n",
        "        paymentsDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/payments.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Payments Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPjL3o2XO6HT",
        "outputId": "b0f918de-584d-4e9c-82c0-b491f7bbe33b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing PaymentsDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" PaymentsDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" PaymentsDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGIUSuRdPDFT",
        "outputId": "7c5d98a3-6fc8-4316-d734-e87523f17ba4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:15:46 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:15:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:15:47 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:15:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:15:47 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:15:47 INFO SparkContext: Submitted application: Payments Data Ingestion\n",
            "25/08/06 07:15:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:15:47 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:15:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:15:47 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:15:47 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:15:47 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:15:47 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:15:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:15:47 INFO Utils: Successfully started service 'sparkDriver' on port 35723.\n",
            "25/08/06 07:15:47 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:15:47 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:15:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:15:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:15:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:15:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-185a02aa-707c-4012-8ea2-a0140a26585e\n",
            "25/08/06 07:15:48 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:15:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:15:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:15:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:15:48 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:15:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:15:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39843.\n",
            "25/08/06 07:15:48 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:39843\n",
            "25/08/06 07:15:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:15:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 39843, None)\n",
            "25/08/06 07:15:48 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:39843 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 39843, None)\n",
            "25/08/06 07:15:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 39843, None)\n",
            "25/08/06 07:15:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 39843, None)\n",
            "25/08/06 07:15:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:15:49 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:15:51 INFO InMemoryFileIndex: It took 92 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:15:51 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:15:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:15:56 INFO CodeGenerator: Code generated in 667.826753 ms\n",
            "25/08/06 07:15:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:39843 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:15:57 INFO SparkContext: Created broadcast 0 from csv at PaymentsDataIngestion.java:13\n",
            "25/08/06 07:15:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:57 INFO SparkContext: Starting job: csv at PaymentsDataIngestion.java:13\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Got job 0 (csv at PaymentsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Final stage: ResultStage 0 (csv at PaymentsDataIngestion.java:13)\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at PaymentsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:15:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:15:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:39843 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:15:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at PaymentsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:15:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:15:58 INFO FileScanRDD: Reading File path: file:///content/data/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 07:15:58 INFO CodeGenerator: Code generated in 40.931918 ms\n",
            "25/08/06 07:15:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1657 bytes result sent to driver\n",
            "25/08/06 07:15:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:15:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:15:58 INFO DAGScheduler: ResultStage 0 (csv at PaymentsDataIngestion.java:13) finished in 0.677 s\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:15:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Job 0 finished: csv at PaymentsDataIngestion.java:13, took 0.794348 s\n",
            "25/08/06 07:15:58 INFO CodeGenerator: Code generated in 28.732674 ms\n",
            "25/08/06 07:15:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:15:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:15:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:39843 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:58 INFO SparkContext: Created broadcast 2 from csv at PaymentsDataIngestion.java:13\n",
            "25/08/06 07:15:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:58 INFO SparkContext: Starting job: csv at PaymentsDataIngestion.java:13\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Got job 1 (csv at PaymentsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Final stage: ResultStage 1 (csv at PaymentsDataIngestion.java:13)\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at PaymentsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:15:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:15:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:39843 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at PaymentsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:15:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:15:58 INFO FileScanRDD: Reading File path: file:///content/data/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 07:15:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1722 bytes result sent to driver\n",
            "25/08/06 07:15:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 201 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:15:59 INFO DAGScheduler: ResultStage 1 (csv at PaymentsDataIngestion.java:13) finished in 0.294 s\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:15:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:15:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Job 1 finished: csv at PaymentsDataIngestion.java:13, took 0.303499 s\n",
            "25/08/06 07:15:59 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:15:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:15:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:15:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:15:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:39843 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:15:59 INFO SparkContext: Created broadcast 4 from parquet at PaymentsDataIngestion.java:17\n",
            "25/08/06 07:15:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:15:59 INFO SparkContext: Starting job: parquet at PaymentsDataIngestion.java:17\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Got job 2 (parquet at PaymentsDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at PaymentsDataIngestion.java:17)\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at PaymentsDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:15:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:15:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:15:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:39843 (size: 76.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:15:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:15:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at PaymentsDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:15:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:15:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:15:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:15:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:15:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:15:59 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:15:59 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:15:59 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:15:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional int32 paymentDate (DATE);\n",
            "  optional double amount;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:15:59 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:15:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:39843 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:15:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:39843 in memory (size: 12.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:16:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:39843 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:39843 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:00 INFO FileScanRDD: Reading File path: file:///content/data/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 07:16:00 INFO CodeGenerator: Code generated in 23.51039 ms\n",
            "25/08/06 07:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060715595738882455032928849_0002_m_000000_2' to file:/content/data/parquet/payments.parquet/_temporary/0/task_202508060715595738882455032928849_0002_m_000000\n",
            "25/08/06 07:16:01 INFO SparkHadoopMapRedUtil: attempt_202508060715595738882455032928849_0002_m_000000_2: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 07:16:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:16:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1712 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:16:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:16:01 INFO DAGScheduler: ResultStage 2 (parquet at PaymentsDataIngestion.java:17) finished in 1.787 s\n",
            "25/08/06 07:16:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:16:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:16:01 INFO DAGScheduler: Job 2 finished: parquet at PaymentsDataIngestion.java:17, took 1.795892 s\n",
            "25/08/06 07:16:01 INFO FileFormatWriter: Start to commit write Job 826a6657-196d-42e9-bf8a-13407e3906da.\n",
            "25/08/06 07:16:01 INFO FileFormatWriter: Write Job 826a6657-196d-42e9-bf8a-13407e3906da committed. Elapsed time: 33 ms.\n",
            "25/08/06 07:16:01 INFO FileFormatWriter: Finished processing stats for write job 826a6657-196d-42e9-bf8a-13407e3906da.\n",
            "✅ Payments Parquet File Created Successfully.\n",
            "25/08/06 07:16:01 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:16:01 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:16:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:16:01 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:16:01 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:16:01 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:16:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:16:01 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:16:01 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:16:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-43ddf1f7-f35a-4245-a0ca-854c8f9ee0ee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductLinesDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class ProductLinesDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"ProductLines Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> productLinesDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/productlines.csv\");\n",
        "\n",
        "        productLinesDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/productlines.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ ProductLines Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZ8Rkv7PFN4",
        "outputId": "71375969-dcde-4ba6-862a-f0c73bb0fc43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ProductLinesDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" ProductLinesDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" ProductLinesDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkelr8VUPPhC",
        "outputId": "d10229d1-be9b-4ddb-9d69-ab1f90f05685"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:16:33 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:16:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:16:34 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:16:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:16:34 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:16:34 INFO SparkContext: Submitted application: ProductLines Data Ingestion\n",
            "25/08/06 07:16:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:16:34 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:16:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:16:34 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:16:34 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:16:34 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:16:34 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:16:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:16:35 INFO Utils: Successfully started service 'sparkDriver' on port 35299.\n",
            "25/08/06 07:16:35 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:16:35 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:16:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:16:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:16:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:16:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8a325f91-5ab5-447f-93ce-0b29c51b024b\n",
            "25/08/06 07:16:35 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:16:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:16:35 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:16:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:16:35 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:16:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:16:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44909.\n",
            "25/08/06 07:16:35 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:44909\n",
            "25/08/06 07:16:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:16:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 44909, None)\n",
            "25/08/06 07:16:35 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:44909 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 44909, None)\n",
            "25/08/06 07:16:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 44909, None)\n",
            "25/08/06 07:16:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 44909, None)\n",
            "25/08/06 07:16:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:16:36 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:16:38 INFO InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:16:38 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:16:42 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:16:42 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:16:43 INFO CodeGenerator: Code generated in 488.10902 ms\n",
            "25/08/06 07:16:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:16:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:44909 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:16:43 INFO SparkContext: Created broadcast 0 from csv at ProductLinesDataIngestion.java:13\n",
            "25/08/06 07:16:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:16:44 INFO SparkContext: Starting job: csv at ProductLinesDataIngestion.java:13\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Got job 0 (csv at ProductLinesDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Final stage: ResultStage 0 (csv at ProductLinesDataIngestion.java:13)\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at ProductLinesDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:16:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:16:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:16:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:44909 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:16:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:16:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at ProductLinesDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:16:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:16:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:16:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:16:45 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:16:45 INFO CodeGenerator: Code generated in 42.575503 ms\n",
            "25/08/06 07:16:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1660 bytes result sent to driver\n",
            "25/08/06 07:16:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 769 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:16:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:16:45 INFO DAGScheduler: ResultStage 0 (csv at ProductLinesDataIngestion.java:13) finished in 1.234 s\n",
            "25/08/06 07:16:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:16:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:16:45 INFO DAGScheduler: Job 0 finished: csv at ProductLinesDataIngestion.java:13, took 1.389822 s\n",
            "25/08/06 07:16:45 INFO CodeGenerator: Code generated in 31.492114 ms\n",
            "25/08/06 07:16:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:16:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:16:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:16:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:16:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:44909 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:45 INFO SparkContext: Created broadcast 2 from csv at ProductLinesDataIngestion.java:13\n",
            "25/08/06 07:16:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:16:46 INFO SparkContext: Starting job: csv at ProductLinesDataIngestion.java:13\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Got job 1 (csv at ProductLinesDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Final stage: ResultStage 1 (csv at ProductLinesDataIngestion.java:13)\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at ProductLinesDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:16:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:44909 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at ProductLinesDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:16:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:16:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:16:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:16:46 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:16:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1618 bytes result sent to driver\n",
            "25/08/06 07:16:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 261 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:16:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:16:46 INFO DAGScheduler: ResultStage 1 (csv at ProductLinesDataIngestion.java:13) finished in 0.476 s\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:16:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Job 1 finished: csv at ProductLinesDataIngestion.java:13, took 0.499631 s\n",
            "25/08/06 07:16:46 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:16:46 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:16:46 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:16:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:16:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:16:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:16:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:16:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:16:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:16:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:44909 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:46 INFO SparkContext: Created broadcast 4 from parquet at ProductLinesDataIngestion.java:17\n",
            "25/08/06 07:16:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:16:46 INFO SparkContext: Starting job: parquet at ProductLinesDataIngestion.java:17\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Got job 2 (parquet at ProductLinesDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at ProductLinesDataIngestion.java:17)\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at ProductLinesDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 210.9 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:16:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:16:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:44909 (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:16:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at ProductLinesDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:16:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:16:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:16:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:16:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:16:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:16:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:16:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:16:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:16:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:16:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:16:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:16:47 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:16:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"htmlDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"image\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "  optional binary htmlDescription (STRING);\n",
            "  optional binary image (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:16:47 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:16:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:44909 in memory (size: 12.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:16:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:44909 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:16:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:44909 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:44909 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:16:47 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:16:47 INFO CodeGenerator: Code generated in 24.253294 ms\n",
            "25/08/06 07:16:48 INFO FileOutputCommitter: Saved output of task 'attempt_202508060716468335404846779214685_0002_m_000000_2' to file:/content/data/parquet/productlines.parquet/_temporary/0/task_202508060716468335404846779214685_0002_m_000000\n",
            "25/08/06 07:16:48 INFO SparkHadoopMapRedUtil: attempt_202508060716468335404846779214685_0002_m_000000_2: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:16:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:16:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1324 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:16:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:16:48 INFO DAGScheduler: ResultStage 2 (parquet at ProductLinesDataIngestion.java:17) finished in 1.390 s\n",
            "25/08/06 07:16:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:16:48 INFO DAGScheduler: Job 2 finished: parquet at ProductLinesDataIngestion.java:17, took 1.399616 s\n",
            "25/08/06 07:16:48 INFO FileFormatWriter: Start to commit write Job e81427fd-1774-4121-b36f-e42f14e9439f.\n",
            "25/08/06 07:16:48 INFO FileFormatWriter: Write Job e81427fd-1774-4121-b36f-e42f14e9439f committed. Elapsed time: 24 ms.\n",
            "25/08/06 07:16:48 INFO FileFormatWriter: Finished processing stats for write job e81427fd-1774-4121-b36f-e42f14e9439f.\n",
            "✅ ProductLines Parquet File Created Successfully.\n",
            "25/08/06 07:16:48 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:16:48 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:16:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:16:48 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:16:48 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:16:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:16:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:16:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:16:48 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:16:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9a706d1-0abd-4390-82a2-c1ca3404be10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductsDataIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class ProductsDataIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Products Data Ingestion\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> productsDF = spark.read()\n",
        "            .option(\"header\", \"true\")\n",
        "            .option(\"inferSchema\", \"true\")\n",
        "            .csv(\"file:///content/data/products.csv\");\n",
        "\n",
        "        productsDF.write()\n",
        "            .mode(SaveMode.Overwrite)\n",
        "            .parquet(\"file:///content/data/parquet/products.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Products Parquet File Created Successfully.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00pKKrv_PQus",
        "outputId": "dfbe3160-7dc0-4077-a83e-0746423c780c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ProductsDataIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.4.1-bin-hadoop3/jars/*\" ProductsDataIngestion.java\n",
        "!java -cp \".:/content/spark-3.4.1-bin-hadoop3/jars/*\" ProductsDataIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyC2uV0PWoz",
        "outputId": "e4c99a84-c66d-4e36-9efd-4297b2f8d4bf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:17:10 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:17:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:17:11 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:17:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:17:11 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:17:11 INFO SparkContext: Submitted application: Products Data Ingestion\n",
            "25/08/06 07:17:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:17:11 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:17:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:17:11 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:17:11 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:17:11 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:17:11 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:17:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:17:11 INFO Utils: Successfully started service 'sparkDriver' on port 34181.\n",
            "25/08/06 07:17:11 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:17:12 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:17:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:17:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:17:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:17:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7658e452-bfa3-42a8-ac0e-7ffd4e6726e8\n",
            "25/08/06 07:17:12 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:17:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:17:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:17:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:17:12 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:17:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:17:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46489.\n",
            "25/08/06 07:17:12 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:46489\n",
            "25/08/06 07:17:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:17:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 46489, None)\n",
            "25/08/06 07:17:12 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:46489 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 46489, None)\n",
            "25/08/06 07:17:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 46489, None)\n",
            "25/08/06 07:17:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 46489, None)\n",
            "25/08/06 07:17:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:17:13 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:17:15 INFO InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:17:15 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:17:19 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:17:19 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:17:20 INFO CodeGenerator: Code generated in 453.645651 ms\n",
            "25/08/06 07:17:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:17:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:17:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:46489 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:17:20 INFO SparkContext: Created broadcast 0 from csv at ProductsDataIngestion.java:13\n",
            "25/08/06 07:17:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:17:20 INFO SparkContext: Starting job: csv at ProductsDataIngestion.java:13\n",
            "25/08/06 07:17:20 INFO DAGScheduler: Got job 0 (csv at ProductsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:17:20 INFO DAGScheduler: Final stage: ResultStage 0 (csv at ProductsDataIngestion.java:13)\n",
            "25/08/06 07:17:20 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:17:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:17:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at ProductsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:17:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:17:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:17:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:46489 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:17:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:17:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at ProductsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:17:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:17:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:17:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:17:21 INFO FileScanRDD: Reading File path: file:///content/data/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 07:17:21 INFO CodeGenerator: Code generated in 35.446155 ms\n",
            "25/08/06 07:17:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1698 bytes result sent to driver\n",
            "25/08/06 07:17:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 787 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:17:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:17:22 INFO DAGScheduler: ResultStage 0 (csv at ProductsDataIngestion.java:13) finished in 1.082 s\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:17:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Job 0 finished: csv at ProductsDataIngestion.java:13, took 1.178971 s\n",
            "25/08/06 07:17:22 INFO CodeGenerator: Code generated in 72.951732 ms\n",
            "25/08/06 07:17:22 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:17:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:17:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:17:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:17:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:46489 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:17:22 INFO SparkContext: Created broadcast 2 from csv at ProductsDataIngestion.java:13\n",
            "25/08/06 07:17:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:17:22 INFO SparkContext: Starting job: csv at ProductsDataIngestion.java:13\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Got job 1 (csv at ProductsDataIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Final stage: ResultStage 1 (csv at ProductsDataIngestion.java:13)\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at ProductsDataIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:17:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:17:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:17:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:46489 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:17:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:17:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at ProductsDataIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:17:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:17:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:17:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:17:22 INFO FileScanRDD: Reading File path: file:///content/data/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 07:17:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1595 bytes result sent to driver\n",
            "25/08/06 07:17:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 319 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:17:23 INFO DAGScheduler: ResultStage 1 (csv at ProductsDataIngestion.java:13) finished in 0.503 s\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:17:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Job 1 finished: csv at ProductsDataIngestion.java:13, took 0.528288 s\n",
            "25/08/06 07:17:23 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:17:23 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:17:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:17:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:17:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:17:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:17:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:17:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:46489 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:17:23 INFO SparkContext: Created broadcast 4 from parquet at ProductsDataIngestion.java:17\n",
            "25/08/06 07:17:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:17:23 INFO SparkContext: Starting job: parquet at ProductsDataIngestion.java:17\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Got job 2 (parquet at ProductsDataIngestion.java:17) with 1 output partitions\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at ProductsDataIngestion.java:17)\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at ProductsDataIngestion.java:17), which has no missing parents\n",
            "25/08/06 07:17:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 212.1 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:17:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:17:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:46489 (size: 76.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:17:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:17:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at ProductsDataIngestion.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:17:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:17:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:17:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:17:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:17:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:17:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:17:23 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:17:23 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:17:24 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:17:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productScale\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productVendor\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityInStock\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"buyPrice\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MSRP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary productScale (STRING);\n",
            "  optional binary productVendor (STRING);\n",
            "  optional binary productDescription (STRING);\n",
            "  optional binary quantityInStock (STRING);\n",
            "  optional binary buyPrice (STRING);\n",
            "  optional binary MSRP (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:17:24 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:17:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:46489 in memory (size: 34.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:17:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:46489 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:17:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:46489 in memory (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:17:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:46489 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:17:24 INFO FileScanRDD: Reading File path: file:///content/data/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 07:17:24 INFO CodeGenerator: Code generated in 30.805148 ms\n",
            "25/08/06 07:17:25 INFO FileOutputCommitter: Saved output of task 'attempt_202508060717234673866078228050372_0002_m_000000_2' to file:/content/data/parquet/products.parquet/_temporary/0/task_202508060717234673866078228050372_0002_m_000000\n",
            "25/08/06 07:17:25 INFO SparkHadoopMapRedUtil: attempt_202508060717234673866078228050372_0002_m_000000_2: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:17:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2545 bytes result sent to driver\n",
            "25/08/06 07:17:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1937 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:17:25 INFO DAGScheduler: ResultStage 2 (parquet at ProductsDataIngestion.java:17) finished in 2.044 s\n",
            "25/08/06 07:17:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:17:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:17:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:17:25 INFO DAGScheduler: Job 2 finished: parquet at ProductsDataIngestion.java:17, took 2.059344 s\n",
            "25/08/06 07:17:25 INFO FileFormatWriter: Start to commit write Job de8c39c2-5ceb-4205-9e0e-4eb65aaadff9.\n",
            "25/08/06 07:17:25 INFO FileFormatWriter: Write Job de8c39c2-5ceb-4205-9e0e-4eb65aaadff9 committed. Elapsed time: 38 ms.\n",
            "25/08/06 07:17:25 INFO FileFormatWriter: Finished processing stats for write job de8c39c2-5ceb-4205-9e0e-4eb65aaadff9.\n",
            "✅ Products Parquet File Created Successfully.\n",
            "25/08/06 07:17:25 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:17:25 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:17:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:17:25 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:17:25 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:17:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:17:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:17:25 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:17:25 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:17:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-0cbb0be4-bb5b-408e-9e44-544a1522ccae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductLineIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class ProductLineIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Product Line Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType, true)\n",
        "                .add(\"textDescription\", DataTypes.StringType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/productlines.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/productlines.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ ProductLines Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eepSxwtCPZud",
        "outputId": "11a17199-023b-49da-add4-9440292d5bbd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ProductLineIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" ProductLineIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" ProductLineIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr0rzSy3QAaT",
        "outputId": "cdb8cf17-354a-4f6a-9045-c73b88ab2466"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:20:37 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:20:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:20:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:20:38 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:20:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:20:38 INFO SparkContext: Submitted application: Product Line Ingestion\n",
            "25/08/06 07:20:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:20:38 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:20:38 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:20:38 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:20:38 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:20:38 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:20:38 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:20:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:20:38 INFO Utils: Successfully started service 'sparkDriver' on port 39393.\n",
            "25/08/06 07:20:39 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:20:39 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:20:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:20:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:20:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:20:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2a1c9f06-a5bc-47d9-99ff-11b3f98b0061\n",
            "25/08/06 07:20:39 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:20:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:20:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:20:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:20:39 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:20:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:20:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36991.\n",
            "25/08/06 07:20:39 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:36991\n",
            "25/08/06 07:20:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:20:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 36991, None)\n",
            "25/08/06 07:20:39 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:36991 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 36991, None)\n",
            "25/08/06 07:20:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 36991, None)\n",
            "25/08/06 07:20:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 36991, None)\n",
            "25/08/06 07:20:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:20:40 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:20:43 INFO InMemoryFileIndex: It took 150 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:20:46 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:20:46 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:20:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:20:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:20:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:36991 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:20:47 INFO SparkContext: Created broadcast 0 from show at ProductLineIngestion.java:22\n",
            "25/08/06 07:20:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:20:47 INFO SparkContext: Starting job: show at ProductLineIngestion.java:22\n",
            "25/08/06 07:20:47 INFO DAGScheduler: Got job 0 (show at ProductLineIngestion.java:22) with 1 output partitions\n",
            "25/08/06 07:20:47 INFO DAGScheduler: Final stage: ResultStage 0 (show at ProductLineIngestion.java:22)\n",
            "25/08/06 07:20:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:20:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:20:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at ProductLineIngestion.java:22), which has no missing parents\n",
            "25/08/06 07:20:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:20:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:20:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:36991 (size: 5.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:20:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:20:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at ProductLineIngestion.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:20:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:20:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:20:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:20:48 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:20:48 INFO CodeGenerator: Code generated in 380.973788 ms\n",
            "25/08/06 07:20:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/data/productlines.csv\n",
            "25/08/06 07:20:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3594 bytes result sent to driver\n",
            "25/08/06 07:20:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1159 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:20:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:20:49 INFO DAGScheduler: ResultStage 0 (show at ProductLineIngestion.java:22) finished in 1.591 s\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:20:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Job 0 finished: show at ProductLineIngestion.java:22, took 1.698106 s\n",
            "25/08/06 07:20:49 INFO CodeGenerator: Code generated in 54.989918 ms\n",
            "+----------------+--------------------+\n",
            "|     productLine|     textDescription|\n",
            "+----------------+--------------------+\n",
            "|    Classic Cars|Attention car ent...|\n",
            "|     Motorcycles|Our motorcycles a...|\n",
            "|          Planes|Unique, diecast a...|\n",
            "|           Ships|The perfect holid...|\n",
            "|          Trains|Model trains are ...|\n",
            "|Trucks and Buses|The Truck and Bus...|\n",
            "|    Vintage Cars|Our Vintage Car m...|\n",
            "+----------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- productLine: string (nullable = true)\n",
            " |-- textDescription: string (nullable = true)\n",
            "\n",
            "25/08/06 07:20:49 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:20:49 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:20:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:20:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:20:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:20:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:20:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:20:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:20:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:20:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:20:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:20:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:36991 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:20:49 INFO SparkContext: Created broadcast 2 from parquet at ProductLineIngestion.java:27\n",
            "25/08/06 07:20:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:20:49 INFO SparkContext: Starting job: parquet at ProductLineIngestion.java:27\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Got job 1 (parquet at ProductLineIngestion.java:27) with 1 output partitions\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at ProductLineIngestion.java:27)\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at ProductLineIngestion.java:27), which has no missing parents\n",
            "25/08/06 07:20:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 210.4 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:20:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:20:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:36991 (size: 76.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:20:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:20:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at ProductLineIngestion.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:20:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:20:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:20:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:20:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:20:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:20:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:20:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:20:50 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:20:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:20:50 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:20:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:36991 in memory (size: 5.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:20:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:36991 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:20:50 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:20:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/data/productlines.csv\n",
            "25/08/06 07:20:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060720497916536626675666420_0001_m_000000_1' to file:/content/data/parquet/productlines.parquet/_temporary/0/task_202508060720497916536626675666420_0001_m_000000\n",
            "25/08/06 07:20:51 INFO SparkHadoopMapRedUtil: attempt_202508060720497916536626675666420_0001_m_000000_1: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 07:20:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:20:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1267 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:20:51 INFO DAGScheduler: ResultStage 1 (parquet at ProductLineIngestion.java:27) finished in 1.355 s\n",
            "25/08/06 07:20:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:20:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:20:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:20:51 INFO DAGScheduler: Job 1 finished: parquet at ProductLineIngestion.java:27, took 1.366806 s\n",
            "25/08/06 07:20:51 INFO FileFormatWriter: Start to commit write Job 6b1659a0-c88e-444f-a792-299d53245f72.\n",
            "25/08/06 07:20:51 INFO FileFormatWriter: Write Job 6b1659a0-c88e-444f-a792-299d53245f72 committed. Elapsed time: 25 ms.\n",
            "25/08/06 07:20:51 INFO FileFormatWriter: Finished processing stats for write job 6b1659a0-c88e-444f-a792-299d53245f72.\n",
            "✅ ProductLines Parquet File Created Successfully.\n",
            "25/08/06 07:20:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:20:51 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:20:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:20:51 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:20:51 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:20:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:20:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:20:51 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:20:51 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:20:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8139ba8-8a80-4c51-869f-44ab7052d493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile EmployeeIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class EmployeeIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Employee Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"employeeNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"lastName\", DataTypes.StringType, true)\n",
        "                .add(\"firstName\", DataTypes.StringType, true)\n",
        "                .add(\"extension\", DataTypes.StringType, true)\n",
        "                .add(\"email\", DataTypes.StringType, true)\n",
        "                .add(\"officeCode\", DataTypes.StringType, true)\n",
        "                .add(\"reportsTo\", DataTypes.IntegerType, true)\n",
        "                .add(\"jobTitle\", DataTypes.StringType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/employees.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/employees.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Employees Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a_Jt4KQCEH",
        "outputId": "0a9a17ac-9e7d-4ebf-be40-c4a4a3d7f5d6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EmployeeIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" EmployeeIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" EmployeeIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hemWyKTvQYqv",
        "outputId": "b954296a-dee2-497d-90fd-79b201f2d905"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:21:35 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:21:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:21:35 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:21:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:21:35 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:21:35 INFO SparkContext: Submitted application: Employee Ingestion\n",
            "25/08/06 07:21:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:21:36 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:21:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:21:36 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:21:36 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:21:36 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:21:36 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:21:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:21:36 INFO Utils: Successfully started service 'sparkDriver' on port 36895.\n",
            "25/08/06 07:21:36 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:21:36 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:21:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:21:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:21:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:21:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4804d18-b6a3-4e0e-bb16-b051188906ba\n",
            "25/08/06 07:21:37 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:21:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:21:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:21:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:21:37 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:21:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:21:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39109.\n",
            "25/08/06 07:21:37 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:39109\n",
            "25/08/06 07:21:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:21:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 39109, None)\n",
            "25/08/06 07:21:37 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:39109 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 39109, None)\n",
            "25/08/06 07:21:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 39109, None)\n",
            "25/08/06 07:21:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 39109, None)\n",
            "25/08/06 07:21:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:21:38 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:21:40 INFO InMemoryFileIndex: It took 129 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:21:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:21:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:21:46 INFO CodeGenerator: Code generated in 568.560316 ms\n",
            "25/08/06 07:21:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:21:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:21:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:39109 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:21:47 INFO SparkContext: Created broadcast 0 from show at EmployeeIngestion.java:28\n",
            "25/08/06 07:21:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:21:47 INFO SparkContext: Starting job: show at EmployeeIngestion.java:28\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Got job 0 (show at EmployeeIngestion.java:28) with 1 output partitions\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Final stage: ResultStage 0 (show at EmployeeIngestion.java:28)\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at EmployeeIngestion.java:28), which has no missing parents\n",
            "25/08/06 07:21:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:21:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:21:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:39109 (size: 7.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:21:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:21:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at EmployeeIngestion.java:28) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:21:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:21:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:21:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:21:48 INFO FileScanRDD: Reading File path: file:///content/data/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 07:21:48 INFO CodeGenerator: Code generated in 66.902087 ms\n",
            "25/08/06 07:21:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2896 bytes result sent to driver\n",
            "25/08/06 07:21:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 634 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:21:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:21:48 INFO DAGScheduler: ResultStage 0 (show at EmployeeIngestion.java:28) finished in 0.970 s\n",
            "25/08/06 07:21:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:21:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:21:48 INFO DAGScheduler: Job 0 finished: show at EmployeeIngestion.java:28, took 1.105859 s\n",
            "25/08/06 07:21:48 INFO CodeGenerator: Code generated in 91.539914 ms\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "|employeeNumber| lastName|firstName|extension|               email|officeCode|reportsTo|            jobTitle|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "|          1002|   Murphy|    Diane|    x5800|dmurphy@classicmo...|         1|     null|           President|\n",
            "|          1056|Patterson|     Mary|    x4611|mpatterso@classic...|         1|     null|            VP Sales|\n",
            "|          1076| Firrelli|     Jeff|    x9273|jfirrelli@classic...|         1|     null|        VP Marketing|\n",
            "|          1088|Patterson|  William|    x4871|wpatterson@classi...|         6|     null|Sales Manager (APAC)|\n",
            "|          1102|   Bondur|   Gerard|    x5408|gbondur@classicmo...|         4|     null| Sale Manager (EMEA)|\n",
            "|          1143|      Bow|  Anthony|    x5428|abow@classicmodel...|         1|     null|  Sales Manager (NA)|\n",
            "|          1165| Jennings|   Leslie|    x3291|ljennings@classic...|         1|     null|           Sales Rep|\n",
            "|          1166| Thompson|   Leslie|    x4065|lthompson@classic...|         1|     null|           Sales Rep|\n",
            "|          1188| Firrelli|    Julie|    x2173|jfirrelli@classic...|         2|     null|           Sales Rep|\n",
            "|          1216|Patterson|    Steve|    x4334|spatterson@classi...|         2|     null|           Sales Rep|\n",
            "|          1286|    Tseng| Foon Yue|    x2248|ftseng@classicmod...|         3|     null|           Sales Rep|\n",
            "|          1323|   Vanauf|   George|    x4102|gvanauf@classicmo...|         3|     null|           Sales Rep|\n",
            "|          1337|   Bondur|     Loui|    x6493|lbondur@classicmo...|         4|     null|           Sales Rep|\n",
            "|          1370|Hernandez|   Gerard|    x2028|ghernande@classic...|         4|     null|           Sales Rep|\n",
            "|          1401| Castillo|   Pamela|    x2759|pcastillo@classic...|         4|     null|           Sales Rep|\n",
            "|          1501|     Bott|    Larry|    x2311|lbott@classicmode...|         7|     null|           Sales Rep|\n",
            "|          1504|    Jones|    Barry|     x102|bjones@classicmod...|         7|     null|           Sales Rep|\n",
            "|          1611|   Fixter|     Andy|     x101|afixter@classicmo...|         6|     null|           Sales Rep|\n",
            "|          1612|    Marsh|    Peter|     x102|pmarsh@classicmod...|         6|     null|           Sales Rep|\n",
            "|          1619|     King|      Tom|     x103|tking@classicmode...|         6|     null|           Sales Rep|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- employeeNumber: integer (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- firstName: string (nullable = true)\n",
            " |-- extension: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- officeCode: string (nullable = true)\n",
            " |-- reportsTo: integer (nullable = true)\n",
            " |-- jobTitle: string (nullable = true)\n",
            "\n",
            "25/08/06 07:21:48 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:21:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:21:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:21:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:21:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:21:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:21:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:21:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:21:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:21:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:21:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:21:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:39109 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:21:49 INFO SparkContext: Created broadcast 2 from parquet at EmployeeIngestion.java:33\n",
            "25/08/06 07:21:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:21:49 INFO SparkContext: Starting job: parquet at EmployeeIngestion.java:33\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Got job 1 (parquet at EmployeeIngestion.java:33) with 1 output partitions\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at EmployeeIngestion.java:33)\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at EmployeeIngestion.java:33), which has no missing parents\n",
            "25/08/06 07:21:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 211.8 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:21:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:21:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:39109 (size: 76.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:21:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:21:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at EmployeeIngestion.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:21:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:21:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:21:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:21:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:21:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:21:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:21:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:21:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:21:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:21:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:21:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:21:49 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:21:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"firstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"extension\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"email\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"reportsTo\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"jobTitle\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 employeeNumber;\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary firstName (STRING);\n",
            "  optional binary extension (STRING);\n",
            "  optional binary email (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "  optional int32 reportsTo;\n",
            "  optional binary jobTitle (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:21:49 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:21:50 INFO FileScanRDD: Reading File path: file:///content/data/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 07:21:50 INFO FileOutputCommitter: Saved output of task 'attempt_202508060721491672478037175625733_0001_m_000000_1' to file:/content/data/parquet/employees.parquet/_temporary/0/task_202508060721491672478037175625733_0001_m_000000\n",
            "25/08/06 07:21:50 INFO SparkHadoopMapRedUtil: attempt_202508060721491672478037175625733_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:21:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2502 bytes result sent to driver\n",
            "25/08/06 07:21:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1370 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:21:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:21:50 INFO DAGScheduler: ResultStage 1 (parquet at EmployeeIngestion.java:33) finished in 1.493 s\n",
            "25/08/06 07:21:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:21:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:21:50 INFO DAGScheduler: Job 1 finished: parquet at EmployeeIngestion.java:33, took 1.507800 s\n",
            "25/08/06 07:21:50 INFO FileFormatWriter: Start to commit write Job b7003ec4-ec30-426b-90ec-6ed77ca50a5b.\n",
            "25/08/06 07:21:50 INFO FileFormatWriter: Write Job b7003ec4-ec30-426b-90ec-6ed77ca50a5b committed. Elapsed time: 24 ms.\n",
            "25/08/06 07:21:50 INFO FileFormatWriter: Finished processing stats for write job b7003ec4-ec30-426b-90ec-6ed77ca50a5b.\n",
            "✅ Employees Parquet File Created Successfully.\n",
            "25/08/06 07:21:50 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:21:50 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:21:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:21:50 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:21:50 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:21:50 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:21:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:21:50 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:21:50 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:21:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-adf9422d-b7ec-484f-babf-18e946b0084f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CustomerIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class CustomerIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Customer Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"customerName\", DataTypes.StringType, true)\n",
        "                .add(\"contactLastName\", DataTypes.StringType, true)\n",
        "                .add(\"contactFirstName\", DataTypes.StringType, true)\n",
        "                .add(\"phone\", DataTypes.StringType, true)\n",
        "                .add(\"addressLine1\", DataTypes.StringType, true)\n",
        "                .add(\"addressLine2\", DataTypes.StringType, true)\n",
        "                .add(\"city\", DataTypes.StringType, true)\n",
        "                .add(\"state\", DataTypes.StringType, true)\n",
        "                .add(\"postalCode\", DataTypes.StringType, true)\n",
        "                .add(\"country\", DataTypes.StringType, true)\n",
        "                .add(\"salesRepEmployeeNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"creditLimit\", DataTypes.DoubleType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/customers.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/customers.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Customers Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcb9mzczQaKj",
        "outputId": "17beaf7a-ebc0-415c-f807-2f94e163cf2d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CustomerIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" CustomerIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" CustomerIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFdXn1S6QmWa",
        "outputId": "69fc1ba6-b84d-436d-d919-ea082cf2196f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:22:32 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:22:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:22:32 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:22:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:22:32 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:22:32 INFO SparkContext: Submitted application: Customer Ingestion\n",
            "25/08/06 07:22:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:22:32 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:22:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:22:33 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:22:33 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:22:33 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:22:33 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:22:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:22:33 INFO Utils: Successfully started service 'sparkDriver' on port 40491.\n",
            "25/08/06 07:22:33 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:22:33 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:22:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:22:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:22:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:22:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d5d3c0a-1fbe-41e7-ba9c-5a7bac453a34\n",
            "25/08/06 07:22:33 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:22:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:22:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:22:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:22:34 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:22:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:22:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33937.\n",
            "25/08/06 07:22:34 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:33937\n",
            "25/08/06 07:22:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:22:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 33937, None)\n",
            "25/08/06 07:22:34 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:33937 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 33937, None)\n",
            "25/08/06 07:22:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 33937, None)\n",
            "25/08/06 07:22:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 33937, None)\n",
            "25/08/06 07:22:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:22:35 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:22:36 INFO InMemoryFileIndex: It took 79 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:22:41 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:22:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:22:42 INFO CodeGenerator: Code generated in 504.718882 ms\n",
            "25/08/06 07:22:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:22:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:22:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:33937 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:22:42 INFO SparkContext: Created broadcast 0 from show at CustomerIngestion.java:33\n",
            "25/08/06 07:22:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:22:43 INFO SparkContext: Starting job: show at CustomerIngestion.java:33\n",
            "25/08/06 07:22:43 INFO DAGScheduler: Got job 0 (show at CustomerIngestion.java:33) with 1 output partitions\n",
            "25/08/06 07:22:43 INFO DAGScheduler: Final stage: ResultStage 0 (show at CustomerIngestion.java:33)\n",
            "25/08/06 07:22:43 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:22:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:22:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at CustomerIngestion.java:33), which has no missing parents\n",
            "25/08/06 07:22:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:22:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:22:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:33937 (size: 8.3 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:22:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:22:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at CustomerIngestion.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:22:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:22:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:22:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:22:44 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:22:44 INFO CodeGenerator: Code generated in 105.846053 ms\n",
            "25/08/06 07:22:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4902 bytes result sent to driver\n",
            "25/08/06 07:22:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 991 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:22:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:22:45 INFO DAGScheduler: ResultStage 0 (show at CustomerIngestion.java:33) finished in 1.558 s\n",
            "25/08/06 07:22:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:22:45 INFO DAGScheduler: Job 0 finished: show at CustomerIngestion.java:33, took 1.802241 s\n",
            "25/08/06 07:22:45 INFO CodeGenerator: Code generated in 137.848396 ms\n",
            "+--------------+--------------------+---------------+----------------+-----------------+--------------------+--------------------+-------------+--------+----------+---------+----------------------+-----------+\n",
            "|customerNumber|        customerName|contactLastName|contactFirstName|            phone|        addressLine1|        addressLine2|         city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|\n",
            "+--------------+--------------------+---------------+----------------+-----------------+--------------------+--------------------+-------------+--------+----------+---------+----------------------+-----------+\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |       40.32.2555|      54, rue Royale|                null|       Nantes|    null|     44000|   France|                  null|    21000.0|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|       7025551838|     8489 Strong St.|                null|    Las Vegas|      NV|     83030|      USA|                  null|    71800.0|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|     03 9520 4555|   636 St Kilda Road|             Level 3|    Melbourne|Victoria|      3004|Australia|                  null|   117300.0|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |       40.67.8555|67, rue des Cinqu...|                null|       Nantes|    null|     44000|   France|                  null|   118200.0|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |       07-98 9555|Erling Skakkes ga...|                null|      Stavern|    null|      4110|   Norway|                  null|    81700.0|\n",
            "|           124|Mini Gifts Distri...|         Nelson|           Susan|       4155551450|     5677 Strong St.|                null|   San Rafael|      CA|     97562|      USA|                  null|   210500.0|\n",
            "|           125|  Havel & Zbyszek Co|Piestrzeniewicz|        Zbyszek |    (26) 642-7555|     ul. Filtrowa 68|                null|     Warszawa|    null|    01-012|   Poland|                  null|        0.0|\n",
            "|           128|Blauer See Auto, Co.|         Keitel|          Roland|+49 69 66 90 2555|       Lyonerstr. 34|                null|    Frankfurt|    null|     60528|  Germany|                  null|    59700.0|\n",
            "|           129|     Mini Wheels Co.|         Murphy|           Julie|       6505555787|5557 North Pendal...|                null|San Francisco|      CA|     94217|      USA|                  null|    64600.0|\n",
            "|           131|   Land of Toys Inc.|            Lee|            Kwai|       2125557818|897 Long Airport ...|                null|          NYC|      NY|     10022|      USA|                  null|   114900.0|\n",
            "|           141|Euro+ Shopping Ch...|         Freyre|          Diego |   (91) 555 94 44|  C/ Moralzarzal, 86|                null|       Madrid|    null|     28034|    Spain|                  null|   227600.0|\n",
            "|           144|Volvo Model Repli...|       Berglund|      Christina |     0921-12 3555|     Berguvsvägen  8|                null|        Luleå|    null|  S-958 22|   Sweden|                  null|    53100.0|\n",
            "|           145|Danish Wholesale ...|       Petersen|          Jytte |       31 12 3555|        Vinbæltet 34|                null|    Kobenhavn|    null|      1734|  Denmark|                  null|    83400.0|\n",
            "|           146|Saveley & Henriot...|        Saveley|           Mary |       78.32.5555|  2, rue du Commerce|                null|         Lyon|    null|     69004|   France|                  null|   123900.0|\n",
            "|           148|Dragon Souveniers...|      Natividad|            Eric|     +65 221 7555|          Bronz Sok.|Bronz Apt. 3/6 Te...|    Singapore|    null|    079903|Singapore|                  null|   103800.0|\n",
            "|           151|  Muscle Machine Inc|          Young|            Jeff|       2125557413|   4092 Furth Circle|           Suite 400|          NYC|      NY|     10022|      USA|                  null|   138500.0|\n",
            "|           157|Diecast Classics ...|          Leong|          Kelvin|       2155551555|    7586 Pompton St.|                null|    Allentown|      PA|     70267|      USA|                  null|   100600.0|\n",
            "|           161|Technics Stores Inc.|      Hashimoto|            Juri|       6505556809|   9408 Furth Circle|                null|   Burlingame|      CA|     94217|      USA|                  null|    84600.0|\n",
            "|           166|    Handji Gifts& Co|      Victorino|           Wendy|     +65 224 1555|106 Linden Road S...|           2nd Floor|    Singapore|    null|    069045|Singapore|                  null|    97900.0|\n",
            "|           167|        Herkku Gifts|         Oeztan|          Veysel|    +47 2267 3215|     Brehmen St. 121|      PR 334 Sentrum|       Bergen|    null|    N 5804| Norway  |                  null|    96800.0|\n",
            "+--------------+--------------------+---------------+----------------+-----------------+--------------------+--------------------+-------------+--------+----------+---------+----------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- customerNumber: integer (nullable = true)\n",
            " |-- customerName: string (nullable = true)\n",
            " |-- contactLastName: string (nullable = true)\n",
            " |-- contactFirstName: string (nullable = true)\n",
            " |-- phone: string (nullable = true)\n",
            " |-- addressLine1: string (nullable = true)\n",
            " |-- addressLine2: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- postalCode: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- salesRepEmployeeNumber: integer (nullable = true)\n",
            " |-- creditLimit: double (nullable = true)\n",
            "\n",
            "25/08/06 07:22:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:22:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:22:46 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:22:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:22:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:22:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:33937 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:22:46 INFO SparkContext: Created broadcast 2 from parquet at CustomerIngestion.java:38\n",
            "25/08/06 07:22:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:22:46 INFO SparkContext: Starting job: parquet at CustomerIngestion.java:38\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Got job 1 (parquet at CustomerIngestion.java:38) with 1 output partitions\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at CustomerIngestion.java:38)\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at CustomerIngestion.java:38), which has no missing parents\n",
            "25/08/06 07:22:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 213.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:22:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 77.0 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:22:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:33937 (size: 77.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:22:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:22:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at CustomerIngestion.java:38) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:22:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:22:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7914 bytes) \n",
            "25/08/06 07:22:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:22:46 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:22:46 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:22:46 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:22:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional int32 salesRepEmployeeNumber;\n",
            "  optional double creditLimit;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:22:46 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:22:46 INFO FileScanRDD: Reading File path: file:///content/data/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 07:22:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:33937 in memory (size: 8.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:22:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:33937 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:22:47 INFO FileOutputCommitter: Saved output of task 'attempt_202508060722463072206330918116994_0001_m_000000_1' to file:/content/data/parquet/customers.parquet/_temporary/0/task_202508060722463072206330918116994_0001_m_000000\n",
            "25/08/06 07:22:47 INFO SparkHadoopMapRedUtil: attempt_202508060722463072206330918116994_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:22:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:22:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1209 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:22:47 INFO DAGScheduler: ResultStage 1 (parquet at CustomerIngestion.java:38) finished in 1.294 s\n",
            "25/08/06 07:22:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:22:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:22:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:22:47 INFO DAGScheduler: Job 1 finished: parquet at CustomerIngestion.java:38, took 1.312110 s\n",
            "25/08/06 07:22:47 INFO FileFormatWriter: Start to commit write Job 9e7d5b94-fc0a-46e4-9ec8-00828e8b1e2e.\n",
            "25/08/06 07:22:47 INFO FileFormatWriter: Write Job 9e7d5b94-fc0a-46e4-9ec8-00828e8b1e2e committed. Elapsed time: 21 ms.\n",
            "25/08/06 07:22:47 INFO FileFormatWriter: Finished processing stats for write job 9e7d5b94-fc0a-46e4-9ec8-00828e8b1e2e.\n",
            "✅ Customers Parquet File Created Successfully.\n",
            "25/08/06 07:22:47 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:22:47 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:22:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:22:47 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:22:47 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:22:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:22:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:22:47 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:22:47 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:22:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ae51ecd-0aae-49d2-9f99-9c4e66ac3631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrderIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class OrderIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Order Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"orderDate\", DataTypes.StringType, true)\n",
        "                .add(\"requiredDate\", DataTypes.StringType, true)\n",
        "                .add(\"shippedDate\", DataTypes.StringType, true)\n",
        "                .add(\"status\", DataTypes.StringType, true)\n",
        "                .add(\"comments\", DataTypes.StringType, true)\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/orders.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/orders.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Orders Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Slte3AQoM0",
        "outputId": "d43b3e23-dc4e-4dbe-b9b1-bc09f7b30c42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OrderIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" OrderIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" OrderIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWZA1I9lQxPU",
        "outputId": "899a15f8-1b99-42f5-d44f-9ee38052bc59"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:23:17 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:23:17 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:23:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:23:17 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:23:17 INFO SparkContext: Submitted application: Order Ingestion\n",
            "25/08/06 07:23:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:23:17 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:23:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:23:17 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:23:17 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:23:17 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:23:17 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:23:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:23:18 INFO Utils: Successfully started service 'sparkDriver' on port 41869.\n",
            "25/08/06 07:23:18 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:23:18 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:23:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:23:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:23:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:23:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eb692562-01cd-42ac-9de3-d9bd3296efb0\n",
            "25/08/06 07:23:18 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:23:18 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:23:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:23:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:23:19 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:23:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:23:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39373.\n",
            "25/08/06 07:23:19 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:39373\n",
            "25/08/06 07:23:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:23:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 39373, None)\n",
            "25/08/06 07:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:39373 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 39373, None)\n",
            "25/08/06 07:23:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 39373, None)\n",
            "25/08/06 07:23:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 39373, None)\n",
            "25/08/06 07:23:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:23:20 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:23:22 INFO InMemoryFileIndex: It took 84 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:23:26 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:23:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:23:27 INFO CodeGenerator: Code generated in 442.07056 ms\n",
            "25/08/06 07:23:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:23:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:23:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:39373 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:23:27 INFO SparkContext: Created broadcast 0 from show at OrderIngestion.java:27\n",
            "25/08/06 07:23:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:23:28 INFO SparkContext: Starting job: show at OrderIngestion.java:27\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Got job 0 (show at OrderIngestion.java:27) with 1 output partitions\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Final stage: ResultStage 0 (show at OrderIngestion.java:27)\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at OrderIngestion.java:27), which has no missing parents\n",
            "25/08/06 07:23:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:23:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:23:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:39373 (size: 7.6 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:23:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:23:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at OrderIngestion.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:23:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:23:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) \n",
            "25/08/06 07:23:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:23:28 INFO FileScanRDD: Reading File path: file:///content/data/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 07:23:29 INFO CodeGenerator: Code generated in 65.033767 ms\n",
            "25/08/06 07:23:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2656 bytes result sent to driver\n",
            "25/08/06 07:23:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 608 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:23:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:23:29 INFO DAGScheduler: ResultStage 0 (show at OrderIngestion.java:27) finished in 0.915 s\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Job 0 finished: show at OrderIngestion.java:27, took 1.028016 s\n",
            "25/08/06 07:23:29 INFO CodeGenerator: Code generated in 70.640106 ms\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                null|           363|\n",
            "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
            "|      10102|2003-01-10|  2003-01-18| 2003-01-14|Shipped|                null|           181|\n",
            "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
            "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
            "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
            "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
            "|      10107|2003-02-24|  2003-03-03| 2003-02-26|Shipped|Difficult to nego...|           131|\n",
            "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
            "|      10109|2003-03-10|  2003-03-19| 2003-03-11|Shipped|Customer requeste...|           486|\n",
            "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
            "|      10111|2003-03-25|  2003-03-31| 2003-03-30|Shipped|                null|           129|\n",
            "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
            "|      10113|2003-03-26|  2003-04-02| 2003-03-27|Shipped|                null|           124|\n",
            "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
            "|      10115|2003-04-04|  2003-04-12| 2003-04-07|Shipped|                null|           424|\n",
            "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
            "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
            "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
            "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- orderNumber: integer (nullable = true)\n",
            " |-- orderDate: string (nullable = true)\n",
            " |-- requiredDate: string (nullable = true)\n",
            " |-- shippedDate: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- comments: string (nullable = true)\n",
            " |-- customerNumber: integer (nullable = true)\n",
            "\n",
            "25/08/06 07:23:29 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:23:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:23:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:23:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:23:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:23:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:39373 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:23:29 INFO SparkContext: Created broadcast 2 from parquet at OrderIngestion.java:32\n",
            "25/08/06 07:23:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:23:29 INFO SparkContext: Starting job: parquet at OrderIngestion.java:32\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Got job 1 (parquet at OrderIngestion.java:32) with 1 output partitions\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OrderIngestion.java:32)\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at OrderIngestion.java:32), which has no missing parents\n",
            "25/08/06 07:23:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 211.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:23:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:23:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:39373 (size: 76.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:23:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at OrderIngestion.java:32) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:23:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:23:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7911 bytes) \n",
            "25/08/06 07:23:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:23:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:23:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:23:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:23:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:23:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:23:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:23:30 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:23:30 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:23:30 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:23:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"requiredDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"shippedDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"status\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"comments\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary orderDate (STRING);\n",
            "  optional binary requiredDate (STRING);\n",
            "  optional binary shippedDate (STRING);\n",
            "  optional binary status (STRING);\n",
            "  optional binary comments (STRING);\n",
            "  optional int32 customerNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:23:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:23:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:39373 in memory (size: 7.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:23:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:39373 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:23:31 INFO FileScanRDD: Reading File path: file:///content/data/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 07:23:31 INFO FileOutputCommitter: Saved output of task 'attempt_202508060723293870144132278054566_0001_m_000000_1' to file:/content/data/parquet/orders.parquet/_temporary/0/task_202508060723293870144132278054566_0001_m_000000\n",
            "25/08/06 07:23:31 INFO SparkHadoopMapRedUtil: attempt_202508060723293870144132278054566_0001_m_000000_1: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:23:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:23:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1683 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:23:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:23:31 INFO DAGScheduler: ResultStage 1 (parquet at OrderIngestion.java:32) finished in 1.794 s\n",
            "25/08/06 07:23:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:23:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:23:31 INFO DAGScheduler: Job 1 finished: parquet at OrderIngestion.java:32, took 1.808119 s\n",
            "25/08/06 07:23:31 INFO FileFormatWriter: Start to commit write Job 5eb7506e-c3fe-4a16-acfb-792cd530c09f.\n",
            "25/08/06 07:23:31 INFO FileFormatWriter: Write Job 5eb7506e-c3fe-4a16-acfb-792cd530c09f committed. Elapsed time: 30 ms.\n",
            "25/08/06 07:23:31 INFO FileFormatWriter: Finished processing stats for write job 5eb7506e-c3fe-4a16-acfb-792cd530c09f.\n",
            "✅ Orders Parquet File Created Successfully.\n",
            "25/08/06 07:23:31 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:23:31 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:23:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:23:31 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:23:31 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:23:31 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:23:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:23:31 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:23:31 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:23:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-8aa1573e-7bbf-452b-9170-ab344d3c57e9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OrderDetailIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class OrderDetailIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Order Detail Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType, true)\n",
        "                .add(\"productCode\", DataTypes.StringType, true)\n",
        "                .add(\"quantityOrdered\", DataTypes.IntegerType, true)\n",
        "                .add(\"priceEach\", DataTypes.DoubleType, true)\n",
        "                .add(\"orderLineNumber\", DataTypes.IntegerType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/orderdetails.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ OrderDetails Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMY7-wimQzgW",
        "outputId": "44eb52a1-7a64-40bc-a16f-d3087c4e4fc1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OrderDetailIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" OrderDetailIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" OrderDetailIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83dY4AcrQ52t",
        "outputId": "a4f2bcd1-6a9f-4f03-f84d-bd80bcb2d71b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:23:52 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:23:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:23:52 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:23:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:23:52 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:23:52 INFO SparkContext: Submitted application: Order Detail Ingestion\n",
            "25/08/06 07:23:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:23:53 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:23:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:23:53 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:23:53 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:23:53 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:23:53 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:23:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:23:53 INFO Utils: Successfully started service 'sparkDriver' on port 38733.\n",
            "25/08/06 07:23:53 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:23:53 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:23:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:23:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:23:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:23:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ff37e392-6ebd-4770-b5ac-4456180299a5\n",
            "25/08/06 07:23:53 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:23:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:23:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:23:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:23:54 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:23:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:23:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37811.\n",
            "25/08/06 07:23:54 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:37811\n",
            "25/08/06 07:23:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:23:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 37811, None)\n",
            "25/08/06 07:23:54 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:37811 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 37811, None)\n",
            "25/08/06 07:23:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 37811, None)\n",
            "25/08/06 07:23:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 37811, None)\n",
            "25/08/06 07:23:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:23:55 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:23:57 INFO InMemoryFileIndex: It took 171 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:24:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:24:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:24:02 INFO CodeGenerator: Code generated in 534.826138 ms\n",
            "25/08/06 07:24:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:37811 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:24:03 INFO SparkContext: Created broadcast 0 from show at OrderDetailIngestion.java:25\n",
            "25/08/06 07:24:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:24:03 INFO SparkContext: Starting job: show at OrderDetailIngestion.java:25\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Got job 0 (show at OrderDetailIngestion.java:25) with 1 output partitions\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Final stage: ResultStage 0 (show at OrderDetailIngestion.java:25)\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at OrderDetailIngestion.java:25), which has no missing parents\n",
            "25/08/06 07:24:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:37811 (size: 7.6 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:24:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:24:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at OrderDetailIngestion.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:24:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:24:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:24:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:24:04 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:24:04 INFO CodeGenerator: Code generated in 27.230028 ms\n",
            "25/08/06 07:24:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2195 bytes result sent to driver\n",
            "25/08/06 07:24:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 511 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:24:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:24:04 INFO DAGScheduler: ResultStage 0 (show at OrderDetailIngestion.java:25) finished in 0.793 s\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:24:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Job 0 finished: show at OrderDetailIngestion.java:25, took 0.894284 s\n",
            "25/08/06 07:24:04 INFO CodeGenerator: Code generated in 78.691288 ms\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|orderNumber|productCode|quantityOrdered|priceEach|orderLineNumber|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|      10100|   S18_1749|             30|    136.0|              3|\n",
            "|      10100|   S18_2248|             50|    55.09|              2|\n",
            "|      10100|   S18_4409|             22|    75.46|              4|\n",
            "|      10100|   S24_3969|             49|    35.29|              1|\n",
            "|      10101|   S18_2325|             25|   108.06|              4|\n",
            "|      10101|   S18_2795|             26|   167.06|              1|\n",
            "|      10101|   S24_1937|             45|    32.53|              3|\n",
            "|      10101|   S24_2022|             46|    44.35|              2|\n",
            "|      10102|   S18_1342|             39|    95.55|              2|\n",
            "|      10102|   S18_1367|             41|    43.13|              1|\n",
            "|      10103|   S10_1949|             26|    214.3|             11|\n",
            "|      10103|   S10_4962|             42|   119.67|              4|\n",
            "|      10103|   S12_1666|             27|   121.64|              8|\n",
            "|      10103|   S18_1097|             35|     94.5|             10|\n",
            "|      10103|   S18_2432|             22|    58.34|              2|\n",
            "|      10103|   S18_2949|             27|    92.19|             12|\n",
            "|      10103|   S18_2957|             35|    61.84|             14|\n",
            "|      10103|   S18_3136|             25|    86.92|             13|\n",
            "|      10103|   S18_3320|             46|    86.31|             16|\n",
            "|      10103|   S18_4600|             36|    98.07|              5|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- orderNumber: integer (nullable = true)\n",
            " |-- productCode: string (nullable = true)\n",
            " |-- quantityOrdered: integer (nullable = true)\n",
            " |-- priceEach: double (nullable = true)\n",
            " |-- orderLineNumber: integer (nullable = true)\n",
            "\n",
            "25/08/06 07:24:04 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:24:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:24:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:24:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:24:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:37811 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:04 INFO SparkContext: Created broadcast 2 from parquet at OrderDetailIngestion.java:30\n",
            "25/08/06 07:24:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:24:04 INFO SparkContext: Starting job: parquet at OrderDetailIngestion.java:30\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Got job 1 (parquet at OrderDetailIngestion.java:30) with 1 output partitions\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OrderDetailIngestion.java:30)\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at OrderDetailIngestion.java:30), which has no missing parents\n",
            "25/08/06 07:24:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 211.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:24:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:24:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:37811 (size: 76.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:24:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at OrderDetailIngestion.java:30) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:24:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:24:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:24:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:05 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:24:05 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:24:05 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:24:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "  optional double priceEach;\n",
            "  optional int32 orderLineNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:24:05 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:24:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:37811 in memory (size: 7.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:37811 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:05 INFO FileScanRDD: Reading File path: file:///content/data/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 07:24:06 INFO FileOutputCommitter: Saved output of task 'attempt_202508060724046441055545974010735_0001_m_000000_1' to file:/content/data/parquet/orderdetails.parquet/_temporary/0/task_202508060724046441055545974010735_0001_m_000000\n",
            "25/08/06 07:24:06 INFO SparkHadoopMapRedUtil: attempt_202508060724046441055545974010735_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:24:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:24:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1642 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:24:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:24:06 INFO DAGScheduler: ResultStage 1 (parquet at OrderDetailIngestion.java:30) finished in 1.732 s\n",
            "25/08/06 07:24:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:24:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:24:06 INFO DAGScheduler: Job 1 finished: parquet at OrderDetailIngestion.java:30, took 1.741706 s\n",
            "25/08/06 07:24:06 INFO FileFormatWriter: Start to commit write Job a457842b-ee43-4f84-92c9-61ee1358eae5.\n",
            "25/08/06 07:24:06 INFO FileFormatWriter: Write Job a457842b-ee43-4f84-92c9-61ee1358eae5 committed. Elapsed time: 17 ms.\n",
            "25/08/06 07:24:06 INFO FileFormatWriter: Finished processing stats for write job a457842b-ee43-4f84-92c9-61ee1358eae5.\n",
            "✅ OrderDetails Parquet File Created Successfully.\n",
            "25/08/06 07:24:06 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:24:06 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:24:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:24:06 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:24:06 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:24:06 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:24:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:24:06 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:24:06 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:24:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-210d353f-6346-463b-9f3a-1c11f44febcb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class ProductIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Product Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"productCode\", DataTypes.StringType, true)\n",
        "                .add(\"productName\", DataTypes.StringType, true)\n",
        "                .add(\"productLine\", DataTypes.StringType, true)\n",
        "                .add(\"productScale\", DataTypes.StringType, true)\n",
        "                .add(\"productVendor\", DataTypes.StringType, true)\n",
        "                .add(\"productDescription\", DataTypes.StringType, true)\n",
        "                .add(\"quantityInStock\", DataTypes.IntegerType, true)\n",
        "                .add(\"buyPrice\", DataTypes.DoubleType, true)\n",
        "                .add(\"MSRP\", DataTypes.DoubleType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/products.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/products.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Products Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RraS9o3IQ7qF",
        "outputId": "df3df3c4-31d0-4023-c695-ec602a2bc896"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ProductIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" ProductIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" ProductIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HRgxtz8RE0I",
        "outputId": "7e2735c2-7979-4a06-9710-1663235f572f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:24:37 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:24:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:24:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:24:38 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:24:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:24:38 INFO SparkContext: Submitted application: Product Ingestion\n",
            "25/08/06 07:24:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:24:38 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:24:38 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:24:38 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:24:38 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:24:38 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:24:38 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:24:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:24:38 INFO Utils: Successfully started service 'sparkDriver' on port 33047.\n",
            "25/08/06 07:24:38 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:24:38 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:24:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:24:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:24:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:24:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-026edd70-d4ea-41f6-a914-ea55a9bc39b7\n",
            "25/08/06 07:24:39 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:24:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:24:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:24:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:24:39 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:24:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:24:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41489.\n",
            "25/08/06 07:24:39 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:41489\n",
            "25/08/06 07:24:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:24:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 41489, None)\n",
            "25/08/06 07:24:39 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:41489 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 41489, None)\n",
            "25/08/06 07:24:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 41489, None)\n",
            "25/08/06 07:24:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 41489, None)\n",
            "25/08/06 07:24:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:24:40 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:24:42 INFO InMemoryFileIndex: It took 126 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:24:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:24:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:24:47 INFO CodeGenerator: Code generated in 610.412987 ms\n",
            "25/08/06 07:24:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:41489 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:24:48 INFO SparkContext: Created broadcast 0 from show at ProductIngestion.java:29\n",
            "25/08/06 07:24:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:24:48 INFO SparkContext: Starting job: show at ProductIngestion.java:29\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Got job 0 (show at ProductIngestion.java:29) with 1 output partitions\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Final stage: ResultStage 0 (show at ProductIngestion.java:29)\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at ProductIngestion.java:29), which has no missing parents\n",
            "25/08/06 07:24:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.3 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:24:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:24:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:41489 (size: 7.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:24:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:24:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at ProductIngestion.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:24:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:24:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:24:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:24:49 INFO FileScanRDD: Reading File path: file:///content/data/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 07:24:49 INFO CodeGenerator: Code generated in 81.054052 ms\n",
            "25/08/06 07:24:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5861 bytes result sent to driver\n",
            "25/08/06 07:24:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 936 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:24:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:24:49 INFO DAGScheduler: ResultStage 0 (show at ProductIngestion.java:29) finished in 1.221 s\n",
            "25/08/06 07:24:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:24:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:24:49 INFO DAGScheduler: Job 0 finished: show at ProductIngestion.java:29, took 1.315571 s\n",
            "25/08/06 07:24:50 INFO CodeGenerator: Code generated in 63.13139 ms\n",
            "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
            "|productCode|         productName|     productLine|productScale|       productVendor|  productDescription|quantityInStock|buyPrice|  MSRP|\n",
            "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
            "|   S10_1678|1969 Harley David...|     Motorcycles|        1:10|     Min Lin Diecast|This replica feat...|           7933|   48.81|  95.7|\n",
            "|   S10_1949|1952 Alpine Renau...|    Classic Cars|        1:10|Classic Metal Cre...|Turnable front wh...|           7305|   98.58| 214.3|\n",
            "|   S10_2016|1996 Moto Guzzi 1...|     Motorcycles|        1:10|Highway 66 Mini C...|Official Moto Guz...|           6625|   68.99|118.94|\n",
            "|   S10_4698|2003 Harley-David...|     Motorcycles|        1:10|   Red Start Diecast|Model features, o...|           5582|   91.02|193.66|\n",
            "|   S10_4757| 1972 Alfa Romeo GTA|    Classic Cars|        1:10|Motor City Art Cl...|Features include:...|           3252|   85.68| 136.0|\n",
            "|   S10_4962|1962 LanciaA Delt...|    Classic Cars|        1:10| Second Gear Diecast|Features include:...|           6791|  103.42|147.74|\n",
            "|   S12_1099|   1968 Ford Mustang|    Classic Cars|        1:12|Autoart Studio De...|Hood, doors and t...|             68|   95.34|194.57|\n",
            "|   S12_1108|   2001 Ferrari Enzo|    Classic Cars|        1:12| Second Gear Diecast|Turnable front wh...|           3619|   95.59| 207.8|\n",
            "|   S12_1666|      1958 Setra Bus|Trucks and Buses|        1:12|Welly Diecast Pro...|Model features 30...|           1579|    77.9|136.67|\n",
            "|   S12_2823|    2002 Suzuki XREO|     Motorcycles|        1:12|Unimax Art Galleries|Official logos an...|           9997|   66.27|150.62|\n",
            "|   S12_3148|  1969 Corvair Monza|    Classic Cars|        1:18|Welly Diecast Pro...|\"1:18 scale die-c...|           null|    null|6906.0|\n",
            "|   S12_3380|  1968 Dodge Charger|    Classic Cars|        1:12|Welly Diecast Pro...|1:12 scale model ...|           9123|   75.16|117.44|\n",
            "|   S12_3891|    1969 Ford Falcon|    Classic Cars|        1:12| Second Gear Diecast|Turnable front wh...|           1049|   83.05|173.02|\n",
            "|   S12_3990|1970 Plymouth Hem...|    Classic Cars|        1:12| Studio M Art Models|Very detailed 197...|           5663|   31.92|  79.8|\n",
            "|   S12_4473|   1957 Chevy Pickup|Trucks and Buses|        1:12|       Exoto Designs|\"1:12 scale die-c...|           null|  6125.0|  55.7|\n",
            "|   S12_4675|  1969 Dodge Charger|    Classic Cars|        1:12|Welly Diecast Pro...|Detailed model of...|           7323|   58.73|115.16|\n",
            "|   S18_1097|1940 Ford Pickup ...|Trucks and Buses|        1:18| Studio M Art Models|This model featur...|           2613|   58.33|116.67|\n",
            "|   S18_1129|     1993 Mazda RX-7|    Classic Cars|        1:18|Highway 66 Mini C...|This model featur...|           3975|   83.51|141.54|\n",
            "|   S18_1342|1937 Lincoln Berline|    Vintage Cars|        1:18|Motor City Art Cl...|Features opening ...|           8693|   60.62|102.74|\n",
            "|   S18_1367|1936 Mercedes-Ben...|    Vintage Cars|        1:18| Studio M Art Models|This 1:18 scale r...|           8635|   24.26| 53.91|\n",
            "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- productCode: string (nullable = true)\n",
            " |-- productName: string (nullable = true)\n",
            " |-- productLine: string (nullable = true)\n",
            " |-- productScale: string (nullable = true)\n",
            " |-- productVendor: string (nullable = true)\n",
            " |-- productDescription: string (nullable = true)\n",
            " |-- quantityInStock: integer (nullable = true)\n",
            " |-- buyPrice: double (nullable = true)\n",
            " |-- MSRP: double (nullable = true)\n",
            "\n",
            "25/08/06 07:24:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:24:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:24:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:24:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:24:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:41489 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:50 INFO SparkContext: Created broadcast 2 from parquet at ProductIngestion.java:34\n",
            "25/08/06 07:24:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:24:50 INFO SparkContext: Starting job: parquet at ProductIngestion.java:34\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Got job 1 (parquet at ProductIngestion.java:34) with 1 output partitions\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at ProductIngestion.java:34)\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at ProductIngestion.java:34), which has no missing parents\n",
            "25/08/06 07:24:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 212.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:24:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.7 KiB, free 1766.8 MiB)\n",
            "25/08/06 07:24:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:41489 (size: 76.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:24:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at ProductIngestion.java:34) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:24:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:24:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes) \n",
            "25/08/06 07:24:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:24:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:24:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:24:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:24:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:24:50 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:24:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productScale\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productVendor\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityInStock\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"buyPrice\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MSRP\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary productScale (STRING);\n",
            "  optional binary productVendor (STRING);\n",
            "  optional binary productDescription (STRING);\n",
            "  optional int32 quantityInStock;\n",
            "  optional double buyPrice;\n",
            "  optional double MSRP;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:24:50 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:24:51 INFO FileScanRDD: Reading File path: file:///content/data/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 07:24:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:41489 in memory (size: 7.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:41489 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:24:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060724507996308674974140453_0001_m_000000_1' to file:/content/data/parquet/products.parquet/_temporary/0/task_202508060724507996308674974140453_0001_m_000000\n",
            "25/08/06 07:24:51 INFO SparkHadoopMapRedUtil: attempt_202508060724507996308674974140453_0001_m_000000_1: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 07:24:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:24:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1383 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:24:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:24:51 INFO DAGScheduler: ResultStage 1 (parquet at ProductIngestion.java:34) finished in 1.455 s\n",
            "25/08/06 07:24:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:24:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:24:51 INFO DAGScheduler: Job 1 finished: parquet at ProductIngestion.java:34, took 1.467665 s\n",
            "25/08/06 07:24:51 INFO FileFormatWriter: Start to commit write Job 18a03ddb-594d-4cce-808c-c111ca7bdf5d.\n",
            "25/08/06 07:24:51 INFO FileFormatWriter: Write Job 18a03ddb-594d-4cce-808c-c111ca7bdf5d committed. Elapsed time: 26 ms.\n",
            "25/08/06 07:24:51 INFO FileFormatWriter: Finished processing stats for write job 18a03ddb-594d-4cce-808c-c111ca7bdf5d.\n",
            "✅ Products Parquet File Created Successfully.\n",
            "25/08/06 07:24:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:24:52 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:24:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:24:52 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:24:52 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:24:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:24:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:24:52 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:24:52 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:24:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce070450-2ed9-4746-ace3-759a6387c340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductLineIngestion.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class ProductLineIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"ProductLine Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType, true)\n",
        "                .add(\"textDescription\", DataTypes.StringType, true);\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(\"file:///content/data/productlines.csv\");\n",
        "\n",
        "        df.show();\n",
        "        df.printSchema();\n",
        "\n",
        "        df.write()\n",
        "          .mode(\"overwrite\")\n",
        "          .parquet(\"file:///content/data/parquet/productlines.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ ProductLines Parquet File Created Successfully.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZs4XoJ1RG2m",
        "outputId": "ebd686dd-e2e6-4de0-d76c-e14891820b04"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ProductLineIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" ProductLineIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" ProductLineIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUinyNKRROGD",
        "outputId": "4d95bf14-c8d5-4381-94c4-c402ad72afe2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:25:29 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:25:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:25:30 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:25:30 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:25:30 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:25:30 INFO SparkContext: Submitted application: ProductLine Ingestion\n",
            "25/08/06 07:25:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:25:30 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:25:30 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:25:30 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:25:30 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:25:30 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:25:30 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:25:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:25:30 INFO Utils: Successfully started service 'sparkDriver' on port 33549.\n",
            "25/08/06 07:25:30 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:25:30 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:25:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:25:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:25:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:25:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-300fa5f1-04af-4a62-94f0-ff6aeb3a08ad\n",
            "25/08/06 07:25:31 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:25:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:25:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:25:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:25:31 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:25:31 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:25:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41857.\n",
            "25/08/06 07:25:31 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:41857\n",
            "25/08/06 07:25:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:25:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 41857, None)\n",
            "25/08/06 07:25:31 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:41857 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 41857, None)\n",
            "25/08/06 07:25:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 41857, None)\n",
            "25/08/06 07:25:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 41857, None)\n",
            "25/08/06 07:25:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:25:32 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:25:34 INFO InMemoryFileIndex: It took 169 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:25:38 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:25:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:25:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:25:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:25:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:41857 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:25:39 INFO SparkContext: Created broadcast 0 from show at ProductLineIngestion.java:22\n",
            "25/08/06 07:25:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:25:39 INFO SparkContext: Starting job: show at ProductLineIngestion.java:22\n",
            "25/08/06 07:25:39 INFO DAGScheduler: Got job 0 (show at ProductLineIngestion.java:22) with 1 output partitions\n",
            "25/08/06 07:25:39 INFO DAGScheduler: Final stage: ResultStage 0 (show at ProductLineIngestion.java:22)\n",
            "25/08/06 07:25:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:25:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:25:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at ProductLineIngestion.java:22), which has no missing parents\n",
            "25/08/06 07:25:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:25:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:25:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:41857 (size: 5.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:25:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:25:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at ProductLineIngestion.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:25:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:25:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:25:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:25:40 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:25:40 INFO CodeGenerator: Code generated in 396.090778 ms\n",
            "25/08/06 07:25:41 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/data/productlines.csv\n",
            "25/08/06 07:25:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3594 bytes result sent to driver\n",
            "25/08/06 07:25:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1127 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:25:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:25:41 INFO DAGScheduler: ResultStage 0 (show at ProductLineIngestion.java:22) finished in 1.413 s\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Job 0 finished: show at ProductLineIngestion.java:22, took 1.539362 s\n",
            "25/08/06 07:25:41 INFO CodeGenerator: Code generated in 53.30096 ms\n",
            "+----------------+--------------------+\n",
            "|     productLine|     textDescription|\n",
            "+----------------+--------------------+\n",
            "|    Classic Cars|Attention car ent...|\n",
            "|     Motorcycles|Our motorcycles a...|\n",
            "|          Planes|Unique, diecast a...|\n",
            "|           Ships|The perfect holid...|\n",
            "|          Trains|Model trains are ...|\n",
            "|Trucks and Buses|The Truck and Bus...|\n",
            "|    Vintage Cars|Our Vintage Car m...|\n",
            "+----------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- productLine: string (nullable = true)\n",
            " |-- textDescription: string (nullable = true)\n",
            "\n",
            "25/08/06 07:25:41 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:25:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:25:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:25:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:25:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:25:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:25:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:25:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:25:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:25:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:25:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:25:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:41857 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:25:41 INFO SparkContext: Created broadcast 2 from parquet at ProductLineIngestion.java:27\n",
            "25/08/06 07:25:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:25:41 INFO SparkContext: Starting job: parquet at ProductLineIngestion.java:27\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Got job 1 (parquet at ProductLineIngestion.java:27) with 1 output partitions\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at ProductLineIngestion.java:27)\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:25:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at ProductLineIngestion.java:27), which has no missing parents\n",
            "25/08/06 07:25:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 210.4 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:25:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:25:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:41857 (size: 76.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:25:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:25:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at ProductLineIngestion.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:25:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:25:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 07:25:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:25:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:25:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:25:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:25:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:25:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:25:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:25:42 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:25:42 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:25:42 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:25:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:25:42 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:25:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:41857 in memory (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:25:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:41857 in memory (size: 5.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:25:42 INFO FileScanRDD: Reading File path: file:///content/data/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 07:25:42 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/data/productlines.csv\n",
            "25/08/06 07:25:43 INFO FileOutputCommitter: Saved output of task 'attempt_202508060725419219616786358251817_0001_m_000000_1' to file:/content/data/parquet/productlines.parquet/_temporary/0/task_202508060725419219616786358251817_0001_m_000000\n",
            "25/08/06 07:25:43 INFO SparkHadoopMapRedUtil: attempt_202508060725419219616786358251817_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:25:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver\n",
            "25/08/06 07:25:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1382 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:25:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:25:43 INFO DAGScheduler: ResultStage 1 (parquet at ProductLineIngestion.java:27) finished in 1.473 s\n",
            "25/08/06 07:25:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:25:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:25:43 INFO DAGScheduler: Job 1 finished: parquet at ProductLineIngestion.java:27, took 1.484330 s\n",
            "25/08/06 07:25:43 INFO FileFormatWriter: Start to commit write Job d92927cf-dae7-4485-b761-6726429bc3c2.\n",
            "25/08/06 07:25:43 INFO FileFormatWriter: Write Job d92927cf-dae7-4485-b761-6726429bc3c2 committed. Elapsed time: 23 ms.\n",
            "25/08/06 07:25:43 INFO FileFormatWriter: Finished processing stats for write job d92927cf-dae7-4485-b761-6726429bc3c2.\n",
            "✅ ProductLines Parquet File Created Successfully.\n",
            "25/08/06 07:25:43 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:25:43 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:25:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:25:43 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:25:43 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:25:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:25:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:25:43 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:25:43 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:25:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-f630c267-8782-413b-a247-1eb5270c9e0b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2.1: Top 10 Products by Total Quantity Sold"
      ],
      "metadata": {
        "id": "tjhkC-D-RdkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TopProductsByQuantity.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class TopProductsByQuantity {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Top 10 Products by Quantity Sold\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> products = spark.read().parquet(\"file:///content/data/parquet/products.parquet\");\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        Dataset<Row> joined = orderDetails.join(products, \"productCode\");\n",
        "\n",
        "        Dataset<Row> result = joined.groupBy(\"productName\")\n",
        "                .agg(sum(\"quantityOrdered\").alias(\"totalQuantity\"))\n",
        "                .orderBy(col(\"totalQuantity\").desc())\n",
        "                .limit(10);\n",
        "\n",
        "        result.show();\n",
        "\n",
        "        result.write()\n",
        "              .mode(\"overwrite\")\n",
        "              .parquet(\"file:///content/output/processed/top_10_products_by_quantity.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Top 10 Products by Quantity Parquet File Created.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB_mBRCiRT2F",
        "outputId": "0c338271-9198-42ef-cf0f-a145ad2a2987"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing TopProductsByQuantity.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" TopProductsByQuantity.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" TopProductsByQuantity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EpB88HpRjCO",
        "outputId": "d18a3e61-8c46-49e8-c128-993f0d153976"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:26:40 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:26:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:26:40 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:26:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:26:40 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:26:40 INFO SparkContext: Submitted application: Top 10 Products by Quantity Sold\n",
            "25/08/06 07:26:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:26:40 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:26:40 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:26:40 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:26:40 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:26:40 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:26:40 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:26:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:26:41 INFO Utils: Successfully started service 'sparkDriver' on port 41893.\n",
            "25/08/06 07:26:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:26:41 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:26:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:26:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:26:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:26:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59c4956b-e0a7-4891-a362-9e334f2bd310\n",
            "25/08/06 07:26:41 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:26:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:26:41 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:26:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:26:42 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:26:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:26:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32979.\n",
            "25/08/06 07:26:42 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:32979\n",
            "25/08/06 07:26:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:26:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 32979, None)\n",
            "25/08/06 07:26:42 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:32979 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 32979, None)\n",
            "25/08/06 07:26:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 32979, None)\n",
            "25/08/06 07:26:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 32979, None)\n",
            "25/08/06 07:26:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:26:43 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:26:44 INFO InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:26:45 INFO SparkContext: Starting job: parquet at TopProductsByQuantity.java:11\n",
            "25/08/06 07:26:45 INFO DAGScheduler: Got job 0 (parquet at TopProductsByQuantity.java:11) with 1 output partitions\n",
            "25/08/06 07:26:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at TopProductsByQuantity.java:11)\n",
            "25/08/06 07:26:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at TopProductsByQuantity.java:11), which has no missing parents\n",
            "25/08/06 07:26:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:26:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:26:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:32979 (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at TopProductsByQuantity.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes) \n",
            "25/08/06 07:26:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:26:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2225 bytes result sent to driver\n",
            "25/08/06 07:26:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1533 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:48 INFO DAGScheduler: ResultStage 0 (parquet at TopProductsByQuantity.java:11) finished in 2.030 s\n",
            "25/08/06 07:26:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:26:48 INFO DAGScheduler: Job 0 finished: parquet at TopProductsByQuantity.java:11, took 2.236521 s\n",
            "25/08/06 07:26:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:32979 in memory (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:50 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:26:50 INFO SparkContext: Starting job: parquet at TopProductsByQuantity.java:12\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Got job 1 (parquet at TopProductsByQuantity.java:12) with 1 output partitions\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at TopProductsByQuantity.java:12)\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at TopProductsByQuantity.java:12), which has no missing parents\n",
            "25/08/06 07:26:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:26:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:26:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:32979 (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at TopProductsByQuantity.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7573 bytes) \n",
            "25/08/06 07:26:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:26:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2013 bytes result sent to driver\n",
            "25/08/06 07:26:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 49 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:50 INFO DAGScheduler: ResultStage 1 (parquet at TopProductsByQuantity.java:12) finished in 0.107 s\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:26:50 INFO DAGScheduler: Job 1 finished: parquet at TopProductsByQuantity.java:12, took 0.116507 s\n",
            "25/08/06 07:26:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:32979 in memory (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:26:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#0)\n",
            "25/08/06 07:26:53 INFO CodeGenerator: Code generated in 475.939586 ms\n",
            "25/08/06 07:26:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:26:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:26:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:32979 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:53 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210466 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:26:53 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:26:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:26:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:26:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:32979 (size: 6.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 07:26:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:26:53 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet/part-00000-439af649-9bde-4bab-9e42-4830f2f72c31-c000.snappy.parquet, range: 0-16162, partition values: [empty row]\n",
            "25/08/06 07:26:53 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:26:54 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 07:26:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 5505 bytes result sent to driver\n",
            "25/08/06 07:26:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1173 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:54 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.203 s\n",
            "25/08/06 07:26:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:26:54 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.227351 s\n",
            "25/08/06 07:26:54 INFO CodeGenerator: Code generated in 16.683289 ms\n",
            "25/08/06 07:26:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 64.0 MiB, free 1703.3 MiB)\n",
            "25/08/06 07:26:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1703.3 MiB)\n",
            "25/08/06 07:26:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:32979 (size: 4.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:26:54 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:54 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:26:55 INFO CodeGenerator: Code generated in 208.661557 ms\n",
            "25/08/06 07:26:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:26:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:26:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:32979 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:26:55 INFO SparkContext: Created broadcast 5 from show at TopProductsByQuantity.java:21\n",
            "25/08/06 07:26:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Registering RDD 11 (show at TopProductsByQuantity.java:21) as input to shuffle 0\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Got map stage job 3 (show at TopProductsByQuantity.java:21) with 1 output partitions\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at TopProductsByQuantity.java:21)\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at show at TopProductsByQuantity.java:21), which has no missing parents\n",
            "25/08/06 07:26:55 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 48.0 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:26:55 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:26:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:32979 (size: 21.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:26:55 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at show at TopProductsByQuantity.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:26:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 07:26:55 INFO CodeGenerator: Code generated in 40.22223 ms\n",
            "25/08/06 07:26:55 INFO CodeGenerator: Code generated in 20.830852 ms\n",
            "25/08/06 07:26:55 INFO CodeGenerator: Code generated in 22.939383 ms\n",
            "25/08/06 07:26:55 INFO CodeGenerator: Code generated in 20.388885 ms\n",
            "25/08/06 07:26:55 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:26:55 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:26:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3926 bytes result sent to driver\n",
            "25/08/06 07:26:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 554 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:56 INFO DAGScheduler: ShuffleMapStage 3 (show at TopProductsByQuantity.java:21) finished in 0.600 s\n",
            "25/08/06 07:26:56 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:26:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:56 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:26:56 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:26:56 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:26:56 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:26:56 INFO CodeGenerator: Code generated in 28.353163 ms\n",
            "25/08/06 07:26:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:26:56 INFO CodeGenerator: Code generated in 40.176834 ms\n",
            "25/08/06 07:26:56 INFO SparkContext: Starting job: show at TopProductsByQuantity.java:21\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Got job 4 (show at TopProductsByQuantity.java:21) with 1 output partitions\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Final stage: ResultStage 5 (show at TopProductsByQuantity.java:21)\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at show at TopProductsByQuantity.java:21), which has no missing parents\n",
            "25/08/06 07:26:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 46.7 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:26:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:26:56 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:32979 (size: 21.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:26:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at show at TopProductsByQuantity.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:56 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:56 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:26:56 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 07:26:56 INFO ShuffleBlockFetcherIterator: Getting 1 (8.8 KiB) non-empty blocks including 1 (8.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:26:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/08/06 07:26:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 6971 bytes result sent to driver\n",
            "25/08/06 07:26:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 183 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:56 INFO DAGScheduler: ResultStage 5 (show at TopProductsByQuantity.java:21) finished in 0.210 s\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 07:26:56 INFO DAGScheduler: Job 4 finished: show at TopProductsByQuantity.java:21, took 0.259757 s\n",
            "25/08/06 07:26:56 INFO CodeGenerator: Code generated in 20.181831 ms\n",
            "25/08/06 07:26:56 INFO CodeGenerator: Code generated in 26.405796 ms\n",
            "+--------------------+-------------+\n",
            "|         productName|totalQuantity|\n",
            "+--------------------+-------------+\n",
            "|1992 Ferrari 360 ...|         1808|\n",
            "|1937 Lincoln Berline|         1111|\n",
            "|American Airlines...|         1085|\n",
            "|1941 Chevrolet Sp...|         1076|\n",
            "|1930 Buick Marque...|         1074|\n",
            "|    1940s Ford truck|         1061|\n",
            "|1969 Harley David...|         1057|\n",
            "|   1957 Chevy Pickup|         1056|\n",
            "|1964 Mercedes Tou...|         1053|\n",
            "|1956 Porsche 356A...|         1052|\n",
            "+--------------------+-------------+\n",
            "\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#0)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.2 KiB, free 1702.8 MiB)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:32979 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210466 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:26:57 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:32979 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 07:26:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 07:26:57 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet/part-00000-439af649-9bde-4bab-9e42-4830f2f72c31-c000.snappy.parquet, range: 0-16162, partition values: [empty row]\n",
            "25/08/06 07:26:57 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:26:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 5462 bytes result sent to driver\n",
            "25/08/06 07:26:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 60 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:57 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.076 s\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.087483 s\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 64.0 MiB, free 1638.7 MiB)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1638.7 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:32979 (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:26:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 200.2 KiB, free 1638.5 MiB)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1638.5 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:32979 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO SparkContext: Created broadcast 11 from parquet at TopProductsByQuantity.java:25\n",
            "25/08/06 07:26:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Registering RDD 23 (parquet at TopProductsByQuantity.java:25) as input to shuffle 1\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Got map stage job 6 (parquet at TopProductsByQuantity.java:25) with 1 output partitions\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (parquet at TopProductsByQuantity.java:25)\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at parquet at TopProductsByQuantity.java:25), which has no missing parents\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 48.1 KiB, free 1638.4 MiB)\n",
            "25/08/06 07:26:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1638.4 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:32979 (size: 21.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at parquet at TopProductsByQuantity.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:57 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:57 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:26:57 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24d0e2dab55f:32979 in memory (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:26:57 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:32979 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 24d0e2dab55f:32979 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 24d0e2dab55f:32979 in memory (size: 21.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:32979 in memory (size: 21.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 24d0e2dab55f:32979 in memory (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:26:57 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 3926 bytes result sent to driver\n",
            "25/08/06 07:26:57 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 343 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:57 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:57 INFO DAGScheduler: ShuffleMapStage 7 (parquet at TopProductsByQuantity.java:25) finished in 0.378 s\n",
            "25/08/06 07:26:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:26:57 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:26:57 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:26:57 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:26:57 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:32979 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:26:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:26:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:26:58 INFO SparkContext: Starting job: parquet at TopProductsByQuantity.java:25\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Got job 7 (parquet at TopProductsByQuantity.java:25) with 1 output partitions\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at TopProductsByQuantity.java:25)\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[27] at parquet at TopProductsByQuantity.java:25), which has no missing parents\n",
            "25/08/06 07:26:58 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 248.0 KiB, free 1702.8 MiB)\n",
            "25/08/06 07:26:58 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 92.0 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:26:58 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:32979 (size: 92.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:58 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at parquet at TopProductsByQuantity.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:26:58 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:26:58 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:26:58 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
            "25/08/06 07:26:58 INFO ShuffleBlockFetcherIterator: Getting 1 (8.8 KiB) non-empty blocks including 1 (8.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:26:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 07:26:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:26:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:26:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:26:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:26:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:26:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:26:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:26:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:26:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:26:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalQuantity\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional int64 totalQuantity;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:26:58 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:26:58 INFO FileOutputCommitter: Saved output of task 'attempt_202508060726583418709592025162556_0009_m_000000_7' to file:/content/output/processed/top_10_products_by_quantity.parquet/_temporary/0/task_202508060726583418709592025162556_0009_m_000000\n",
            "25/08/06 07:26:58 INFO SparkHadoopMapRedUtil: attempt_202508060726583418709592025162556_0009_m_000000_7: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:26:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 24d0e2dab55f:32979 in memory (size: 21.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:26:58 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 8461 bytes result sent to driver\n",
            "25/08/06 07:26:58 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 654 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:26:58 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:26:58 INFO DAGScheduler: ResultStage 9 (parquet at TopProductsByQuantity.java:25) finished in 0.759 s\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:26:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 07:26:58 INFO DAGScheduler: Job 7 finished: parquet at TopProductsByQuantity.java:25, took 0.790927 s\n",
            "25/08/06 07:26:58 INFO FileFormatWriter: Start to commit write Job f3897e01-c2db-47ab-a5d8-d677aba9048c.\n",
            "25/08/06 07:26:58 INFO FileFormatWriter: Write Job f3897e01-c2db-47ab-a5d8-d677aba9048c committed. Elapsed time: 83 ms.\n",
            "25/08/06 07:26:58 INFO FileFormatWriter: Finished processing stats for write job f3897e01-c2db-47ab-a5d8-d677aba9048c.\n",
            "✅ Top 10 Products by Quantity Parquet File Created.\n",
            "25/08/06 07:26:58 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:26:59 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:26:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:26:59 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:26:59 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:26:59 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:26:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:26:59 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:26:59 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:26:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5557a5a0-0597-4e0e-849f-8aa0b2481630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 2.2: Product-wise Revenue Report.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ask ChatGPT\n"
      ],
      "metadata": {
        "id": "mQwb0AjORpJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ProductRevenueReport.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class ProductRevenueReport {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Product-Wise Revenue Report\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> products = spark.read().parquet(\"file:///content/data/parquet/products.parquet\");\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        Dataset<Row> joined = orderDetails.join(products, \"productCode\");\n",
        "\n",
        "        Dataset<Row> revenueDF = joined.withColumn(\"revenue\", col(\"quantityOrdered\").multiply(col(\"priceEach\")))\n",
        "                .groupBy(\"productName\")\n",
        "                .agg(round(sum(\"revenue\"), 2).alias(\"totalRevenue\"))\n",
        "                .orderBy(col(\"totalRevenue\").desc());\n",
        "\n",
        "        revenueDF.show();\n",
        "\n",
        "        revenueDF.write()\n",
        "                .mode(\"overwrite\")\n",
        "                .parquet(\"file:///content/output/processed/product_revenue_report.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Product Revenue Report Parquet File Created.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx7VFEWRRk2f",
        "outputId": "71a6fa8b-4276-4385-b29a-f01941faf996"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ProductRevenueReport.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" ProductRevenueReport.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" ProductRevenueReport\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbX1gTY6R1aS",
        "outputId": "4ac58bec-8938-4010-d53b-4d1faffb8e0d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:27:55 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:27:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:27:56 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:27:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:27:56 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:27:56 INFO SparkContext: Submitted application: Product-Wise Revenue Report\n",
            "25/08/06 07:27:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:27:56 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:27:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:27:56 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:27:56 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:27:56 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:27:56 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:27:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:27:56 INFO Utils: Successfully started service 'sparkDriver' on port 35677.\n",
            "25/08/06 07:27:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:27:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:27:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:27:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:27:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:27:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-658d8cb3-137e-4551-aa5a-b8ffc113bfb3\n",
            "25/08/06 07:27:57 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:27:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:27:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:27:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:27:58 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:27:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:27:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32967.\n",
            "25/08/06 07:27:58 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:32967\n",
            "25/08/06 07:27:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:27:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 32967, None)\n",
            "25/08/06 07:27:58 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:32967 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 32967, None)\n",
            "25/08/06 07:27:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 32967, None)\n",
            "25/08/06 07:27:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 32967, None)\n",
            "25/08/06 07:27:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:27:59 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:28:01 INFO InMemoryFileIndex: It took 119 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:28:02 INFO SparkContext: Starting job: parquet at ProductRevenueReport.java:11\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Got job 0 (parquet at ProductRevenueReport.java:11) with 1 output partitions\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at ProductRevenueReport.java:11)\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at ProductRevenueReport.java:11), which has no missing parents\n",
            "25/08/06 07:28:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:28:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:28:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:32967 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at ProductRevenueReport.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes) \n",
            "25/08/06 07:28:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:28:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2225 bytes result sent to driver\n",
            "25/08/06 07:28:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 778 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:03 INFO DAGScheduler: ResultStage 0 (parquet at ProductRevenueReport.java:11) finished in 1.114 s\n",
            "25/08/06 07:28:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:28:03 INFO DAGScheduler: Job 0 finished: parquet at ProductRevenueReport.java:11, took 1.241149 s\n",
            "25/08/06 07:28:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:32967 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:06 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:28:06 INFO SparkContext: Starting job: parquet at ProductRevenueReport.java:12\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Got job 1 (parquet at ProductRevenueReport.java:12) with 1 output partitions\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at ProductRevenueReport.java:12)\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at ProductRevenueReport.java:12), which has no missing parents\n",
            "25/08/06 07:28:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:28:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:28:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:32967 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at ProductRevenueReport.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7573 bytes) \n",
            "25/08/06 07:28:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:28:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2013 bytes result sent to driver\n",
            "25/08/06 07:28:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 61 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:06 INFO DAGScheduler: ResultStage 1 (parquet at ProductRevenueReport.java:12) finished in 0.104 s\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:28:06 INFO DAGScheduler: Job 1 finished: parquet at ProductRevenueReport.java:12, took 0.116714 s\n",
            "25/08/06 07:28:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:32967 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:28:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#0)\n",
            "25/08/06 07:28:08 INFO CodeGenerator: Code generated in 430.046452 ms\n",
            "25/08/06 07:28:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:28:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:28:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:32967 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:08 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210466 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:28:08 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:28:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:28:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:28:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:32967 (size: 6.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 07:28:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:28:09 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet/part-00000-439af649-9bde-4bab-9e42-4830f2f72c31-c000.snappy.parquet, range: 0-16162, partition values: [empty row]\n",
            "25/08/06 07:28:09 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:28:09 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 07:28:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 5505 bytes result sent to driver\n",
            "25/08/06 07:28:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 848 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:09 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.863 s\n",
            "25/08/06 07:28:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:28:09 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.874211 s\n",
            "25/08/06 07:28:09 INFO CodeGenerator: Code generated in 40.347714 ms\n",
            "25/08/06 07:28:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 64.0 MiB, free 1703.3 MiB)\n",
            "25/08/06 07:28:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1703.3 MiB)\n",
            "25/08/06 07:28:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:32967 (size: 4.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:09 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:28:10 INFO CodeGenerator: Code generated in 276.682277 ms\n",
            "25/08/06 07:28:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.3 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:32967 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:10 INFO SparkContext: Created broadcast 5 from show at ProductRevenueReport.java:21\n",
            "25/08/06 07:28:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Registering RDD 11 (show at ProductRevenueReport.java:21) as input to shuffle 0\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Got map stage job 3 (show at ProductRevenueReport.java:21) with 1 output partitions\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at ProductRevenueReport.java:21)\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at show at ProductRevenueReport.java:21), which has no missing parents\n",
            "25/08/06 07:28:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.5 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.0 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:32967 (size: 22.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:10 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at show at ProductRevenueReport.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:28:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 07:28:11 INFO CodeGenerator: Code generated in 40.253247 ms\n",
            "25/08/06 07:28:11 INFO CodeGenerator: Code generated in 16.352371 ms\n",
            "25/08/06 07:28:11 INFO CodeGenerator: Code generated in 33.916205 ms\n",
            "25/08/06 07:28:11 INFO CodeGenerator: Code generated in 28.699689 ms\n",
            "25/08/06 07:28:11 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:28:11 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:28:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3926 bytes result sent to driver\n",
            "25/08/06 07:28:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 751 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:11 INFO DAGScheduler: ShuffleMapStage 3 (show at ProductRevenueReport.java:21) finished in 0.829 s\n",
            "25/08/06 07:28:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:28:11 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:28:11 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:28:11 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:28:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:11 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:28:11 INFO CodeGenerator: Code generated in 55.012835 ms\n",
            "25/08/06 07:28:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:28:12 INFO CodeGenerator: Code generated in 98.152842 ms\n",
            "25/08/06 07:28:12 INFO SparkContext: Starting job: show at ProductRevenueReport.java:21\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Got job 4 (show at ProductRevenueReport.java:21) with 1 output partitions\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Final stage: ResultStage 5 (show at ProductRevenueReport.java:21)\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at show at ProductRevenueReport.java:21), which has no missing parents\n",
            "25/08/06 07:28:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 49.0 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:32967 (size: 22.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at show at ProductRevenueReport.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:28:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 07:28:12 INFO ShuffleBlockFetcherIterator: Getting 1 (9.2 KiB) non-empty blocks including 1 (9.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:28:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "25/08/06 07:28:12 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 7667 bytes result sent to driver\n",
            "25/08/06 07:28:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 317 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:12 INFO DAGScheduler: ResultStage 5 (show at ProductRevenueReport.java:21) finished in 0.348 s\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 07:28:12 INFO DAGScheduler: Job 4 finished: show at ProductRevenueReport.java:21, took 0.432555 s\n",
            "25/08/06 07:28:12 INFO CodeGenerator: Code generated in 44.587713 ms\n",
            "25/08/06 07:28:12 INFO CodeGenerator: Code generated in 51.267062 ms\n",
            "+--------------------+------------+\n",
            "|         productName|totalRevenue|\n",
            "+--------------------+------------+\n",
            "|1992 Ferrari 360 ...|   276839.98|\n",
            "|   2001 Ferrari Enzo|   190755.86|\n",
            "|1952 Alpine Renau...|   190017.96|\n",
            "|2003 Harley-David...|    170686.0|\n",
            "|   1968 Ford Mustang|   161531.48|\n",
            "|    1969 Ford Falcon|   152543.02|\n",
            "|1980s Black Hawk ...|   144959.91|\n",
            "|1998 Chrysler Ply...|   142530.63|\n",
            "|1917 Grand Tourin...|    140535.6|\n",
            "|    2002 Suzuki XREO|   135767.03|\n",
            "|1956 Porsche 356A...|   134240.71|\n",
            "|  1969 Corvair Monza|   132363.79|\n",
            "|1928 Mercedes-Ben...|   132275.98|\n",
            "|1957 Corvette Con...|   130749.31|\n",
            "| 1972 Alfa Romeo GTA|   127924.32|\n",
            "|1962 LanciaA Delt...|   123123.01|\n",
            "|1970 Triumph Spit...|   122254.75|\n",
            "|1976 Ford Gran To...|    121890.6|\n",
            "|1948 Porsche Type...|   121653.46|\n",
            "|      1958 Setra Bus|   119085.25|\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#0)\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.2 KiB, free 1702.8 MiB)\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:32967 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:28:13 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210466 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:28:13 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:32967 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:28:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:13 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:13 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:32967 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 07:28:13 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet/part-00000-439af649-9bde-4bab-9e42-4830f2f72c31-c000.snappy.parquet, range: 0-16162, partition values: [empty row]\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 24d0e2dab55f:32967 in memory (size: 22.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:28:13 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 5419 bytes result sent to driver\n",
            "25/08/06 07:28:13 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 142 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:13 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:13 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.235 s\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 07:28:13 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.243786 s\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 64.0 MiB, free 1639.0 MiB)\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1639.0 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:32967 (size: 4.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24d0e2dab55f:32967 in memory (size: 4.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 07:28:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#19)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:32967 in memory (size: 22.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 24d0e2dab55f:32967 in memory (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:32967 in memory (size: 6.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:28:13 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 200.3 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:32967 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:14 INFO SparkContext: Created broadcast 11 from parquet at ProductRevenueReport.java:25\n",
            "25/08/06 07:28:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Registering RDD 23 (parquet at ProductRevenueReport.java:25) as input to shuffle 1\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Got map stage job 6 (parquet at ProductRevenueReport.java:25) with 1 output partitions\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (parquet at ProductRevenueReport.java:25)\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at parquet at ProductRevenueReport.java:25), which has no missing parents\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 49.7 KiB, free 1703.1 MiB)\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:32967 (size: 22.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:14 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at parquet at ProductRevenueReport.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:28:14 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 07:28:14 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 24d0e2dab55f:32967 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:14 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:28:14 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 07:28:14 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 3926 bytes result sent to driver\n",
            "25/08/06 07:28:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 341 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:14 INFO DAGScheduler: ShuffleMapStage 7 (parquet at ProductRevenueReport.java:25) finished in 0.369 s\n",
            "25/08/06 07:28:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:28:14 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:28:14 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:28:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:28:14 INFO CodeGenerator: Code generated in 22.7192 ms\n",
            "25/08/06 07:28:14 INFO SparkContext: Starting job: parquet at ProductRevenueReport.java:25\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Got job 7 (parquet at ProductRevenueReport.java:25) with 1 output partitions\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at ProductRevenueReport.java:25)\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at parquet at ProductRevenueReport.java:25), which has no missing parents\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 49.3 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 1703.0 MiB)\n",
            "25/08/06 07:28:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:32967 (size: 22.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:14 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at parquet at ProductRevenueReport.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:14 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:28:14 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
            "25/08/06 07:28:14 INFO ShuffleBlockFetcherIterator: Getting 1 (9.2 KiB) non-empty blocks including 1 (9.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:28:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 07:28:14 INFO CodeGenerator: Code generated in 18.232702 ms\n",
            "25/08/06 07:28:14 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 10041 bytes result sent to driver\n",
            "25/08/06 07:28:14 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 105 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:14 INFO DAGScheduler: ResultStage 9 (parquet at ProductRevenueReport.java:25) finished in 0.122 s\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Job 7 finished: parquet at ProductRevenueReport.java:25, took 0.143738 s\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Registering RDD 29 (parquet at ProductRevenueReport.java:25) as input to shuffle 2\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Got map stage job 8 (parquet at ProductRevenueReport.java:25) with 1 output partitions\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at ProductRevenueReport.java:25)\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[29] at parquet at ProductRevenueReport.java:25), which has no missing parents\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 52.8 KiB, free 1702.9 MiB)\n",
            "25/08/06 07:28:14 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1702.9 MiB)\n",
            "25/08/06 07:28:14 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:32967 (size: 23.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:28:14 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[29] at parquet at ProductRevenueReport.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:14 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:14 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 07:28:14 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)\n",
            "25/08/06 07:28:14 INFO ShuffleBlockFetcherIterator: Getting 1 (9.2 KiB) non-empty blocks including 1 (9.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:28:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 07:28:15 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 6528 bytes result sent to driver\n",
            "25/08/06 07:28:15 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 390 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:15 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:15 INFO DAGScheduler: ShuffleMapStage 11 (parquet at ProductRevenueReport.java:25) finished in 0.414 s\n",
            "25/08/06 07:28:15 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:28:15 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:28:15 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:28:15 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:28:15 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:28:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:28:15 INFO CodeGenerator: Code generated in 39.941964 ms\n",
            "25/08/06 07:28:15 INFO SparkContext: Starting job: parquet at ProductRevenueReport.java:25\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Got job 9 (parquet at ProductRevenueReport.java:25) with 1 output partitions\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at ProductRevenueReport.java:25)\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[32] at parquet at ProductRevenueReport.java:25), which has no missing parents\n",
            "25/08/06 07:28:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 245.5 KiB, free 1702.7 MiB)\n",
            "25/08/06 07:28:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 91.9 KiB, free 1702.6 MiB)\n",
            "25/08/06 07:28:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:32967 (size: 91.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:28:15 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:28:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[32] at parquet at ProductRevenueReport.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:28:15 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:28:15 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:28:15 INFO Executor: Running task 0.0 in stage 14.0 (TID 9)\n",
            "25/08/06 07:28:15 INFO ShuffleBlockFetcherIterator: Getting 1 (10.5 KiB) non-empty blocks including 1 (10.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:28:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 07:28:15 INFO CodeGenerator: Code generated in 29.28438 ms\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:28:15 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:28:15 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:28:15 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:28:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:28:15 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:28:16 INFO FileOutputCommitter: Saved output of task 'attempt_202508060728156441963550289812083_0014_m_000000_9' to file:/content/output/processed/product_revenue_report.parquet/_temporary/0/task_202508060728156441963550289812083_0014_m_000000\n",
            "25/08/06 07:28:16 INFO SparkHadoopMapRedUtil: attempt_202508060728156441963550289812083_0014_m_000000_9: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:28:16 INFO Executor: Finished task 0.0 in stage 14.0 (TID 9). 8767 bytes result sent to driver\n",
            "25/08/06 07:28:16 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 436 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:28:16 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:28:16 INFO DAGScheduler: ResultStage 14 (parquet at ProductRevenueReport.java:25) finished in 0.498 s\n",
            "25/08/06 07:28:16 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:28:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "25/08/06 07:28:16 INFO DAGScheduler: Job 9 finished: parquet at ProductRevenueReport.java:25, took 0.518503 s\n",
            "25/08/06 07:28:16 INFO FileFormatWriter: Start to commit write Job dbe5f359-7c23-4918-ae0d-5e17de05e953.\n",
            "25/08/06 07:28:16 INFO FileFormatWriter: Write Job dbe5f359-7c23-4918-ae0d-5e17de05e953 committed. Elapsed time: 34 ms.\n",
            "25/08/06 07:28:16 INFO FileFormatWriter: Finished processing stats for write job dbe5f359-7c23-4918-ae0d-5e17de05e953.\n",
            "✅ Product Revenue Report Parquet File Created.\n",
            "25/08/06 07:28:16 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:28:16 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:28:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:28:16 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:28:16 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:28:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:28:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:28:16 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:28:16 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:28:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec752559-7b11-47c1-a2d9-211ecdd25f56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 2.3: Average Order Value per Customer."
      ],
      "metadata": {
        "id": "9nJB1AV9R9i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile AvgOrderValuePerCustomer.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class AvgOrderValuePerCustomer {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Average Order Value per Customer\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orders = spark.read().parquet(\"file:///content/data/parquet/orders.parquet\");\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"file:///content/data/parquet/orderdetails.parquet\");\n",
        "\n",
        "        Dataset<Row> orderTotal = orderDetails.withColumn(\"lineTotal\",\n",
        "                col(\"quantityOrdered\").multiply(col(\"priceEach\")))\n",
        "                .groupBy(\"orderNumber\")\n",
        "                .agg(sum(\"lineTotal\").alias(\"orderValue\"));\n",
        "\n",
        "        Dataset<Row> ordersWithValue = orders.join(orderTotal, \"orderNumber\");\n",
        "\n",
        "        Dataset<Row> avgOrderValPerCustomer = ordersWithValue.groupBy(\"customerNumber\")\n",
        "                .agg(round(avg(\"orderValue\"), 2).alias(\"avgOrderValue\"))\n",
        "                .orderBy(desc(\"avgOrderValue\"));\n",
        "\n",
        "        avgOrderValPerCustomer.show();\n",
        "\n",
        "        avgOrderValPerCustomer.write()\n",
        "                .mode(\"overwrite\")\n",
        "                .parquet(\"file:///content/output/processed/avg_order_value_per_customer.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Average Order Value per Customer Parquet File Created.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukcn2pvHR3eD",
        "outputId": "d38733e9-b299-45d2-def1-b1fd90c7a0eb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing AvgOrderValuePerCustomer.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" AvgOrderValuePerCustomer.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" AvgOrderValuePerCustomer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU_BpxHcSGVO",
        "outputId": "61288d56-b83b-404a-a109-af98843b1de3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:29:04 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:29:04 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:29:04 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:29:04 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:29:04 INFO SparkContext: Submitted application: Average Order Value per Customer\n",
            "25/08/06 07:29:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:29:04 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:29:04 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:29:04 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:29:04 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:29:04 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:29:04 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:29:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:29:05 INFO Utils: Successfully started service 'sparkDriver' on port 46093.\n",
            "25/08/06 07:29:05 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:29:05 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:29:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:29:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:29:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:29:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1428acaf-2867-467b-add1-1cb00bae4472\n",
            "25/08/06 07:29:05 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:29:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:29:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:29:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:29:06 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:29:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:29:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37163.\n",
            "25/08/06 07:29:06 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:37163\n",
            "25/08/06 07:29:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:29:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 37163, None)\n",
            "25/08/06 07:29:06 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:37163 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 37163, None)\n",
            "25/08/06 07:29:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 37163, None)\n",
            "25/08/06 07:29:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 37163, None)\n",
            "25/08/06 07:29:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:29:07 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:29:08 INFO InMemoryFileIndex: It took 130 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:29:09 INFO SparkContext: Starting job: parquet at AvgOrderValuePerCustomer.java:11\n",
            "25/08/06 07:29:09 INFO DAGScheduler: Got job 0 (parquet at AvgOrderValuePerCustomer.java:11) with 1 output partitions\n",
            "25/08/06 07:29:09 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at AvgOrderValuePerCustomer.java:11)\n",
            "25/08/06 07:29:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:09 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at AvgOrderValuePerCustomer.java:11), which has no missing parents\n",
            "25/08/06 07:29:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:29:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:29:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:37163 (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:29:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at AvgOrderValuePerCustomer.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7567 bytes) \n",
            "25/08/06 07:29:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:29:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2078 bytes result sent to driver\n",
            "25/08/06 07:29:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 746 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:10 INFO DAGScheduler: ResultStage 0 (parquet at AvgOrderValuePerCustomer.java:11) finished in 1.076 s\n",
            "25/08/06 07:29:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:29:10 INFO DAGScheduler: Job 0 finished: parquet at AvgOrderValuePerCustomer.java:11, took 1.200108 s\n",
            "25/08/06 07:29:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:37163 in memory (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:29:14 INFO InMemoryFileIndex: It took 24 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:29:14 INFO SparkContext: Starting job: parquet at AvgOrderValuePerCustomer.java:12\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Got job 1 (parquet at AvgOrderValuePerCustomer.java:12) with 1 output partitions\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at AvgOrderValuePerCustomer.java:12)\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at AvgOrderValuePerCustomer.java:12), which has no missing parents\n",
            "25/08/06 07:29:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:29:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:29:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:37163 (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:29:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at AvgOrderValuePerCustomer.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7573 bytes) \n",
            "25/08/06 07:29:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:29:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2056 bytes result sent to driver\n",
            "25/08/06 07:29:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 100 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:14 INFO DAGScheduler: ResultStage 1 (parquet at AvgOrderValuePerCustomer.java:12) finished in 0.165 s\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:29:14 INFO DAGScheduler: Job 1 finished: parquet at AvgOrderValuePerCustomer.java:12, took 0.185744 s\n",
            "25/08/06 07:29:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 07:29:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#0)\n",
            "25/08/06 07:29:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 07:29:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14)\n",
            "25/08/06 07:29:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:37163 in memory (size: 36.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:29:17 INFO CodeGenerator: Code generated in 630.997645 ms\n",
            "25/08/06 07:29:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:29:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:29:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:37163 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:29:18 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4205912 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:29:18 INFO CodeGenerator: Code generated in 815.282111 ms\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.3 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:29:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:37163 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:18 INFO SparkContext: Created broadcast 3 from show at AvgOrderValuePerCustomer.java:25\n",
            "25/08/06 07:29:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:29:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:29:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:37163 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7991 bytes) \n",
            "25/08/06 07:29:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Registering RDD 11 (show at AvgOrderValuePerCustomer.java:25) as input to shuffle 0\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Got map stage job 3 (show at AvgOrderValuePerCustomer.java:25) with 1 output partitions\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at AvgOrderValuePerCustomer.java:25)\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at show at AvgOrderValuePerCustomer.java:25), which has no missing parents\n",
            "25/08/06 07:29:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders.parquet/part-00000-1921228a-b024-41fc-a72e-c771bc390a0e-c000.snappy.parquet, range: 0-11608, partition values: [empty row]\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 42.8 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:29:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:29:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:37163 (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at show at AvgOrderValuePerCustomer.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:18 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 07:29:19 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 07:29:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4828 bytes result sent to driver\n",
            "25/08/06 07:29:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:29:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 07:29:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1163 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:19 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.212 s\n",
            "25/08/06 07:29:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:29:19 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.232515 s\n",
            "25/08/06 07:29:19 INFO CodeGenerator: Code generated in 43.968667 ms\n",
            "25/08/06 07:29:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1026.6 KiB, free 1766.1 MiB)\n",
            "25/08/06 07:29:19 INFO CodeGenerator: Code generated in 28.182397 ms\n",
            "25/08/06 07:29:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 07:29:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:37163 (size: 4.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:19 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:19 INFO CodeGenerator: Code generated in 29.368485 ms\n",
            "25/08/06 07:29:19 INFO CodeGenerator: Code generated in 19.072412 ms\n",
            "25/08/06 07:29:19 INFO CodeGenerator: Code generated in 15.715614 ms\n",
            "25/08/06 07:29:20 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:29:20 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 07:29:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3029 bytes result sent to driver\n",
            "25/08/06 07:29:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 603 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:20 INFO DAGScheduler: ShuffleMapStage 3 (show at AvgOrderValuePerCustomer.java:25) finished in 1.577 s\n",
            "25/08/06 07:29:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:29:20 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:29:20 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:29:20 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:29:20 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:29:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:29:20 INFO CodeGenerator: Code generated in 82.25716 ms\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Registering RDD 14 (show at AvgOrderValuePerCustomer.java:25) as input to shuffle 1\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Got map stage job 4 (show at AvgOrderValuePerCustomer.java:25) with 1 output partitions\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (show at AvgOrderValuePerCustomer.java:25)\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[14] at show at AvgOrderValuePerCustomer.java:25), which has no missing parents\n",
            "25/08/06 07:29:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 61.5 KiB, free 1766.0 MiB)\n",
            "25/08/06 07:29:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.1 KiB, free 1766.0 MiB)\n",
            "25/08/06 07:29:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:37163 (size: 26.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[14] at show at AvgOrderValuePerCustomer.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 07:29:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 07:29:20 INFO ShuffleBlockFetcherIterator: Getting 1 (12.8 KiB) non-empty blocks including 1 (12.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
            "25/08/06 07:29:20 INFO CodeGenerator: Code generated in 12.217726 ms\n",
            "25/08/06 07:29:20 INFO CodeGenerator: Code generated in 10.469624 ms\n",
            "25/08/06 07:29:20 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 6958 bytes result sent to driver\n",
            "25/08/06 07:29:20 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 247 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:20 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:20 INFO DAGScheduler: ShuffleMapStage 5 (show at AvgOrderValuePerCustomer.java:25) finished in 0.275 s\n",
            "25/08/06 07:29:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:29:20 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:29:20 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:29:20 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:29:20 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:37163 in memory (size: 26.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24d0e2dab55f:37163 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:21 INFO CodeGenerator: Code generated in 76.568378 ms\n",
            "25/08/06 07:29:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 24d0e2dab55f:37163 in memory (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:21 INFO CodeGenerator: Code generated in 60.597154 ms\n",
            "25/08/06 07:29:21 INFO SparkContext: Starting job: show at AvgOrderValuePerCustomer.java:25\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Got job 5 (show at AvgOrderValuePerCustomer.java:25) with 1 output partitions\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Final stage: ResultStage 8 (show at AvgOrderValuePerCustomer.java:25)\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at show at AvgOrderValuePerCustomer.java:25), which has no missing parents\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 58.0 KiB, free 1766.1 MiB)\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.9 KiB, free 1766.1 MiB)\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:37163 (size: 25.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at show at AvgOrderValuePerCustomer.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:21 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:21 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:29:21 INFO Executor: Running task 0.0 in stage 8.0 (TID 5)\n",
            "25/08/06 07:29:21 INFO ShuffleBlockFetcherIterator: Getting 1 (6.2 KiB) non-empty blocks including 1 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 07:29:21 INFO Executor: Finished task 0.0 in stage 8.0 (TID 5). 9096 bytes result sent to driver\n",
            "25/08/06 07:29:21 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 103 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:21 INFO DAGScheduler: ResultStage 8 (show at AvgOrderValuePerCustomer.java:25) finished in 0.131 s\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Job 5 finished: show at AvgOrderValuePerCustomer.java:25, took 0.149389 s\n",
            "25/08/06 07:29:21 INFO CodeGenerator: Code generated in 12.203864 ms\n",
            "25/08/06 07:29:21 INFO CodeGenerator: Code generated in 12.031402 ms\n",
            "+--------------+-------------+\n",
            "|customerNumber|avgOrderValue|\n",
            "+--------------+-------------+\n",
            "|           298|     54388.96|\n",
            "|           187|     49470.03|\n",
            "|           286|     45272.69|\n",
            "|           227|      44954.9|\n",
            "|           259|     44611.57|\n",
            "|           151|     44478.49|\n",
            "|           146|     43435.12|\n",
            "|           278|      42509.9|\n",
            "|           386|     41835.19|\n",
            "|           249|     41111.62|\n",
            "|           448|     40314.51|\n",
            "|           239|     40187.62|\n",
            "|           119|     39643.28|\n",
            "|           319|     39216.08|\n",
            "|           363|     38816.43|\n",
            "|           458|     37480.03|\n",
            "|           131|     37271.29|\n",
            "|           114|     36117.01|\n",
            "|           240|     35891.88|\n",
            "|           450|     35884.07|\n",
            "+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 07:29:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 07:29:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#0)\n",
            "25/08/06 07:29:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 07:29:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14)\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.3 KiB, free 1765.7 MiB)\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1765.7 MiB)\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.6 MiB)\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:37163 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:21 INFO SparkContext: Created broadcast 10 from parquet at AvgOrderValuePerCustomer.java:29\n",
            "25/08/06 07:29:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4218451 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.6 MiB)\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:37163 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:21 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4205912 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Registering RDD 24 (parquet at AvgOrderValuePerCustomer.java:29) as input to shuffle 2\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Got map stage job 6 (parquet at AvgOrderValuePerCustomer.java:29) with 1 output partitions\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (parquet at AvgOrderValuePerCustomer.java:29)\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at parquet at AvgOrderValuePerCustomer.java:29), which has no missing parents\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 42.9 KiB, free 1765.6 MiB)\n",
            "25/08/06 07:29:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1765.5 MiB)\n",
            "25/08/06 07:29:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:37163 (size: 19.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:21 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at parquet at AvgOrderValuePerCustomer.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 07:29:21 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)\n",
            "25/08/06 07:29:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.6 KiB, free 1765.5 MiB)\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.5 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:37163 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:22 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet/part-00000-596ba751-0168-4d89-91b2-0a37ffdab1a6-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 07:29:22 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 07:29:22 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 2986 bytes result sent to driver\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7991 bytes) \n",
            "25/08/06 07:29:22 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 163 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:22 INFO DAGScheduler: ShuffleMapStage 9 (parquet at AvgOrderValuePerCustomer.java:29) finished in 0.188 s\n",
            "25/08/06 07:29:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:29:22 INFO DAGScheduler: running: Set(ResultStage 10)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:29:22 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)\n",
            "25/08/06 07:29:22 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders.parquet/part-00000-1921228a-b024-41fc-a72e-c771bc390a0e-c000.snappy.parquet, range: 0-11608, partition values: [empty row]\n",
            "25/08/06 07:29:22 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 07:29:22 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 4785 bytes result sent to driver\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 7) in 62 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:22 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.194 s\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.208291 s\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 1026.6 KiB, free 1764.5 MiB)\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 1764.5 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:37163 (size: 4.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:29:22 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:29:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Registering RDD 29 (parquet at AvgOrderValuePerCustomer.java:29) as input to shuffle 3\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Got map stage job 8 (parquet at AvgOrderValuePerCustomer.java:29) with 1 output partitions\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (parquet at AvgOrderValuePerCustomer.java:29)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at parquet at AvgOrderValuePerCustomer.java:29), which has no missing parents\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 61.9 KiB, free 1764.4 MiB)\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 1764.4 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:37163 (size: 26.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at parquet at AvgOrderValuePerCustomer.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 07:29:22 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)\n",
            "25/08/06 07:29:22 INFO ShuffleBlockFetcherIterator: Getting 1 (12.8 KiB) non-empty blocks including 1 (12.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 24d0e2dab55f:37163 in memory (size: 25.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 24d0e2dab55f:37163 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:37163 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:37163 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:22 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 7001 bytes result sent to driver\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 343 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:22 INFO DAGScheduler: ShuffleMapStage 12 (parquet at AvgOrderValuePerCustomer.java:29) finished in 0.377 s\n",
            "25/08/06 07:29:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:29:22 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:29:22 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 24d0e2dab55f:37163 in memory (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 24d0e2dab55f:37163 in memory (size: 4.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:22 INFO CodeGenerator: Code generated in 19.999606 ms\n",
            "25/08/06 07:29:22 INFO SparkContext: Starting job: parquet at AvgOrderValuePerCustomer.java:29\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Got job 9 (parquet at AvgOrderValuePerCustomer.java:29) with 1 output partitions\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at AvgOrderValuePerCustomer.java:29)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[34] at parquet at AvgOrderValuePerCustomer.java:29), which has no missing parents\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.6 KiB, free 1766.0 MiB)\n",
            "25/08/06 07:29:22 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 1766.0 MiB)\n",
            "25/08/06 07:29:22 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:37163 (size: 26.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:22 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[34] at parquet at AvgOrderValuePerCustomer.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 9) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:29:22 INFO Executor: Running task 0.0 in stage 15.0 (TID 9)\n",
            "25/08/06 07:29:22 INFO ShuffleBlockFetcherIterator: Getting 1 (6.2 KiB) non-empty blocks including 1 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 07:29:22 INFO CodeGenerator: Code generated in 11.609672 ms\n",
            "25/08/06 07:29:22 INFO Executor: Finished task 0.0 in stage 15.0 (TID 9). 11638 bytes result sent to driver\n",
            "25/08/06 07:29:22 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 9) in 72 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:22 INFO DAGScheduler: ResultStage 15 (parquet at AvgOrderValuePerCustomer.java:29) finished in 0.090 s\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Job 9 finished: parquet at AvgOrderValuePerCustomer.java:29, took 0.114967 s\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Registering RDD 35 (parquet at AvgOrderValuePerCustomer.java:29) as input to shuffle 4\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Got map stage job 10 (parquet at AvgOrderValuePerCustomer.java:29) with 1 output partitions\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at AvgOrderValuePerCustomer.java:29)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:22 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[35] at parquet at AvgOrderValuePerCustomer.java:29), which has no missing parents\n",
            "25/08/06 07:29:23 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 61.7 KiB, free 1765.9 MiB)\n",
            "25/08/06 07:29:23 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 07:29:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 24d0e2dab55f:37163 (size: 27.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:29:23 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[35] at parquet at AvgOrderValuePerCustomer.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 10) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 07:29:23 INFO Executor: Running task 0.0 in stage 18.0 (TID 10)\n",
            "25/08/06 07:29:23 INFO ShuffleBlockFetcherIterator: Getting 1 (6.2 KiB) non-empty blocks including 1 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 07:29:23 INFO Executor: Finished task 0.0 in stage 18.0 (TID 10). 8477 bytes result sent to driver\n",
            "25/08/06 07:29:23 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 10) in 144 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:23 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:23 INFO DAGScheduler: ShuffleMapStage 18 (parquet at AvgOrderValuePerCustomer.java:29) finished in 0.167 s\n",
            "25/08/06 07:29:23 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:29:23 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:29:23 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:29:23 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:29:23 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:29:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:29:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:29:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:29:23 INFO CodeGenerator: Code generated in 18.970096 ms\n",
            "25/08/06 07:29:23 INFO SparkContext: Starting job: parquet at AvgOrderValuePerCustomer.java:29\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Got job 11 (parquet at AvgOrderValuePerCustomer.java:29) with 1 output partitions\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at AvgOrderValuePerCustomer.java:29)\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[38] at parquet at AvgOrderValuePerCustomer.java:29), which has no missing parents\n",
            "25/08/06 07:29:23 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 254.3 KiB, free 1765.6 MiB)\n",
            "25/08/06 07:29:23 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 94.3 KiB, free 1765.5 MiB)\n",
            "25/08/06 07:29:23 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 24d0e2dab55f:37163 (size: 94.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:29:23 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[38] at parquet at AvgOrderValuePerCustomer.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:29:23 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:29:23 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 11) (24d0e2dab55f, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:29:23 INFO Executor: Running task 0.0 in stage 22.0 (TID 11)\n",
            "25/08/06 07:29:23 INFO ShuffleBlockFetcherIterator: Getting 1 (6.3 KiB) non-empty blocks including 1 (6.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:29:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 07:29:23 INFO CodeGenerator: Code generated in 13.19366 ms\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:29:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:29:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:29:23 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:29:23 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:29:23 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:29:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"avgOrderValue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional double avgOrderValue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:29:23 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:29:23 INFO FileOutputCommitter: Saved output of task 'attempt_2025080607292338908485595530704_0022_m_000000_11' to file:/content/output/processed/avg_order_value_per_customer.parquet/_temporary/0/task_2025080607292338908485595530704_0022_m_000000\n",
            "25/08/06 07:29:23 INFO SparkHadoopMapRedUtil: attempt_2025080607292338908485595530704_0022_m_000000_11: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:29:23 INFO Executor: Finished task 0.0 in stage 22.0 (TID 11). 10727 bytes result sent to driver\n",
            "25/08/06 07:29:23 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 11) in 381 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:29:23 INFO DAGScheduler: ResultStage 22 (parquet at AvgOrderValuePerCustomer.java:29) finished in 0.502 s\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:29:23 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:29:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 07:29:23 INFO DAGScheduler: Job 11 finished: parquet at AvgOrderValuePerCustomer.java:29, took 0.515675 s\n",
            "25/08/06 07:29:23 INFO FileFormatWriter: Start to commit write Job 96959c47-9b5b-4848-99bd-8da4a27fa687.\n",
            "25/08/06 07:29:23 INFO FileFormatWriter: Write Job 96959c47-9b5b-4848-99bd-8da4a27fa687 committed. Elapsed time: 24 ms.\n",
            "25/08/06 07:29:23 INFO FileFormatWriter: Finished processing stats for write job 96959c47-9b5b-4848-99bd-8da4a27fa687.\n",
            "✅ Average Order Value per Customer Parquet File Created.\n",
            "25/08/06 07:29:23 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:29:23 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:29:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:29:24 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:29:24 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:29:24 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:29:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:29:24 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:29:24 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:29:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-685e3e0d-3807-4b92-bced-ab64058b9daa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Regional Sales Insights\n",
        "  3.1: Sales per Region (Office)\n",
        "  "
      ],
      "metadata": {
        "id": "PjpP_l8YSMTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OfficeIngestion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OfficeIngestion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Office Data Ingestion\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .option(\"inferSchema\", \"true\")\n",
        "                .csv(\"data/offices.csv\");\n",
        "\n",
        "        df.write().mode(SaveMode.Overwrite).parquet(\"data/parquet/offices.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Offices Parquet File Written Successfully\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf3RS4t3SH5_",
        "outputId": "962344cf-4345-4c07-86dc-172b4c959e20"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OfficeIngestion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" OfficeIngestion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" OfficeIngestion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkHIK06hSacS",
        "outputId": "addb53b7-0f72-4631-adf3-f5ee17351c40"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:32:04 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:32:05 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:32:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:32:05 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:32:05 INFO SparkContext: Submitted application: Office Data Ingestion\n",
            "25/08/06 07:32:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:32:05 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:32:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:32:05 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:32:05 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:32:05 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:32:05 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:32:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:32:05 INFO Utils: Successfully started service 'sparkDriver' on port 40211.\n",
            "25/08/06 07:32:05 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:32:05 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:32:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:32:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:32:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:32:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae77ac0c-2731-46d0-a7cf-2b853b94319c\n",
            "25/08/06 07:32:06 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:32:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:32:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:32:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:32:06 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 07:32:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:32:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39219.\n",
            "25/08/06 07:32:06 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:39219\n",
            "25/08/06 07:32:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:32:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 39219, None)\n",
            "25/08/06 07:32:06 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:39219 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 39219, None)\n",
            "25/08/06 07:32:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 39219, None)\n",
            "25/08/06 07:32:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 39219, None)\n",
            "25/08/06 07:32:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:32:07 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:32:09 INFO InMemoryFileIndex: It took 98 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:32:09 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:32:14 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:32:14 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 07:32:15 INFO CodeGenerator: Code generated in 349.236786 ms\n",
            "25/08/06 07:32:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:32:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:32:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:39219 (size: 34.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:32:15 INFO SparkContext: Created broadcast 0 from csv at OfficeIngestion.java:13\n",
            "25/08/06 07:32:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:32:15 INFO SparkContext: Starting job: csv at OfficeIngestion.java:13\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Got job 0 (csv at OfficeIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Final stage: ResultStage 0 (csv at OfficeIngestion.java:13)\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at OfficeIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:32:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:32:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:32:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:39219 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:32:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:32:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at OfficeIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:32:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:32:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 07:32:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:32:16 INFO FileScanRDD: Reading File path: file:///content/data/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 07:32:16 INFO CodeGenerator: Code generated in 23.096754 ms\n",
            "25/08/06 07:32:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1691 bytes result sent to driver\n",
            "25/08/06 07:32:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 445 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:32:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:32:16 INFO DAGScheduler: ResultStage 0 (csv at OfficeIngestion.java:13) finished in 0.748 s\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:32:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Job 0 finished: csv at OfficeIngestion.java:13, took 0.854041 s\n",
            "25/08/06 07:32:16 INFO CodeGenerator: Code generated in 40.113045 ms\n",
            "25/08/06 07:32:16 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:32:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:32:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:32:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:32:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:39219 (size: 34.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:32:16 INFO SparkContext: Created broadcast 2 from csv at OfficeIngestion.java:13\n",
            "25/08/06 07:32:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:32:16 INFO SparkContext: Starting job: csv at OfficeIngestion.java:13\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Got job 1 (csv at OfficeIngestion.java:13) with 1 output partitions\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Final stage: ResultStage 1 (csv at OfficeIngestion.java:13)\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:32:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at OfficeIngestion.java:13), which has no missing parents\n",
            "25/08/06 07:32:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:32:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:32:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:39219 (size: 12.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:32:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at OfficeIngestion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:32:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:32:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 07:32:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:32:17 INFO FileScanRDD: Reading File path: file:///content/data/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 07:32:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1646 bytes result sent to driver\n",
            "25/08/06 07:32:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 131 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:32:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:32:17 INFO DAGScheduler: ResultStage 1 (csv at OfficeIngestion.java:13) finished in 0.227 s\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:32:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Job 1 finished: csv at OfficeIngestion.java:13, took 0.239312 s\n",
            "25/08/06 07:32:17 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 07:32:17 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 07:32:17 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:32:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:32:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:32:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:32:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:32:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:39219 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:32:17 INFO SparkContext: Created broadcast 4 from parquet at OfficeIngestion.java:15\n",
            "25/08/06 07:32:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:32:17 INFO SparkContext: Starting job: parquet at OfficeIngestion.java:15\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Got job 2 (parquet at OfficeIngestion.java:15) with 1 output partitions\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OfficeIngestion.java:15)\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at parquet at OfficeIngestion.java:15), which has no missing parents\n",
            "25/08/06 07:32:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 212.1 KiB, free 1766.7 MiB)\n",
            "25/08/06 07:32:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1766.6 MiB)\n",
            "25/08/06 07:32:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:39219 (size: 76.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:32:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:32:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at parquet at OfficeIngestion.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:32:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:32:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 07:32:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:32:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:32:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:32:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:32:17 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:32:17 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:32:17 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:32:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"territory\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 officeCode;\n",
            "  optional binary city (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary territory (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:32:17 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:32:18 INFO FileScanRDD: Reading File path: file:///content/data/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 07:32:18 INFO CodeGenerator: Code generated in 37.095514 ms\n",
            "25/08/06 07:32:18 INFO FileOutputCommitter: Saved output of task 'attempt_202508060732177659301689271554926_0002_m_000000_2' to file:/content/data/parquet/offices.parquet/_temporary/0/task_202508060732177659301689271554926_0002_m_000000\n",
            "25/08/06 07:32:18 INFO SparkHadoopMapRedUtil: attempt_202508060732177659301689271554926_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 07:32:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver\n",
            "25/08/06 07:32:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1068 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 07:32:18 INFO DAGScheduler: ResultStage 2 (parquet at OfficeIngestion.java:15) finished in 1.158 s\n",
            "25/08/06 07:32:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:32:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:32:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:32:18 INFO DAGScheduler: Job 2 finished: parquet at OfficeIngestion.java:15, took 1.172393 s\n",
            "25/08/06 07:32:18 INFO FileFormatWriter: Start to commit write Job fe63e9cf-06fc-43fc-b481-fc61c536ff6b.\n",
            "25/08/06 07:32:18 INFO FileFormatWriter: Write Job fe63e9cf-06fc-43fc-b481-fc61c536ff6b committed. Elapsed time: 27 ms.\n",
            "25/08/06 07:32:18 INFO FileFormatWriter: Finished processing stats for write job fe63e9cf-06fc-43fc-b481-fc61c536ff6b.\n",
            "✅ Offices Parquet File Written Successfully\n",
            "25/08/06 07:32:18 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:32:18 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 07:32:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:32:18 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:32:18 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:32:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:32:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:32:18 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:32:18 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:32:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bba965d-b293-4a32-a63d-ee6d07a132d1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SalesByRegion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SalesByRegion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Sales by Region\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Step 1: Load Parquet files\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices.parquet\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"data/parquet/employees.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments.parquet\");\n",
        "\n",
        "        // Step 2: Join DataFrames\n",
        "        Dataset<Row> joined = offices.alias(\"o\")\n",
        "                .join(employees.alias(\"e\"), functions.col(\"o.officeCode\").equalTo(functions.col(\"e.officeCode\")))\n",
        "                .join(customers.alias(\"c\"), functions.col(\"e.employeeNumber\").equalTo(functions.col(\"c.salesRepEmployeeNumber\")))\n",
        "                .join(payments.alias(\"p\"), functions.col(\"c.customerNumber\").equalTo(functions.col(\"p.customerNumber\")));\n",
        "\n",
        "        // Step 3: Select required columns with alias to avoid ambiguity\n",
        "        Dataset<Row> selected = joined.select(\n",
        "                functions.col(\"o.country\").alias(\"regionCountry\"),\n",
        "                functions.col(\"p.amount\")\n",
        "        );\n",
        "\n",
        "        // Step 4: Group by regionCountry and calculate totalRevenue\n",
        "        Dataset<Row> salesByRegion = selected.groupBy(\"regionCountry\")\n",
        "                .agg(functions.round(functions.sum(\"amount\"), 2).alias(\"totalRevenue\"))\n",
        "                .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        // Step 5: Save output\n",
        "        salesByRegion.write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(\"output/processed/sales_by_country.parquet\");\n",
        "\n",
        "        // Step 6: Show result\n",
        "        salesByRegion.show(false);\n",
        "        System.out.println(\"✅ Sales by Region written to output/processed/sales_by_country.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz_RrUaWSb-D",
        "outputId": "d932d4e0-4e75-4e0d-c6f7-6f1d1b9d3584"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SalesByRegion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SalesByRegion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" SalesByRegion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMNiD5hVecKF",
        "outputId": "bb417914-ee56-44d6-d3f8-5267c65944a0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:28:21 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:28:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:28:22 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:28:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:28:22 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:28:22 INFO SparkContext: Submitted application: Sales by Region\n",
            "25/08/06 08:28:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:28:22 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:28:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:28:22 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:28:22 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:28:22 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:28:22 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:28:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:28:23 INFO Utils: Successfully started service 'sparkDriver' on port 38223.\n",
            "25/08/06 08:28:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:28:23 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:28:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:28:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:28:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:28:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-740a181b-067a-403f-a766-36370d0e3fcd\n",
            "25/08/06 08:28:23 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:28:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:28:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:28:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:28:24 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 08:28:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:28:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46515.\n",
            "25/08/06 08:28:24 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:46515\n",
            "25/08/06 08:28:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:28:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 46515, None)\n",
            "25/08/06 08:28:24 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:46515 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 46515, None)\n",
            "25/08/06 08:28:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 46515, None)\n",
            "25/08/06 08:28:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 46515, None)\n",
            "25/08/06 08:28:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:28:24 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:28:27 INFO InMemoryFileIndex: It took 176 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:28:28 INFO SparkContext: Starting job: parquet at SalesByRegion.java:11\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Got job 0 (parquet at SalesByRegion.java:11) with 1 output partitions\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at SalesByRegion.java:11)\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at SalesByRegion.java:11), which has no missing parents\n",
            "25/08/06 08:28:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:28:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:28:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:46515 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at SalesByRegion.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7568 bytes) \n",
            "25/08/06 08:28:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:28:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2189 bytes result sent to driver\n",
            "25/08/06 08:28:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 915 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:29 INFO DAGScheduler: ResultStage 0 (parquet at SalesByRegion.java:11) finished in 1.222 s\n",
            "25/08/06 08:28:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:28:29 INFO DAGScheduler: Job 0 finished: parquet at SalesByRegion.java:11, took 1.302045 s\n",
            "25/08/06 08:28:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:46515 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:32 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:28:32 INFO SparkContext: Starting job: parquet at SalesByRegion.java:12\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Got job 1 (parquet at SalesByRegion.java:12) with 1 output partitions\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at SalesByRegion.java:12)\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at SalesByRegion.java:12), which has no missing parents\n",
            "25/08/06 08:28:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:28:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:28:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:46515 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at SalesByRegion.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:28:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:28:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2067 bytes result sent to driver\n",
            "25/08/06 08:28:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 57 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:32 INFO DAGScheduler: ResultStage 1 (parquet at SalesByRegion.java:12) finished in 0.109 s\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Job 1 finished: parquet at SalesByRegion.java:12, took 0.122999 s\n",
            "25/08/06 08:28:32 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:28:32 INFO SparkContext: Starting job: parquet at SalesByRegion.java:13\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Got job 2 (parquet at SalesByRegion.java:13) with 1 output partitions\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at SalesByRegion.java:13)\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at SalesByRegion.java:13), which has no missing parents\n",
            "25/08/06 08:28:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:28:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:28:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:46515 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at SalesByRegion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:28:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:28:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2391 bytes result sent to driver\n",
            "25/08/06 08:28:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 57 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:32 INFO DAGScheduler: ResultStage 2 (parquet at SalesByRegion.java:13) finished in 0.100 s\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:28:32 INFO DAGScheduler: Job 2 finished: parquet at SalesByRegion.java:13, took 0.126402 s\n",
            "25/08/06 08:28:32 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:28:33 INFO SparkContext: Starting job: parquet at SalesByRegion.java:14\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Got job 3 (parquet at SalesByRegion.java:14) with 1 output partitions\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at SalesByRegion.java:14)\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at SalesByRegion.java:14), which has no missing parents\n",
            "25/08/06 08:28:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:28:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:28:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:46515 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at SalesByRegion.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes) \n",
            "25/08/06 08:28:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:28:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2054 bytes result sent to driver\n",
            "25/08/06 08:28:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 50 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:33 INFO DAGScheduler: ResultStage 3 (parquet at SalesByRegion.java:14) finished in 0.082 s\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:28:33 INFO DAGScheduler: Job 3 finished: parquet at SalesByRegion.java:14, took 0.092179 s\n",
            "25/08/06 08:28:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:46515 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:46515 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:46515 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:28:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 08:28:35 INFO CodeGenerator: Code generated in 397.007947 ms\n",
            "25/08/06 08:28:35 INFO CodeGenerator: Code generated in 400.791605 ms\n",
            "25/08/06 08:28:35 INFO CodeGenerator: Code generated in 404.125325 ms\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:46515 (size: 34.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:28:35 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:46515 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:35 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:46515 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:35 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4201533 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197390 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4209513 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7994 bytes) \n",
            "25/08/06 08:28:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:36 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers.parquet/part-00000-7f8fc258-1bd2-4256-ae31-afd444ff7374-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:28:36 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:28:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1795 bytes result sent to driver\n",
            "25/08/06 08:28:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes) \n",
            "25/08/06 08:28:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 403 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:36 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.422 s\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:28:36 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.434225 s\n",
            "25/08/06 08:28:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 24.0 B, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices.parquet/part-00000-164824fd-fabd-4f10-96d7-81cf771dd9a5-c000.snappy.parquet, range: 0-3086, partition values: [empty row]\n",
            "25/08/06 08:28:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 120.0 B, free 1766.9 MiB)\n",
            "25/08/06 08:28:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:46515 (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:28:36 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 08:28:36 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:46515 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:28:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:28:36 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:28:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:28:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:28:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:28:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:28:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:28:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:28:37 INFO SparkContext: Starting job: parquet at SalesByRegion.java:36\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Got job 7 (parquet at SalesByRegion.java:36) with 1 output partitions\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at SalesByRegion.java:36)\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at parquet at SalesByRegion.java:36), which has no missing parents\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 205.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 08:28:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:46515 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:28:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at parquet at SalesByRegion.java:36) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1985 bytes result sent to driver\n",
            "25/08/06 08:28:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 08:28:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 900 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:37 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.288 s\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.335858 s\n",
            "25/08/06 08:28:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:28:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments.parquet/part-00000-c3edcc87-7baa-4b61-bf48-9cb12c7a8181-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:28:37 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:28:37 INFO CodeGenerator: Code generated in 38.573239 ms\n",
            "25/08/06 08:28:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4915 bytes result sent to driver\n",
            "25/08/06 08:28:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 08:28:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 116 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:28:37 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.358 s\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.438070 s\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 1024.1 KiB, free 1765.6 MiB)\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 1027.1 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:28:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:46515 (size: 5.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:28:37 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 362.0 B, free 1764.6 MiB)\n",
            "25/08/06 08:28:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:46515 (size: 362.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:28:37 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:28:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:28:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:28:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:28:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:28:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:28:37 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:28:37 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:28:37 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:28:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"regionCountry\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary regionCountry (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:28:37 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:28:37 INFO FileOutputCommitter: Saved output of task 'attempt_202508060828371432098689715686794_0007_m_000000_7' to file:/content/output/processed/sales_by_country.parquet/_temporary/0/task_202508060828371432098689715686794_0007_m_000000\n",
            "25/08/06 08:28:37 INFO SparkHadoopMapRedUtil: attempt_202508060828371432098689715686794_0007_m_000000_7: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 08:28:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2291 bytes result sent to driver\n",
            "25/08/06 08:28:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 383 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:37 INFO DAGScheduler: ResultStage 7 (parquet at SalesByRegion.java:36) finished in 0.857 s\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:28:37 INFO DAGScheduler: Job 7 finished: parquet at SalesByRegion.java:36, took 0.866951 s\n",
            "25/08/06 08:28:37 INFO FileFormatWriter: Start to commit write Job b27088d5-6033-466e-a8e1-d8f9e3ac8200.\n",
            "25/08/06 08:28:37 INFO FileFormatWriter: Write Job b27088d5-6033-466e-a8e1-d8f9e3ac8200 committed. Elapsed time: 43 ms.\n",
            "25/08/06 08:28:37 INFO FileFormatWriter: Finished processing stats for write job b27088d5-6033-466e-a8e1-d8f9e3ac8200.\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:28:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.2 KiB, free 1764.4 MiB)\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.2 KiB, free 1764.2 MiB)\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.2 KiB, free 1764.0 MiB)\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:46515 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:28:38 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4201533 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:46515 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:38 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197390 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 24d0e2dab55f:46515 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:38 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4209513 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:28:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.6 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:38 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 08:28:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:38 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments.parquet/part-00000-c3edcc87-7baa-4b61-bf48-9cb12c7a8181-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:28:38 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:38 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:38 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:39 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4958 bytes result sent to driver\n",
            "25/08/06 08:28:39 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 107 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:39 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes) \n",
            "25/08/06 08:28:39 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:28:39 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices.parquet/part-00000-164824fd-fabd-4f10-96d7-81cf771dd9a5-c000.snappy.parquet, range: 0-3086, partition values: [empty row]\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 13.6 KiB, free 1763.9 MiB)\n",
            "25/08/06 08:28:39 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1763.8 MiB)\n",
            "25/08/06 08:28:39 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 24d0e2dab55f:46515 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:39 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:28:39 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.276 s\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.287374 s\n",
            "25/08/06 08:28:39 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1985 bytes result sent to driver\n",
            "25/08/06 08:28:39 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7994 bytes) \n",
            "25/08/06 08:28:39 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 08:28:39 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 121 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:39 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.209 s\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.304489 s\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 1024.1 KiB, free 1762.8 MiB)\n",
            "25/08/06 08:28:39 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers.parquet/part-00000-7f8fc258-1bd2-4256-ae31-afd444ff7374-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 1027.1 KiB, free 1761.8 MiB)\n",
            "25/08/06 08:28:39 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1761.8 MiB)\n",
            "25/08/06 08:28:39 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 24d0e2dab55f:46515 (size: 5.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:28:39 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 362.0 B, free 1761.8 MiB)\n",
            "25/08/06 08:28:39 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 24d0e2dab55f:46515 (size: 362.0 B, free: 1767.3 MiB)\n",
            "25/08/06 08:28:39 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:39 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1795 bytes result sent to driver\n",
            "25/08/06 08:28:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 169 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:28:39 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.278 s\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:28:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 08:28:39 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.412748 s\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 24.0 B, free 1761.8 MiB)\n",
            "25/08/06 08:28:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:28:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:28:39 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 120.0 B, free 1761.8 MiB)\n",
            "25/08/06 08:28:39 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 24d0e2dab55f:46515 (size: 120.0 B, free: 1767.3 MiB)\n",
            "25/08/06 08:28:39 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:28:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:28:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "+-------------+------------+\n",
            "|regionCountry|totalRevenue|\n",
            "+-------------+------------+\n",
            "+-------------+------------+\n",
            "\n",
            "✅ Sales by Region written to output/processed/sales_by_country.parquet\n",
            "25/08/06 08:28:39 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:28:39 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 08:28:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:28:39 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:28:39 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:28:39 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:28:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:28:39 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:28:39 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:28:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-7229c100-dd8d-4cb6-9797-ec8f9f3a97b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile OptimizedSalesByRegion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OptimizedSalesByRegion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Optimized Sales by Region\")\n",
        "                .master(\"local\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Load parquet data\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices.parquet\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"data/parquet/employees.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments.parquet\");\n",
        "\n",
        "        // Join + cache\n",
        "        Dataset<Row> joined = offices.alias(\"o\")\n",
        "                .join(employees.alias(\"e\"), functions.col(\"o.officeCode\").equalTo(functions.col(\"e.officeCode\")))\n",
        "                .join(customers.alias(\"c\"), functions.col(\"e.employeeNumber\").equalTo(functions.col(\"c.salesRepEmployeeNumber\")))\n",
        "                .join(payments.alias(\"p\"), functions.col(\"c.customerNumber\").equalTo(functions.col(\"p.customerNumber\")))\n",
        "                .select(\n",
        "                        functions.col(\"o.country\").alias(\"regionCountry\"),\n",
        "                        functions.col(\"p.amount\")\n",
        "                )\n",
        "                .repartition(4) // optimization: increase parallelism\n",
        "                .persist();     // optimization: cache joined data\n",
        "\n",
        "        // Aggregation\n",
        "        Dataset<Row> salesByRegion = joined.groupBy(\"regionCountry\")\n",
        "                .agg(functions.round(functions.sum(\"amount\"), 2).alias(\"totalRevenue\"))\n",
        "                .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        // Coalesce before saving\n",
        "        salesByRegion.coalesce(1)\n",
        "                .write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(\"output/optimized/sales_by_country_optimized.parquet\");\n",
        "\n",
        "        salesByRegion.show(false);\n",
        "        System.out.println(\"✅ Optimized sales report saved.\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "324DfwrYe1jK",
        "outputId": "c1fab82c-5443-4e7c-a33e-97043e87f0c0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing OptimizedSalesByRegion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" OptimizedSalesByRegion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" OptimizedSalesByRegion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7SoSmflf8EP",
        "outputId": "a33b9342-2648-47ee-e3f4-504a2c24738d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:29:32 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:29:32 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:29:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:29:32 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:29:32 INFO SparkContext: Submitted application: Optimized Sales by Region\n",
            "25/08/06 08:29:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:29:32 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:29:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:29:32 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:29:32 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:29:32 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:29:32 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:29:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:29:33 INFO Utils: Successfully started service 'sparkDriver' on port 46309.\n",
            "25/08/06 08:29:33 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:29:33 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:29:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:29:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:29:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:29:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fcdff41-d0f8-4609-bba6-3e259f9373e4\n",
            "25/08/06 08:29:33 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:29:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:29:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:29:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:29:34 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 08:29:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:29:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45177.\n",
            "25/08/06 08:29:34 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:45177\n",
            "25/08/06 08:29:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:29:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 45177, None)\n",
            "25/08/06 08:29:34 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:45177 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 45177, None)\n",
            "25/08/06 08:29:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 45177, None)\n",
            "25/08/06 08:29:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 45177, None)\n",
            "25/08/06 08:29:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:29:35 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:29:36 INFO InMemoryFileIndex: It took 89 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:29:37 INFO SparkContext: Starting job: parquet at OptimizedSalesByRegion.java:11\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Got job 0 (parquet at OptimizedSalesByRegion.java:11) with 1 output partitions\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at OptimizedSalesByRegion.java:11)\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at OptimizedSalesByRegion.java:11), which has no missing parents\n",
            "25/08/06 08:29:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:29:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:29:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:45177 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at OptimizedSalesByRegion.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7568 bytes) \n",
            "25/08/06 08:29:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:29:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2146 bytes result sent to driver\n",
            "25/08/06 08:29:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1049 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:39 INFO DAGScheduler: ResultStage 0 (parquet at OptimizedSalesByRegion.java:11) finished in 1.384 s\n",
            "25/08/06 08:29:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:29:39 INFO DAGScheduler: Job 0 finished: parquet at OptimizedSalesByRegion.java:11, took 1.487683 s\n",
            "25/08/06 08:29:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:45177 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:42 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:29:42 INFO SparkContext: Starting job: parquet at OptimizedSalesByRegion.java:12\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Got job 1 (parquet at OptimizedSalesByRegion.java:12) with 1 output partitions\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OptimizedSalesByRegion.java:12)\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at OptimizedSalesByRegion.java:12), which has no missing parents\n",
            "25/08/06 08:29:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:29:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:29:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:45177 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at OptimizedSalesByRegion.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:29:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:29:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2067 bytes result sent to driver\n",
            "25/08/06 08:29:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:42 INFO DAGScheduler: ResultStage 1 (parquet at OptimizedSalesByRegion.java:12) finished in 0.105 s\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:29:42 INFO DAGScheduler: Job 1 finished: parquet at OptimizedSalesByRegion.java:12, took 0.115254 s\n",
            "25/08/06 08:29:42 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:29:43 INFO SparkContext: Starting job: parquet at OptimizedSalesByRegion.java:13\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Got job 2 (parquet at OptimizedSalesByRegion.java:13) with 1 output partitions\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OptimizedSalesByRegion.java:13)\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at OptimizedSalesByRegion.java:13), which has no missing parents\n",
            "25/08/06 08:29:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:29:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:29:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:45177 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at OptimizedSalesByRegion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:29:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:29:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:45177 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2434 bytes result sent to driver\n",
            "25/08/06 08:29:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 96 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:43 INFO DAGScheduler: ResultStage 2 (parquet at OptimizedSalesByRegion.java:13) finished in 0.154 s\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Job 2 finished: parquet at OptimizedSalesByRegion.java:13, took 0.166871 s\n",
            "25/08/06 08:29:43 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:29:43 INFO SparkContext: Starting job: parquet at OptimizedSalesByRegion.java:14\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Got job 3 (parquet at OptimizedSalesByRegion.java:14) with 1 output partitions\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at OptimizedSalesByRegion.java:14)\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at OptimizedSalesByRegion.java:14), which has no missing parents\n",
            "25/08/06 08:29:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:29:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:29:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:45177 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at OptimizedSalesByRegion.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes) \n",
            "25/08/06 08:29:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:29:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2054 bytes result sent to driver\n",
            "25/08/06 08:29:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 50 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:43 INFO DAGScheduler: ResultStage 3 (parquet at OptimizedSalesByRegion.java:14) finished in 0.092 s\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:29:43 INFO DAGScheduler: Job 3 finished: parquet at OptimizedSalesByRegion.java:14, took 0.103454 s\n",
            "25/08/06 08:29:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:45177 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:45177 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:29:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 08:29:46 INFO CodeGenerator: Code generated in 567.692327 ms\n",
            "25/08/06 08:29:46 INFO CodeGenerator: Code generated in 92.994839 ms\n",
            "25/08/06 08:29:46 INFO CodeGenerator: Code generated in 108.069438 ms\n",
            "25/08/06 08:29:46 INFO CodeGenerator: Code generated in 114.454141 ms\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:45177 (size: 34.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:29:46 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:45177 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:46 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:45177 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:46 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4209513 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:29:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197390 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:29:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4201533 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:29:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:29:46 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:29:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:46 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:29:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:45177 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:47 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes) \n",
            "25/08/06 08:29:47 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:29:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:45177 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:47 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:29:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:29:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:45177 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:47 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices.parquet/part-00000-164824fd-fabd-4f10-96d7-81cf771dd9a5-c000.snappy.parquet, range: 0-3086, partition values: [empty row]\n",
            "25/08/06 08:29:47 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 08:29:47 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:29:47 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2028 bytes result sent to driver\n",
            "25/08/06 08:29:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 08:29:47 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 891 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:47 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.923 s\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.941104 s\n",
            "25/08/06 08:29:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:29:47 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments.parquet/part-00000-c3edcc87-7baa-4b61-bf48-9cb12c7a8181-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:29:47 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:29:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4915 bytes result sent to driver\n",
            "25/08/06 08:29:47 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7994 bytes) \n",
            "25/08/06 08:29:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 76 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:47 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.949 s\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:29:47 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.011012 s\n",
            "25/08/06 08:29:47 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:29:48 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers.parquet/part-00000-7f8fc258-1bd2-4256-ae31-afd444ff7374-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 60.70258 ms\n",
            "25/08/06 08:29:48 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1024.1 KiB, free 1765.9 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1027.1 KiB, free 1764.9 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:45177 (size: 5.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 362.0 B, free 1764.8 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:45177 (size: 362.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:29:48 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1795 bytes result sent to driver\n",
            "25/08/06 08:29:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 90 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:48 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.007 s\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.085872 s\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 24.0 B, free 1764.8 MiB)\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 120.0 B, free 1764.8 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:45177 (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 56.797306 ms\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.2 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:45177 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 13 from parquet at OptimizedSalesByRegion.java:37\n",
            "25/08/06 08:29:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197742 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Registering RDD 24 (parquet at OptimizedSalesByRegion.java:37) as input to shuffle 0\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Registering RDD 32 (parquet at OptimizedSalesByRegion.java:37) as input to shuffle 1\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Got map stage job 7 (parquet at OptimizedSalesByRegion.java:37) with 4 output partitions\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (parquet at OptimizedSalesByRegion.java:37)\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[24] at parquet at OptimizedSalesByRegion.java:37), which has no missing parents\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.7 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:45177 (size: 8.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[24] at parquet at OptimizedSalesByRegion.java:37) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:48 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:48 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 08:29:48 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:29:48 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees.parquet/part-00000-c833c37b-8abe-4e10-b263-92deaa958168-c000.snappy.parquet, range: 0-3438, partition values: [empty row]\n",
            "25/08/06 08:29:48 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 08:29:48 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2123 bytes result sent to driver\n",
            "25/08/06 08:29:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 150 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:48 INFO DAGScheduler: ShuffleMapStage 7 (parquet at OptimizedSalesByRegion.java:37) finished in 0.188 s\n",
            "25/08/06 08:29:48 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:29:48 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:29:48 INFO DAGScheduler: waiting: Set(ShuffleMapStage 8)\n",
            "25/08/06 08:29:48 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[32] at parquet at OptimizedSalesByRegion.java:37), which has no missing parents\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 40.4 KiB, free 1764.6 MiB)\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 1764.5 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:45177 (size: 18.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:48 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:48 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[32] at parquet at OptimizedSalesByRegion.java:37) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "25/08/06 08:29:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 4 tasks resource profile 0\n",
            "25/08/06 08:29:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:48 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 08:29:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:29:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "25/08/06 08:29:48 INFO MemoryStore: Block rdd_27_0 stored as values in memory (estimated size 16.0 B, free 1764.5 MiB)\n",
            "25/08/06 08:29:48 INFO BlockManagerInfo: Added rdd_27_0 in memory on 24d0e2dab55f:45177 (size: 16.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 15.027518 ms\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 48.96343 ms\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 35.444374 ms\n",
            "25/08/06 08:29:48 INFO CodeGenerator: Code generated in 18.516457 ms\n",
            "25/08/06 08:29:49 INFO CodeGenerator: Code generated in 11.157432 ms\n",
            "25/08/06 08:29:49 INFO CodeGenerator: Code generated in 16.4801 ms\n",
            "25/08/06 08:29:49 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4715 bytes result sent to driver\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 9) (24d0e2dab55f, executor driver, partition 1, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:49 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 463 ms on 24d0e2dab55f (executor driver) (1/4)\n",
            "25/08/06 08:29:49 INFO Executor: Running task 1.0 in stage 8.0 (TID 9)\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:29:49 INFO MemoryStore: Block rdd_27_1 stored as values in memory (estimated size 16.0 B, free 1764.5 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Added rdd_27_1 in memory on 24d0e2dab55f:45177 (size: 16.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO Executor: Finished task 1.0 in stage 8.0 (TID 9). 4715 bytes result sent to driver\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 10) (24d0e2dab55f, executor driver, partition 2, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:49 INFO Executor: Running task 2.0 in stage 8.0 (TID 10)\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 9) in 82 ms on 24d0e2dab55f (executor driver) (2/4)\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:29:49 INFO MemoryStore: Block rdd_27_2 stored as values in memory (estimated size 16.0 B, free 1764.5 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Added rdd_27_2 in memory on 24d0e2dab55f:45177 (size: 16.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO Executor: Finished task 2.0 in stage 8.0 (TID 10). 4715 bytes result sent to driver\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 11) (24d0e2dab55f, executor driver, partition 3, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:49 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 10) in 69 ms on 24d0e2dab55f (executor driver) (3/4)\n",
            "25/08/06 08:29:49 INFO Executor: Running task 3.0 in stage 8.0 (TID 11)\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 08:29:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 08:29:49 INFO MemoryStore: Block rdd_27_3 stored as values in memory (estimated size 16.0 B, free 1764.5 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Added rdd_27_3 in memory on 24d0e2dab55f:45177 (size: 16.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 24d0e2dab55f:45177 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:45177 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO Executor: Finished task 3.0 in stage 8.0 (TID 11). 4758 bytes result sent to driver\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 11) in 98 ms on 24d0e2dab55f (executor driver) (4/4)\n",
            "25/08/06 08:29:49 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:49 INFO DAGScheduler: ShuffleMapStage 8 (parquet at OptimizedSalesByRegion.java:37) finished in 0.739 s\n",
            "25/08/06 08:29:49 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:29:49 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:29:49 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:29:49 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 24d0e2dab55f:45177 in memory (size: 8.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 24d0e2dab55f:45177 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:29:49 INFO SparkContext: Starting job: parquet at OptimizedSalesByRegion.java:37\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Got job 8 (parquet at OptimizedSalesByRegion.java:37) with 1 output partitions\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at OptimizedSalesByRegion.java:37)\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at parquet at OptimizedSalesByRegion.java:37), which has no missing parents\n",
            "25/08/06 08:29:49 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 205.0 KiB, free 1764.4 MiB)\n",
            "25/08/06 08:29:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1764.3 MiB)\n",
            "25/08/06 08:29:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 24d0e2dab55f:45177 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:49 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at parquet at OptimizedSalesByRegion.java:37) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:29:49 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 08:29:49 INFO Executor: Running task 0.0 in stage 9.0 (TID 12)\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:29:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:29:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:29:49 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:29:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"regionCountry\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary regionCountry (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:29:49 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:29:49 INFO FileOutputCommitter: Saved output of task 'attempt_20250806082949799038403411437066_0009_m_000000_12' to file:/content/output/optimized/sales_by_country_optimized.parquet/_temporary/0/task_20250806082949799038403411437066_0009_m_000000\n",
            "25/08/06 08:29:49 INFO SparkHadoopMapRedUtil: attempt_20250806082949799038403411437066_0009_m_000000_12: Committed. Elapsed time: 6 ms.\n",
            "25/08/06 08:29:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 12). 2291 bytes result sent to driver\n",
            "25/08/06 08:29:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 277 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:29:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:49 INFO DAGScheduler: ResultStage 9 (parquet at OptimizedSalesByRegion.java:37) finished in 0.350 s\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:29:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 08:29:49 INFO DAGScheduler: Job 8 finished: parquet at OptimizedSalesByRegion.java:37, took 0.357251 s\n",
            "25/08/06 08:29:49 INFO FileFormatWriter: Start to commit write Job d733fe61-ef12-4569-86bc-fb3573a25b3b.\n",
            "25/08/06 08:29:49 INFO FileFormatWriter: Write Job d733fe61-ef12-4569-86bc-fb3573a25b3b committed. Elapsed time: 33 ms.\n",
            "25/08/06 08:29:49 INFO FileFormatWriter: Finished processing stats for write job d733fe61-ef12-4569-86bc-fb3573a25b3b.\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Registering RDD 41 (show at OptimizedSalesByRegion.java:39) as input to shuffle 2\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Got map stage job 9 (show at OptimizedSalesByRegion.java:39) with 4 output partitions\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (show at OptimizedSalesByRegion.java:39)\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[41] at show at OptimizedSalesByRegion.java:39), which has no missing parents\n",
            "25/08/06 08:29:50 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 40.4 KiB, free 1764.3 MiB)\n",
            "25/08/06 08:29:50 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 1764.3 MiB)\n",
            "25/08/06 08:29:50 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 24d0e2dab55f:45177 (size: 18.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:29:50 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:29:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[41] at show at OptimizedSalesByRegion.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "25/08/06 08:29:50 INFO TaskSchedulerImpl: Adding task set 11.0 with 4 tasks resource profile 0\n",
            "25/08/06 08:29:50 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 13) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:50 INFO Executor: Running task 0.0 in stage 11.0 (TID 13)\n",
            "25/08/06 08:29:50 INFO BlockManager: Found block rdd_27_0 locally\n",
            "25/08/06 08:29:50 INFO Executor: Finished task 0.0 in stage 11.0 (TID 13). 4027 bytes result sent to driver\n",
            "25/08/06 08:29:50 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 14) (24d0e2dab55f, executor driver, partition 1, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:50 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 13) in 56 ms on 24d0e2dab55f (executor driver) (1/4)\n",
            "25/08/06 08:29:50 INFO Executor: Running task 1.0 in stage 11.0 (TID 14)\n",
            "25/08/06 08:29:50 INFO BlockManager: Found block rdd_27_1 locally\n",
            "25/08/06 08:29:50 INFO Executor: Finished task 1.0 in stage 11.0 (TID 14). 4027 bytes result sent to driver\n",
            "25/08/06 08:29:50 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 15) (24d0e2dab55f, executor driver, partition 2, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:50 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 14) in 48 ms on 24d0e2dab55f (executor driver) (2/4)\n",
            "25/08/06 08:29:50 INFO Executor: Running task 2.0 in stage 11.0 (TID 15)\n",
            "25/08/06 08:29:50 INFO BlockManager: Found block rdd_27_2 locally\n",
            "25/08/06 08:29:50 INFO Executor: Finished task 2.0 in stage 11.0 (TID 15). 4027 bytes result sent to driver\n",
            "25/08/06 08:29:50 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 16) (24d0e2dab55f, executor driver, partition 3, PROCESS_LOCAL, 7352 bytes) \n",
            "25/08/06 08:29:50 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 15) in 47 ms on 24d0e2dab55f (executor driver) (3/4)\n",
            "25/08/06 08:29:50 INFO Executor: Running task 3.0 in stage 11.0 (TID 16)\n",
            "25/08/06 08:29:50 INFO BlockManager: Found block rdd_27_3 locally\n",
            "25/08/06 08:29:50 INFO Executor: Finished task 3.0 in stage 11.0 (TID 16). 4070 bytes result sent to driver\n",
            "25/08/06 08:29:50 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 16) in 54 ms on 24d0e2dab55f (executor driver) (4/4)\n",
            "25/08/06 08:29:50 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:29:50 INFO DAGScheduler: ShuffleMapStage 11 (show at OptimizedSalesByRegion.java:39) finished in 0.226 s\n",
            "25/08/06 08:29:50 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 08:29:50 INFO DAGScheduler: running: Set()\n",
            "25/08/06 08:29:50 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 08:29:50 INFO DAGScheduler: failed: Set()\n",
            "+-------------+------------+\n",
            "|regionCountry|totalRevenue|\n",
            "+-------------+------------+\n",
            "+-------------+------------+\n",
            "\n",
            "✅ Optimized sales report saved.\n",
            "25/08/06 08:29:50 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:29:50 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 08:29:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:29:50 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:29:50 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:29:50 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:29:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:29:50 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:29:50 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:29:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-b44dfd53-7268-4539-8236-4965a851144e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SalesPerRegion.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SalesPerRegion {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Sales Per Region\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Read parquet files\n",
        "        Dataset<Row> officesDF = spark.read().parquet(\"/content/data/parquet/offices.parquet\").alias(\"offices\");\n",
        "        Dataset<Row> employeesDF = spark.read().parquet(\"/content/data/parquet/employees.parquet\").alias(\"employees\");\n",
        "        Dataset<Row> customersDF = spark.read().parquet(\"/content/data/parquet/customers.parquet\").alias(\"customers\");\n",
        "        Dataset<Row> paymentsDF = spark.read().parquet(\"/content/data/parquet/payments.parquet\").alias(\"payments\");\n",
        "\n",
        "        // Join: offices -> employees -> customers -> payments\n",
        "        Dataset<Row> salesByRegion = officesDF\n",
        "                .join(employeesDF, functions.col(\"offices.officeCode\").equalTo(functions.col(\"employees.officeCode\")))\n",
        "                .join(customersDF, functions.col(\"employees.employeeNumber\").equalTo(functions.col(\"customers.salesRepEmployeeNumber\")))\n",
        "                .join(paymentsDF, functions.col(\"customers.customerNumber\").equalTo(functions.col(\"payments.customerNumber\")))\n",
        "                .groupBy(functions.col(\"offices.country\"))\n",
        "                .agg(functions.sum(\"payments.amount\").alias(\"totalRevenue\"))\n",
        "                .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        // Display result\n",
        "        salesByRegion.show();\n",
        "\n",
        "        // Save output\n",
        "        salesByRegion.write()\n",
        "                .mode(\"overwrite\")\n",
        "                .parquet(\"/content/output/processed/sales_by_region.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Sales Per Region Parquet File Saved.\");\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i04Zc9nKf9qs",
        "outputId": "f488d018-e7f9-4869-e852-cebc0d0da33a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SalesPerRegion.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SalesPerRegion.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" SalesPerRegion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhxap1BIhSeZ",
        "outputId": "f92b7c13-2653-47a7-b694-7d967e7472c5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:38:12 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:38:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:38:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:38:13 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:38:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:38:13 INFO SparkContext: Submitted application: Sales Per Region\n",
            "25/08/06 08:38:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:38:13 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:38:13 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:38:13 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:38:13 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:38:13 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:38:13 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:38:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:38:14 INFO Utils: Successfully started service 'sparkDriver' on port 42727.\n",
            "25/08/06 08:38:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:38:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:38:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:38:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:38:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:38:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d93ace7-8d24-430e-bb68-ccdc9cb443c5\n",
            "25/08/06 08:38:14 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:38:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:38:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:38:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:38:15 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 08:38:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:38:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39119.\n",
            "25/08/06 08:38:15 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:39119\n",
            "25/08/06 08:38:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:38:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 39119, None)\n",
            "25/08/06 08:38:15 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:39119 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 39119, None)\n",
            "25/08/06 08:38:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 39119, None)\n",
            "25/08/06 08:38:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 39119, None)\n",
            "25/08/06 08:38:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:38:16 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 08:38:17 INFO InMemoryFileIndex: It took 101 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:18 INFO SparkContext: Starting job: parquet at SalesPerRegion.java:11\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Got job 0 (parquet at SalesPerRegion.java:11) with 1 output partitions\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at SalesPerRegion.java:11)\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at SalesPerRegion.java:11), which has no missing parents\n",
            "25/08/06 08:38:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:38:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:38:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:39119 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at SalesPerRegion.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7568 bytes) \n",
            "25/08/06 08:38:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 08:38:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2146 bytes result sent to driver\n",
            "25/08/06 08:38:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 741 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:19 INFO DAGScheduler: ResultStage 0 (parquet at SalesPerRegion.java:11) finished in 1.056 s\n",
            "25/08/06 08:38:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 08:38:19 INFO DAGScheduler: Job 0 finished: parquet at SalesPerRegion.java:11, took 1.189792 s\n",
            "25/08/06 08:38:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:39119 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:22 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:22 INFO SparkContext: Starting job: parquet at SalesPerRegion.java:12\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Got job 1 (parquet at SalesPerRegion.java:12) with 1 output partitions\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at SalesPerRegion.java:12)\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at SalesPerRegion.java:12), which has no missing parents\n",
            "25/08/06 08:38:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:38:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 08:38:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:39119 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at SalesPerRegion.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:38:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 08:38:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2067 bytes result sent to driver\n",
            "25/08/06 08:38:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 78 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:22 INFO DAGScheduler: ResultStage 1 (parquet at SalesPerRegion.java:12) finished in 0.130 s\n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Job 1 finished: parquet at SalesPerRegion.java:12, took 0.138360 s\n",
            "25/08/06 08:38:22 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:22 INFO SparkContext: Starting job: parquet at SalesPerRegion.java:13\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Got job 2 (parquet at SalesPerRegion.java:13) with 1 output partitions\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at SalesPerRegion.java:13)\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at SalesPerRegion.java:13), which has no missing parents\n",
            "25/08/06 08:38:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:38:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 08:38:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:39119 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at SalesPerRegion.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 08:38:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 08:38:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2391 bytes result sent to driver\n",
            "25/08/06 08:38:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 52 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:22 INFO DAGScheduler: ResultStage 2 (parquet at SalesPerRegion.java:13) finished in 0.107 s\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 08:38:22 INFO DAGScheduler: Job 2 finished: parquet at SalesPerRegion.java:13, took 0.116723 s\n",
            "25/08/06 08:38:22 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 08:38:23 INFO SparkContext: Starting job: parquet at SalesPerRegion.java:14\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Got job 3 (parquet at SalesPerRegion.java:14) with 1 output partitions\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at SalesPerRegion.java:14)\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at SalesPerRegion.java:14), which has no missing parents\n",
            "25/08/06 08:38:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:38:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:38:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:39119 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at SalesPerRegion.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:23 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes) \n",
            "25/08/06 08:38:23 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 08:38:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2054 bytes result sent to driver\n",
            "25/08/06 08:38:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 44 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:23 INFO DAGScheduler: ResultStage 3 (parquet at SalesPerRegion.java:14) finished in 0.087 s\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 08:38:23 INFO DAGScheduler: Job 3 finished: parquet at SalesPerRegion.java:14, took 0.099232 s\n",
            "25/08/06 08:38:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:39119 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:23 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:39119 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:39119 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:38:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 08:38:26 INFO CodeGenerator: Code generated in 647.640818 ms\n",
            "25/08/06 08:38:26 INFO CodeGenerator: Code generated in 652.563648 ms\n",
            "25/08/06 08:38:26 INFO CodeGenerator: Code generated in 648.738201 ms\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.2 MiB)\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1767.0 MiB)\n",
            "25/08/06 08:38:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:39119 (size: 34.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 08:38:26 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:39119 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:26 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:39119 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:26 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:27 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:27 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:27 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7994 bytes) \n",
            "25/08/06 08:38:27 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:27 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:27 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:27 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:27 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:27 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 08:38:27 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments.parquet/part-00000-c3edcc87-7baa-4b61-bf48-9cb12c7a8181-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:38:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers.parquet/part-00000-7f8fc258-1bd2-4256-ae31-afd444ff7374-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:38:27 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:38:27 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:38:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1795 bytes result sent to driver\n",
            "25/08/06 08:38:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes) \n",
            "25/08/06 08:38:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 533 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:27 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.547 s\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 08:38:27 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 08:38:27 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.565833 s\n",
            "25/08/06 08:38:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices.parquet/part-00000-164824fd-fabd-4f10-96d7-81cf771dd9a5-c000.snappy.parquet, range: 0-3086, partition values: [empty row]\n",
            "25/08/06 08:38:27 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 24.0 B, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 120.0 B, free 1766.9 MiB)\n",
            "25/08/06 08:38:27 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:39119 (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:38:27 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "+-------+------------+\n",
            "|country|totalRevenue|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n",
            "25/08/06 08:38:28 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:38:28 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 08:38:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.2 KiB, free 1766.7 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 200.2 KiB, free 1766.3 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.2 KiB, free 1766.5 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:38:28 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1985 bytes result sent to driver\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:39119 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:39119 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:39119 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 954 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.369 s\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.496368 s\n",
            "25/08/06 08:38:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 08:38:28 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4958 bytes result sent to driver\n",
            "25/08/06 08:38:28 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1402 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:28 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.426 s\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.516963 s\n",
            "25/08/06 08:38:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 13.6 KiB, free 1766.2 MiB)\n",
            "25/08/06 08:38:28 INFO CodeGenerator: Code generated in 20.661925 ms\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 1024.1 KiB, free 1765.2 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.1 MiB)\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 1027.1 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:28 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes) \n",
            "25/08/06 08:38:28 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 08:38:28 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices.parquet/part-00000-164824fd-fabd-4f10-96d7-81cf771dd9a5-c000.snappy.parquet, range: 0-3086, partition values: [empty row]\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.6 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 24d0e2dab55f:39119 (size: 5.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 362.0 B, free 1764.1 MiB)\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:39119 (size: 362.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:28 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:28 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:28 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:28 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 08:38:28 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7994 bytes) \n",
            "25/08/06 08:38:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:28 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 08:38:28 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 08:38:28 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers.parquet/part-00000-7f8fc258-1bd2-4256-ae31-afd444ff7374-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 08:38:28 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1985 bytes result sent to driver\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1764.1 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 24d0e2dab55f:39119 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 81 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7993 bytes) \n",
            "25/08/06 08:38:29 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 08:38:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 08:38:29 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments.parquet/part-00000-c3edcc87-7baa-4b61-bf48-9cb12c7a8181-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 08:38:29 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.122 s\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.156974 s\n",
            "25/08/06 08:38:29 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1024.1 KiB, free 1763.1 MiB)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 362.0 B, free 1763.1 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 24d0e2dab55f:39119 (size: 362.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1795 bytes result sent to driver\n",
            "25/08/06 08:38:29 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 84 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:29 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.102 s\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.180490 s\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 24.0 B, free 1763.1 MiB)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 120.0 B, free 1763.1 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 24d0e2dab55f:39119 (size: 120.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4958 bytes result sent to driver\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 87 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:29 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.109 s\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.220315 s\n",
            "25/08/06 08:38:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 08:38:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 1027.1 KiB, free 1762.1 MiB)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1762.1 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 24d0e2dab55f:39119 (size: 5.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 08:38:29 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 24d0e2dab55f:39119 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 24d0e2dab55f:39119 in memory (size: 362.0 B, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24d0e2dab55f:39119 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO SparkContext: Starting job: parquet at SalesPerRegion.java:31\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Got job 10 (parquet at SalesPerRegion.java:31) with 1 output partitions\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at SalesPerRegion.java:31)\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[35] at parquet at SalesPerRegion.java:31), which has no missing parents\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 24d0e2dab55f:39119 in memory (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 24d0e2dab55f:39119 in memory (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 205.0 KiB, free 1763.6 MiB)\n",
            "25/08/06 08:38:29 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1763.6 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 24d0e2dab55f:39119 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[35] at parquet at SalesPerRegion.java:31) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 08:38:29 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 24d0e2dab55f:39119 in memory (size: 5.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 08:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 08:38:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:29 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 08:38:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 24d0e2dab55f:39119 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 08:38:29 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 08:38:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 08:38:29 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 08:38:29 INFO FileOutputCommitter: Saved output of task 'attempt_202508060838297541712610113853521_0010_m_000000_10' to file:/content/output/processed/sales_by_region.parquet/_temporary/0/task_202508060838297541712610113853521_0010_m_000000\n",
            "25/08/06 08:38:29 INFO SparkHadoopMapRedUtil: attempt_202508060838297541712610113853521_0010_m_000000_10: Committed. Elapsed time: 4 ms.\n",
            "25/08/06 08:38:29 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2291 bytes result sent to driver\n",
            "25/08/06 08:38:29 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 363 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 08:38:29 INFO DAGScheduler: ResultStage 10 (parquet at SalesPerRegion.java:31) finished in 0.447 s\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 08:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 08:38:29 INFO DAGScheduler: Job 10 finished: parquet at SalesPerRegion.java:31, took 0.453783 s\n",
            "25/08/06 08:38:29 INFO FileFormatWriter: Start to commit write Job d2f512be-741b-4560-b25c-a953541468bb.\n",
            "25/08/06 08:38:29 INFO FileFormatWriter: Write Job d2f512be-741b-4560-b25c-a953541468bb committed. Elapsed time: 42 ms.\n",
            "25/08/06 08:38:29 INFO FileFormatWriter: Finished processing stats for write job d2f512be-741b-4560-b25c-a953541468bb.\n",
            "✅ Sales Per Region Parquet File Saved.\n",
            "25/08/06 08:38:29 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:38:29 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4040\n",
            "25/08/06 08:38:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:38:29 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:38:29 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:38:29 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:38:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:38:29 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:38:29 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:38:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-97f1193d-336f-4d9b-9057-c50664771146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Verify Parquet\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(\"/content/output/processed/sales_by_region.parquet\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRGcNDDKhVYO",
        "outputId": "6acc5581-2340-4845-d5fb-412f64ce1677"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|country|totalRevenue|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 – Subtask 3: Analyze top-performing offices\n",
        "\n"
      ],
      "metadata": {
        "id": "w4vgCXe6ihys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TopPerformingOffices.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class TopPerformingOffices {\n",
        "    public static void main(String[] args) {\n",
        "\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Top Performing Offices\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> offices = spark.read().parquet(\"/data/parquet/offices.parquet\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"/data/parquet/employees.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"/data/parquet/customers.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"/data/parquet/payments.parquet\");\n",
        "\n",
        "        Dataset<Row> result = offices\n",
        "                .join(employees, \"officeCode\")\n",
        "                .join(customers, employees.col(\"employeeNumber\").equalTo(customers.col(\"salesRepEmployeeNumber\")))\n",
        "                .join(payments, \"customerNumber\");\n",
        "\n",
        "        Dataset<Row> summary = result.groupBy(offices.col(\"officeCode\"), offices.col(\"city\"), offices.col(\"country\"))\n",
        "                .agg(\n",
        "                        countDistinct(\"customerNumber\").alias(\"customerCount\"),\n",
        "                        round(sum(\"amount\"), 2).alias(\"totalSales\")\n",
        "                )\n",
        "                .orderBy(desc(\"totalSales\"));\n",
        "\n",
        "        summary.show(false);\n",
        "\n",
        "        summary.write().mode(\"overwrite\").parquet(\"/output/processed/top_offices_summary.parquet\");\n",
        "\n",
        "        System.out.println(\"✅ Top-performing offices saved to /output/processed/top_offices_summary.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c607VViTFp",
        "outputId": "549adeaa-dd5f-4750-a7ef-dff12b557742"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting TopPerformingOffices.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" TopPerformingOffices.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" TopPerformingOffices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usYv7WbAix8e",
        "outputId": "5c00d9bc-a9dd-40f2-d38b-1d4476f1fbf3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:45:49 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:45:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:45:50 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:45:50 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:45:50 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:45:50 INFO SparkContext: Submitted application: Top Performing Offices\n",
            "25/08/06 08:45:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:45:50 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:45:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:45:50 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:45:50 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:45:50 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:45:50 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:45:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:45:50 INFO Utils: Successfully started service 'sparkDriver' on port 44555.\n",
            "25/08/06 08:45:50 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:45:50 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:45:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:45:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:45:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:45:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6467b36b-544a-4b4f-a4c4-3fe1509cf0c1\n",
            "25/08/06 08:45:50 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:45:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:45:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 08:45:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 08:45:51 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 08:45:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:45:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35545.\n",
            "25/08/06 08:45:51 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:35545\n",
            "25/08/06 08:45:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:45:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 35545, None)\n",
            "25/08/06 08:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:35545 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 35545, None)\n",
            "25/08/06 08:45:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 35545, None)\n",
            "25/08/06 08:45:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 35545, None)\n",
            "25/08/06 08:45:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:45:52 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/data/parquet/offices.parquet.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 08:45:54 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 08:45:54 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:45:54 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4041\n",
            "25/08/06 08:45:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:45:54 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:45:54 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:45:54 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:45:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:45:54 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:45:54 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:45:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-67b29c5e-a2c6-49a6-a7af-f0eda39b57b7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oho3ePBEi0FF",
        "outputId": "0eec59d9-68aa-4561-9ada-0f94817740a3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "required_files = [\n",
        "    \"offices.csv\",\n",
        "    \"employees.csv\",\n",
        "    \"customers.csv\",\n",
        "    \"payments.csv\"\n",
        "]\n",
        "\n",
        "for file in required_files:\n",
        "    path = f\"/content/drive/MyDrive/ClassicModels/{file}\"\n",
        "    print(f\"{'✅ Found' if os.path.exists(path) else '❌ Missing'}: {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxu71mskARt",
        "outputId": "0067f9b1-c15d-4be7-b2c8-be22c7062377"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found: offices.csv\n",
            "✅ Found: employees.csv\n",
            "✅ Found: customers.csv\n",
            "✅ Found: payments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TopPerformingOffices.java\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class TopPerformingOffices {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Top Performing Offices\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> offices = spark.read().parquet(\"parquet/offices.parquet\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"parquet/employees.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"parquet/customers.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"parquet/payments.parquet\");\n",
        "\n",
        "        // Rename conflicting 'city' column in offices\n",
        "        Dataset<Row> officesRenamed = offices.withColumnRenamed(\"city\", \"officeCity\");\n",
        "\n",
        "        // Join employees with offices\n",
        "        Dataset<Row> empWithOffices = employees.join(officesRenamed, \"officeCode\");\n",
        "\n",
        "        // Join employees with customers using employeeNumber -> salesRepEmployeeNumber\n",
        "        Dataset<Row> empCust = empWithOffices.join(customers,\n",
        "            empWithOffices.col(\"employeeNumber\").equalTo(customers.col(\"salesRepEmployeeNumber\")));\n",
        "\n",
        "        // Join with payments\n",
        "        Dataset<Row> finalJoin = empCust.join(payments, \"customerNumber\");\n",
        "\n",
        "        // Group by office and calculate total revenue\n",
        "        Dataset<Row> officePerformance = finalJoin.groupBy(\"officeCode\", \"officeCity\")\n",
        "            .agg(round(sum(\"amount\"), 2).alias(\"total_revenue\"))\n",
        "            .orderBy(desc(\"total_revenue\"));\n",
        "\n",
        "        officePerformance.show();\n",
        "\n",
        "        // Save output\n",
        "        officePerformance.write().mode(\"overwrite\").parquet(\"output/top_offices.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV3AwAUDkGKW",
        "outputId": "8e997a11-ed6e-45ad-cf28-389ea0ce8ce7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting TopPerformingOffices.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" TopPerformingOffices.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" TopPerformingOffices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM_huNTIlH01",
        "outputId": "fb3335e6-6c5a-472c-a47c-7494ac8b0298"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:01:56 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:01:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:01:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:01:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:01:57 INFO SparkContext: Submitted application: Top Performing Offices\n",
            "25/08/06 09:01:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:01:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:01:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:01:58 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:01:58 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:01:58 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:01:58 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:01:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:01:58 INFO Utils: Successfully started service 'sparkDriver' on port 33649.\n",
            "25/08/06 09:01:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:01:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:01:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:01:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:01:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:01:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dfb9742a-cdfa-4a3f-a5b3-77c340b32b37\n",
            "25/08/06 09:01:59 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:01:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:01:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:01:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:01:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:01:59 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 09:01:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:02:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46337.\n",
            "25/08/06 09:02:00 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:46337\n",
            "25/08/06 09:02:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:02:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 46337, None)\n",
            "25/08/06 09:02:00 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:46337 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 46337, None)\n",
            "25/08/06 09:02:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 46337, None)\n",
            "25/08/06 09:02:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 46337, None)\n",
            "25/08/06 09:02:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:02:00 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 09:02:02 INFO InMemoryFileIndex: It took 100 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:02:03 INFO SparkContext: Starting job: parquet at TopPerformingOffices.java:13\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Got job 0 (parquet at TopPerformingOffices.java:13) with 1 output partitions\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at TopPerformingOffices.java:13)\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at TopPerformingOffices.java:13), which has no missing parents\n",
            "25/08/06 09:02:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:02:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:02:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:46337 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:02:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at TopPerformingOffices.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7563 bytes) \n",
            "25/08/06 09:02:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:02:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2135 bytes result sent to driver\n",
            "25/08/06 09:02:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 883 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:04 INFO DAGScheduler: ResultStage 0 (parquet at TopPerformingOffices.java:13) finished in 1.255 s\n",
            "25/08/06 09:02:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:02:04 INFO DAGScheduler: Job 0 finished: parquet at TopPerformingOffices.java:13, took 1.385253 s\n",
            "25/08/06 09:02:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:46337 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:02:07 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:02:07 INFO SparkContext: Starting job: parquet at TopPerformingOffices.java:14\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Got job 1 (parquet at TopPerformingOffices.java:14) with 1 output partitions\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at TopPerformingOffices.java:14)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at TopPerformingOffices.java:14), which has no missing parents\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:02:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:46337 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:02:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at TopPerformingOffices.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes) \n",
            "25/08/06 09:02:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:02:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2016 bytes result sent to driver\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 76 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: ResultStage 1 (parquet at TopPerformingOffices.java:14) finished in 0.128 s\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 1 finished: parquet at TopPerformingOffices.java:14, took 0.138559 s\n",
            "25/08/06 09:02:07 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:02:07 INFO SparkContext: Starting job: parquet at TopPerformingOffices.java:15\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Got job 2 (parquet at TopPerformingOffices.java:15) with 1 output partitions\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at TopPerformingOffices.java:15)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at TopPerformingOffices.java:15), which has no missing parents\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 09:02:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:46337 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at TopPerformingOffices.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes) \n",
            "25/08/06 09:02:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 09:02:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2290 bytes result sent to driver\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 61 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: ResultStage 2 (parquet at TopPerformingOffices.java:15) finished in 0.107 s\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 2 finished: parquet at TopPerformingOffices.java:15, took 0.119371 s\n",
            "25/08/06 09:02:07 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:02:07 INFO SparkContext: Starting job: parquet at TopPerformingOffices.java:16\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Got job 3 (parquet at TopPerformingOffices.java:16) with 1 output partitions\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at TopPerformingOffices.java:16)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at TopPerformingOffices.java:16), which has no missing parents\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 09:02:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.2 MiB)\n",
            "25/08/06 09:02:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24d0e2dab55f:46337 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at TopPerformingOffices.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes) \n",
            "25/08/06 09:02:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 09:02:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1905 bytes result sent to driver\n",
            "25/08/06 09:02:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 49 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:07 INFO DAGScheduler: ResultStage 3 (parquet at TopPerformingOffices.java:16) finished in 0.090 s\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 09:02:07 INFO DAGScheduler: Job 3 finished: parquet at TopPerformingOffices.java:16, took 0.104197 s\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:02:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 09:02:11 INFO CodeGenerator: Code generated in 820.930188 ms\n",
            "25/08/06 09:02:11 INFO CodeGenerator: Code generated in 817.040372 ms\n",
            "25/08/06 09:02:11 INFO CodeGenerator: Code generated in 800.228726 ms\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1766.8 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1766.6 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24d0e2dab55f:46337 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.6 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7988 bytes) \n",
            "25/08/06 09:02:11 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.6 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:11 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:11 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7989 bytes) \n",
            "25/08/06 09:02:11 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:11 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.8 KiB, free 1766.5 MiB)\n",
            "25/08/06 09:02:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.4 MiB)\n",
            "25/08/06 09:02:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:11 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:12 INFO FileScanRDD: Reading File path: file:///content/parquet/customers.parquet/part-00000-58e93cce-6998-48da-9c12-61c9d55d0878-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:02:12 INFO FileScanRDD: Reading File path: file:///content/parquet/payments.parquet/part-00000-2525503a-9d1c-4fed-81d1-99f1e7f2d874-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 09:02:12 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 09:02:12 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:02:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 24d0e2dab55f:46337 in memory (size: 36.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24d0e2dab55f:46337 in memory (size: 36.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 24d0e2dab55f:46337 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:13 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 09:02:13 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 09:02:13 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2764 bytes result sent to driver\n",
            "25/08/06 09:02:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 5925 bytes result sent to driver\n",
            "25/08/06 09:02:13 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7987 bytes) \n",
            "25/08/06 09:02:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1885 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:13 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.920 s\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.942610 s\n",
            "25/08/06 09:02:13 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1781 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:13 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 09:02:13 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.877 s\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.938182 s\n",
            "25/08/06 09:02:13 INFO FileScanRDD: Reading File path: file:///content/parquet/offices.parquet/part-00000-552f4b81-10a5-40ec-8ec0-b2be9f2d4f08-c000.snappy.parquet, range: 0-3077, partition values: [empty row]\n",
            "25/08/06 09:02:13 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 09:02:13 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1962 bytes result sent to driver\n",
            "25/08/06 09:02:13 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 82 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:13 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.813 s\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 09:02:13 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.947164 s\n",
            "25/08/06 09:02:13 INFO CodeGenerator: Code generated in 64.17702 ms\n",
            "25/08/06 09:02:13 INFO CodeGenerator: Code generated in 60.490458 ms\n",
            "25/08/06 09:02:13 INFO CodeGenerator: Code generated in 53.115083 ms\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 MiB, free 1734.8 MiB)\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 1734.8 MiB)\n",
            "25/08/06 09:02:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 24d0e2dab55f:46337 (size: 5.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:13 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 32.0 MiB, free 1702.8 MiB)\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 1971.0 B, free 1702.8 MiB)\n",
            "25/08/06 09:02:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 24d0e2dab55f:46337 (size: 1971.0 B, free: 1767.5 MiB)\n",
            "25/08/06 09:02:13 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 MiB, free 1670.8 MiB)\n",
            "25/08/06 09:02:13 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 368.0 B, free 1670.8 MiB)\n",
            "25/08/06 09:02:13 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 24d0e2dab55f:46337 (size: 368.0 B, free: 1767.5 MiB)\n",
            "25/08/06 09:02:13 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:14 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:14 INFO CodeGenerator: Code generated in 149.633986 ms\n",
            "25/08/06 09:02:14 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.2 KiB, free 1670.6 MiB)\n",
            "25/08/06 09:02:14 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1670.6 MiB)\n",
            "25/08/06 09:02:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:14 INFO SparkContext: Created broadcast 13 from show at TopPerformingOffices.java:36\n",
            "25/08/06 09:02:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Registering RDD 23 (show at TopPerformingOffices.java:36) as input to shuffle 0\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Got map stage job 7 (show at TopPerformingOffices.java:36) with 1 output partitions\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (show at TopPerformingOffices.java:36)\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at show at TopPerformingOffices.java:36), which has no missing parents\n",
            "25/08/06 09:02:14 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 60.5 KiB, free 1670.5 MiB)\n",
            "25/08/06 09:02:14 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.1 KiB, free 1670.5 MiB)\n",
            "25/08/06 09:02:14 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 24d0e2dab55f:46337 (size: 25.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:14 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at show at TopPerformingOffices.java:36) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:14 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 09:02:14 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 09:02:14 INFO CodeGenerator: Code generated in 54.534702 ms\n",
            "25/08/06 09:02:14 INFO CodeGenerator: Code generated in 24.165787 ms\n",
            "25/08/06 09:02:14 INFO CodeGenerator: Code generated in 28.333102 ms\n",
            "25/08/06 09:02:14 INFO CodeGenerator: Code generated in 26.516249 ms\n",
            "25/08/06 09:02:14 INFO FileScanRDD: Reading File path: file:///content/parquet/employees.parquet/part-00000-1260969f-5b32-44e0-a933-5a0307db3515-c000.snappy.parquet, range: 0-3574, partition values: [empty row]\n",
            "25/08/06 09:02:14 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 09:02:14 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 5477 bytes result sent to driver\n",
            "25/08/06 09:02:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 332 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:14 INFO DAGScheduler: ShuffleMapStage 7 (show at TopPerformingOffices.java:36) finished in 0.374 s\n",
            "25/08/06 09:02:14 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:02:14 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:02:14 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:02:14 INFO DAGScheduler: failed: Set()\n",
            "+----------+----------+-------------+\n",
            "|officeCode|officeCity|total_revenue|\n",
            "+----------+----------+-------------+\n",
            "+----------+----------+-------------+\n",
            "\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#45),isnotnull(customerNumber#34)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#60)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.2 KiB, free 1670.3 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.2 KiB, free 1670.1 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 200.2 KiB, free 1669.9 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 24d0e2dab55f:46337 in memory (size: 1971.0 B, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1701.9 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1701.9 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1701.8 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 24d0e2dab55f:46337 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 09:02:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24d0e2dab55f:46337 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:15 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1702.1 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.1 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7988 bytes) \n",
            "25/08/06 09:02:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:15 INFO FileScanRDD: Reading File path: file:///content/parquet/payments.parquet/part-00000-2525503a-9d1c-4fed-81d1-99f1e7f2d874-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 13.6 KiB, free 1702.1 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.0 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 24d0e2dab55f:46337 in memory (size: 25.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7989 bytes) \n",
            "25/08/06 09:02:15 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 13.8 KiB, free 1702.1 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.1 MiB)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 24d0e2dab55f:46337 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:15 INFO FileScanRDD: Reading File path: file:///content/parquet/customers.parquet/part-00000-58e93cce-6998-48da-9c12-61c9d55d0878-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 24d0e2dab55f:46337 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 09:02:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 5839 bytes result sent to driver\n",
            "25/08/06 09:02:15 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7987 bytes) \n",
            "25/08/06 09:02:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 144 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:15 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.164 s\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.182869 s\n",
            "25/08/06 09:02:15 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 24d0e2dab55f:46337 in memory (size: 368.0 B, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 32.0 MiB, free 1702.3 MiB)\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 1702.3 MiB)\n",
            "25/08/06 09:02:15 INFO FileScanRDD: Reading File path: file:///content/parquet/offices.parquet/part-00000-552f4b81-10a5-40ec-8ec0-b2be9f2d4f08-c000.snappy.parquet, range: 0-3077, partition values: [empty row]\n",
            "25/08/06 09:02:15 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 24d0e2dab55f:46337 (size: 5.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:15 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:15 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 09:02:15 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2678 bytes result sent to driver\n",
            "25/08/06 09:02:15 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 145 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:15 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.173 s\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.219470 s\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:15 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1962 bytes result sent to driver\n",
            "25/08/06 09:02:15 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 101 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:15 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.180 s\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 09:02:15 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.261137 s\n",
            "25/08/06 09:02:15 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 MiB, free 1670.3 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.0 MiB, free 1638.3 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 368.0 B, free 1638.3 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1971.0 B, free 1638.3 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 24d0e2dab55f:46337 (size: 368.0 B, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 24d0e2dab55f:46337 (size: 1971.0 B, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:02:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 09:02:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#23),isnotnull(employeeNumber#18)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 24d0e2dab55f:46337 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 24d0e2dab55f:46337 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.2 KiB, free 1638.3 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1638.4 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 24d0e2dab55f:46337 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO SparkContext: Created broadcast 24 from parquet at TopPerformingOffices.java:39\n",
            "25/08/06 09:02:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 24d0e2dab55f:46337 in memory (size: 5.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Registering RDD 39 (parquet at TopPerformingOffices.java:39) as input to shuffle 1\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Got map stage job 11 (parquet at TopPerformingOffices.java:39) with 1 output partitions\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at TopPerformingOffices.java:39)\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[39] at parquet at TopPerformingOffices.java:39), which has no missing parents\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 60.5 KiB, free 1670.4 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 25.1 KiB, free 1670.3 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 24d0e2dab55f:46337 (size: 25.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[39] at parquet at TopPerformingOffices.java:39) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:16 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:16 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 09:02:16 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 24d0e2dab55f:46337 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO FileScanRDD: Reading File path: file:///content/parquet/employees.parquet/part-00000-1260969f-5b32-44e0-a933-5a0307db3515-c000.snappy.parquet, range: 0-3574, partition values: [empty row]\n",
            "25/08/06 09:02:16 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 09:02:16 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 5477 bytes result sent to driver\n",
            "25/08/06 09:02:16 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 77 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:16 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:16 INFO DAGScheduler: ShuffleMapStage 11 (parquet at TopPerformingOffices.java:39) finished in 0.121 s\n",
            "25/08/06 09:02:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:02:16 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:02:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:02:16 INFO SparkContext: Starting job: parquet at TopPerformingOffices.java:39\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Got job 12 (parquet at TopPerformingOffices.java:39) with 1 output partitions\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at TopPerformingOffices.java:39)\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[43] at parquet at TopPerformingOffices.java:39), which has no missing parents\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 205.1 KiB, free 1670.4 MiB)\n",
            "25/08/06 09:02:16 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1670.3 MiB)\n",
            "25/08/06 09:02:16 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 24d0e2dab55f:46337 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 09:02:16 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:02:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[43] at parquet at TopPerformingOffices.java:39) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:02:16 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:02:16 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 09:02:16 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:02:16 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:02:16 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:02:16 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 09:02:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCity\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"total_revenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary officeCity (STRING);\n",
            "  optional double total_revenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 09:02:17 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 09:02:17 INFO FileOutputCommitter: Saved output of task 'attempt_20250806090216566504708948243558_0012_m_000000_12' to file:/content/output/top_offices.parquet/_temporary/0/task_20250806090216566504708948243558_0012_m_000000\n",
            "25/08/06 09:02:17 INFO SparkHadoopMapRedUtil: attempt_20250806090216566504708948243558_0012_m_000000_12: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 09:02:17 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2291 bytes result sent to driver\n",
            "25/08/06 09:02:17 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 361 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:02:17 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:02:17 INFO DAGScheduler: ResultStage 12 (parquet at TopPerformingOffices.java:39) finished in 0.461 s\n",
            "25/08/06 09:02:17 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:02:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 09:02:17 INFO DAGScheduler: Job 12 finished: parquet at TopPerformingOffices.java:39, took 0.466008 s\n",
            "25/08/06 09:02:17 INFO FileFormatWriter: Start to commit write Job 2dd27f83-1600-41fd-80f3-3bb8bced8ffc.\n",
            "25/08/06 09:02:17 INFO FileFormatWriter: Write Job 2dd27f83-1600-41fd-80f3-3bb8bced8ffc committed. Elapsed time: 22 ms.\n",
            "25/08/06 09:02:17 INFO FileFormatWriter: Finished processing stats for write job 2dd27f83-1600-41fd-80f3-3bb8bced8ffc.\n",
            "25/08/06 09:02:17 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:02:17 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4041\n",
            "25/08/06 09:02:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:02:17 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:02:17 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:02:17 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:02:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:02:17 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:02:17 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:02:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-5069ee35-165b-48c0-bb1c-b0d7e9d7a2bb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Check Payments Schema\").getOrCreate()\n",
        "\n",
        "payments = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/payments.csv\")\n",
        "payments.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCs2oedGlLNx",
        "outputId": "90cc666a-571d-45c5-8a29-24170c8fef14"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customerNumber: string (nullable = true)\n",
            " |-- checkNumber: string (nullable = true)\n",
            " |-- paymentDate: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"CSV to Parquet\").getOrCreate()\n",
        "\n",
        "# List of CSVs to convert\n",
        "csv_files = [\"offices\", \"employees\", \"customers\", \"payments\"]\n",
        "\n",
        "# Convert each CSV to Parquet\n",
        "for name in csv_files:\n",
        "    df = spark.read.option(\"header\", True).csv(f\"/content/drive/MyDrive/ClassicModels/{name}.csv\")\n",
        "    df.write.mode(\"overwrite\").parquet(f\"/content/parquet/{name}.parquet\")\n",
        "    print(f\"✅ Converted {name}.csv to /content/parquet/{name}.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5Prqi1_laux",
        "outputId": "48019fd3-255b-40d6-c677-37c16895764e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Converted offices.csv to /content/parquet/offices.parquet\n",
            "✅ Converted employees.csv to /content/parquet/employees.parquet\n",
            "✅ Converted customers.csv to /content/parquet/customers.parquet\n",
            "✅ Converted payments.csv to /content/parquet/payments.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Read Parquet\").getOrCreate()\n",
        "\n",
        "# Read the Parquet file\n",
        "offices_df = spark.read.parquet(\"/content/parquet/offices.parquet\")\n",
        "\n",
        "# Show the data\n",
        "offices_df.show()\n",
        "\n",
        "# Print the schema\n",
        "offices_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF8pAqgQl7vo",
        "outputId": "f815922c-1ece-4372-9677-4b55310eef13"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------------+--------------------+------------+----------+---------+----------+---------+\n",
            "|officeCode|         city|           phone|        addressLine1|addressLine2|     state|  country|postalCode|territory|\n",
            "+----------+-------------+----------------+--------------------+------------+----------+---------+----------+---------+\n",
            "|         1|San Francisco| +1 650 219 4782|   100 Market Street|   Suite 300|        CA|      USA|     94080|       NA|\n",
            "|         2|       Boston| +1 215 837 0825|    1550 Court Place|   Suite 102|        MA|      USA|     02107|       NA|\n",
            "|         3|          NYC| +1 212 555 3000|523 East 53rd Street|     apt. 5A|        NY|      USA|     10022|       NA|\n",
            "|         4|        Paris| +33 14 723 4404|43 Rue Jouffroy D...|        null|      null|   France|     75017|     EMEA|\n",
            "|         5|        Tokyo| +81 33 224 5000|         4-1 Kioicho|        null|Chiyoda-Ku|    Japan|  102-8578|    Japan|\n",
            "|         6|       Sydney| +61 2 9264 2451|5-11 Wentworth Av...|    Floor #2|      null|Australia|  NSW 2010|     APAC|\n",
            "|         7|       London|+44 20 7877 2041| 25 Old Broad Street|     Level 7|      null|       UK|  EC2N 1HN|     EMEA|\n",
            "+----------+-------------+----------------+--------------------+------------+----------+---------+----------+---------+\n",
            "\n",
            "root\n",
            " |-- officeCode: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- phone: string (nullable = true)\n",
            " |-- addressLine1: string (nullable = true)\n",
            " |-- addressLine2: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- postalCode: string (nullable = true)\n",
            " |-- territory: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ReadTopOffices.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class ReadTopOffices {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Read Top Performing Offices\")\n",
        "            .master(\"local[*]\")\n",
        "            .config(\"spark.sql.warehouse.dir\", \"file:///tmp/spark-warehouse\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> df = spark.read().parquet(\"output/top_offices.parquet\");\n",
        "        df.show(false);\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBed3p7MmGC0",
        "outputId": "983225d2-a25e-4448-cac9-6d12a755252c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ReadTopOffices.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" ReadTopOffices.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" ReadTopOffices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfqiISFQn8GB",
        "outputId": "dec6c762-08a7-4a1f-9a49-7fddf40fc6d2"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:06:01 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:06:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:06:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:06:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:06:01 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:06:01 INFO SparkContext: Submitted application: Read Top Performing Offices\n",
            "25/08/06 09:06:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:06:01 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:06:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:06:02 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:06:02 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:06:02 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:06:02 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:06:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:06:02 INFO Utils: Successfully started service 'sparkDriver' on port 39547.\n",
            "25/08/06 09:06:02 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:06:02 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:06:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:06:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:06:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:06:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-adae25f2-33b4-4bf4-992f-477f7ec8d1e6\n",
            "25/08/06 09:06:02 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:06:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:06:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:06:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:06:03 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:06:03 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 09:06:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:06:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36937.\n",
            "25/08/06 09:06:03 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:36937\n",
            "25/08/06 09:06:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:06:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 36937, None)\n",
            "25/08/06 09:06:03 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:36937 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 36937, None)\n",
            "25/08/06 09:06:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 36937, None)\n",
            "25/08/06 09:06:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 36937, None)\n",
            "25/08/06 09:06:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:06:04 INFO SharedState: Warehouse path is 'file:/tmp/spark-warehouse'.\n",
            "25/08/06 09:06:07 INFO InMemoryFileIndex: It took 197 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:06:08 INFO SparkContext: Starting job: parquet at ReadTopOffices.java:11\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Got job 0 (parquet at ReadTopOffices.java:11) with 1 output partitions\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at ReadTopOffices.java:11)\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at ReadTopOffices.java:11), which has no missing parents\n",
            "25/08/06 09:06:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:06:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:06:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:36937 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:06:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:06:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at ReadTopOffices.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:06:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:06:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes) \n",
            "25/08/06 09:06:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:06:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1958 bytes result sent to driver\n",
            "25/08/06 09:06:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 829 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:06:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:06:09 INFO DAGScheduler: ResultStage 0 (parquet at ReadTopOffices.java:11) finished in 1.127 s\n",
            "25/08/06 09:06:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:06:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:06:09 INFO DAGScheduler: Job 0 finished: parquet at ReadTopOffices.java:11, took 1.235724 s\n",
            "25/08/06 09:06:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:36937 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:06:13 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 09:06:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 09:06:13 INFO CodeGenerator: Code generated in 445.308671 ms\n",
            "25/08/06 09:06:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.3 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:06:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:06:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:36937 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:06:13 INFO SparkContext: Created broadcast 1 from show at ReadTopOffices.java:12\n",
            "25/08/06 09:06:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:06:13 INFO SparkContext: Starting job: show at ReadTopOffices.java:12\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Got job 1 (show at ReadTopOffices.java:12) with 1 output partitions\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Final stage: ResultStage 1 (show at ReadTopOffices.java:12)\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at show at ReadTopOffices.java:12), which has no missing parents\n",
            "25/08/06 09:06:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:06:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:06:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:36937 (size: 6.2 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:06:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:06:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at ReadTopOffices.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:06:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:06:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7990 bytes) \n",
            "25/08/06 09:06:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:06:14 INFO FileScanRDD: Reading File path: file:///content/output/top_offices.parquet/part-00000-f9a92b52-f0c9-4d2c-8ff6-3719ec97363e-c000.snappy.parquet, range: 0-509, partition values: [empty row]\n",
            "25/08/06 09:06:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1703 bytes result sent to driver\n",
            "25/08/06 09:06:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 259 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:06:14 INFO DAGScheduler: ResultStage 1 (show at ReadTopOffices.java:12) finished in 0.312 s\n",
            "25/08/06 09:06:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:06:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:06:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:06:14 INFO DAGScheduler: Job 1 finished: show at ReadTopOffices.java:12, took 0.335361 s\n",
            "+----------+----------+-------------+\n",
            "|officeCode|officeCity|total_revenue|\n",
            "+----------+----------+-------------+\n",
            "+----------+----------+-------------+\n",
            "\n",
            "25/08/06 09:06:14 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:06:14 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4041\n",
            "25/08/06 09:06:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:06:14 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:06:14 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:06:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:06:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:06:14 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:06:14 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:06:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e2d423-f48a-4dcb-aea6-d81375361ce2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as sum_\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Fix Top Offices\").getOrCreate()\n",
        "\n",
        "# Load CSVs\n",
        "offices = spark.read.option(\"header\", True).csv(\"/content/drive/MyDrive/ClassicModels/offices.csv\")\n",
        "employees = spark.read.option(\"header\", True).csv(\"/content/drive/MyDrive/ClassicModels/employees.csv\")\n",
        "customers = spark.read.option(\"header\", True).csv(\"/content/drive/MyDrive/ClassicModels/customers.csv\")\n",
        "payments = spark.read.option(\"header\", True).csv(\"/content/drive/MyDrive/ClassicModels/payments.csv\")\n",
        "\n",
        "# Cast amount and drop nulls\n",
        "payments = payments.withColumn(\"amount\", col(\"amount\").cast(\"double\")).na.drop(subset=[\"amount\"])\n",
        "\n",
        "# Aliases for disambiguation\n",
        "offices = offices.alias(\"off\")\n",
        "employees = employees.alias(\"emp\")\n",
        "customers = customers.alias(\"cust\")\n",
        "payments = payments.alias(\"pay\")\n",
        "\n",
        "# Join and aggregate\n",
        "result = (\n",
        "    offices\n",
        "    .join(employees, col(\"off.officeCode\") == col(\"emp.officeCode\"))\n",
        "    .join(customers, col(\"emp.employeeNumber\") == col(\"cust.salesRepEmployeeNumber\"))\n",
        "    .join(payments, col(\"cust.customerNumber\") == col(\"pay.customerNumber\"))\n",
        "    .groupBy(col(\"off.officeCode\"), col(\"off.city\").alias(\"officeCity\"))\n",
        "    .agg(sum_(\"pay.amount\").alias(\"total_revenue\"))\n",
        "    .orderBy(col(\"total_revenue\").desc())\n",
        ")\n",
        "\n",
        "# Save to Parquet\n",
        "result.write.mode(\"overwrite\").parquet(\"output/top_offices.parquet\")\n"
      ],
      "metadata": {
        "id": "pzuIW944oUgO"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" ReadTopOffices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dDi0mO8oiFF",
        "outputId": "3a130c33-b3e0-4217-d5a6-e913022ba7c4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:08:58 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:08:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:08:58 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:08:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:08:58 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:08:58 INFO SparkContext: Submitted application: Read Top Performing Offices\n",
            "25/08/06 09:08:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:08:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:08:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:08:58 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:08:58 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:08:58 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:08:58 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:08:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:08:59 INFO Utils: Successfully started service 'sparkDriver' on port 45785.\n",
            "25/08/06 09:08:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:08:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:08:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:08:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:08:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:08:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12bc0c21-7dc9-4441-9441-bae7ab240539\n",
            "25/08/06 09:08:59 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:08:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:09:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:09:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:09:00 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:09:00 INFO Executor: Starting executor ID driver on host 24d0e2dab55f\n",
            "25/08/06 09:09:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:09:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42519.\n",
            "25/08/06 09:09:00 INFO NettyBlockTransferService: Server created on 24d0e2dab55f:42519\n",
            "25/08/06 09:09:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:09:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24d0e2dab55f, 42519, None)\n",
            "25/08/06 09:09:00 INFO BlockManagerMasterEndpoint: Registering block manager 24d0e2dab55f:42519 with 1767.6 MiB RAM, BlockManagerId(driver, 24d0e2dab55f, 42519, None)\n",
            "25/08/06 09:09:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24d0e2dab55f, 42519, None)\n",
            "25/08/06 09:09:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24d0e2dab55f, 42519, None)\n",
            "25/08/06 09:09:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:09:01 INFO SharedState: Warehouse path is 'file:/tmp/spark-warehouse'.\n",
            "25/08/06 09:09:03 INFO InMemoryFileIndex: It took 187 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:09:05 INFO SparkContext: Starting job: parquet at ReadTopOffices.java:11\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Got job 0 (parquet at ReadTopOffices.java:11) with 1 output partitions\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at ReadTopOffices.java:11)\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at ReadTopOffices.java:11), which has no missing parents\n",
            "25/08/06 09:09:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:09:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.5 MiB)\n",
            "25/08/06 09:09:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24d0e2dab55f:42519 (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:09:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:09:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at ReadTopOffices.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes) \n",
            "25/08/06 09:09:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:09:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1958 bytes result sent to driver\n",
            "25/08/06 09:09:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 891 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:09:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:06 INFO DAGScheduler: ResultStage 0 (parquet at ReadTopOffices.java:11) finished in 1.257 s\n",
            "25/08/06 09:09:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:09:06 INFO DAGScheduler: Job 0 finished: parquet at ReadTopOffices.java:11, took 1.394801 s\n",
            "25/08/06 09:09:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 24d0e2dab55f:42519 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:09:10 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 09:09:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 09:09:10 INFO CodeGenerator: Code generated in 461.604626 ms\n",
            "25/08/06 09:09:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.3 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:09:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:09:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24d0e2dab55f:42519 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:09:10 INFO SparkContext: Created broadcast 1 from show at ReadTopOffices.java:12\n",
            "25/08/06 09:09:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:11 INFO SparkContext: Starting job: show at ReadTopOffices.java:12\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Got job 1 (show at ReadTopOffices.java:12) with 1 output partitions\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Final stage: ResultStage 1 (show at ReadTopOffices.java:12)\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at show at ReadTopOffices.java:12), which has no missing parents\n",
            "25/08/06 09:09:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:09:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 09:09:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24d0e2dab55f:42519 (size: 6.2 KiB, free: 1767.6 MiB)\n",
            "25/08/06 09:09:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at ReadTopOffices.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (24d0e2dab55f, executor driver, partition 0, PROCESS_LOCAL, 7990 bytes) \n",
            "25/08/06 09:09:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:09:11 INFO FileScanRDD: Reading File path: file:///content/output/top_offices.parquet/part-00000-f8a4c8e5-cb4b-4d10-811d-f45cb20ef6c3-c000.snappy.parquet, range: 0-509, partition values: [empty row]\n",
            "25/08/06 09:09:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1746 bytes result sent to driver\n",
            "25/08/06 09:09:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 304 ms on 24d0e2dab55f (executor driver) (1/1)\n",
            "25/08/06 09:09:11 INFO DAGScheduler: ResultStage 1 (show at ReadTopOffices.java:12) finished in 0.383 s\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:09:11 INFO DAGScheduler: Job 1 finished: show at ReadTopOffices.java:12, took 0.412722 s\n",
            "+----------+----------+-------------+\n",
            "|officeCode|officeCity|total_revenue|\n",
            "+----------+----------+-------------+\n",
            "+----------+----------+-------------+\n",
            "\n",
            "25/08/06 09:09:11 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:09:11 INFO SparkUI: Stopped Spark web UI at http://24d0e2dab55f:4041\n",
            "25/08/06 09:09:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:09:11 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:09:11 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:09:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:09:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:09:11 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:09:11 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:09:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-24ada779-1fd4-426c-a4f2-b27b8ae2fd40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Performance Optimization"
      ],
      "metadata": {
        "id": "TnDPIYakpUuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as sum_, col, broadcast\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Performance Optimization Example\").getOrCreate()\n",
        "\n",
        "# Load data\n",
        "customers = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/customers.csv\")\n",
        "payments = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/payments.csv\")\n",
        "offices = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/offices.csv\")\n",
        "employees = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/employees.csv\")\n",
        "\n",
        "# Convert amounts to numeric\n",
        "payments = payments.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
        "\n",
        "# Persist reused DataFrame\n",
        "customers.cache()\n",
        "\n",
        "# Add aliases for disambiguation\n",
        "offices = offices.alias(\"off\")\n",
        "employees = employees.alias(\"emp\")\n",
        "customers = customers.alias(\"cust\")\n",
        "payments = payments.alias(\"pay\")\n",
        "\n",
        "# Join pipeline (corrected join keys and use aliases)\n",
        "joined_df = (customers\n",
        "             .join(payments, col(\"cust.customerNumber\") == col(\"pay.customerNumber\"))\n",
        "             .join(employees, col(\"cust.salesRepEmployeeNumber\") == col(\"emp.employeeNumber\"))\n",
        "             .join(broadcast(offices), col(\"emp.officeCode\") == col(\"off.officeCode\")))\n",
        "\n",
        "\n",
        "# Total revenue by office (using aliased city column)\n",
        "result = (joined_df\n",
        "          .groupBy(col(\"off.officeCode\"), col(\"off.city\").alias(\"officeCity\"))\n",
        "          .agg(sum_(\"pay.amount\").alias(\"total_revenue\"))\n",
        "          .orderBy(col(\"total_revenue\").desc()))\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k5GEnqyolH4",
        "outputId": "e95f87dc-41b4-48a2-f82d-bcfdcbd4d9e4"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------+\n",
            "|officeCode|officeCity|total_revenue|\n",
            "+----------+----------+-------------+\n",
            "+----------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as sum_, col, broadcast\n",
        "\n",
        "# Step 1: Create Spark Session\n",
        "spark = SparkSession.builder.appName(\"Performance Optimization\").getOrCreate()\n",
        "\n",
        "# Step 2: Load DataFrames\n",
        "customers = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/customers.csv\")\n",
        "payments = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/payments.csv\")\n",
        "employees = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/employees.csv\")\n",
        "offices = spark.read.option(\"header\", True).csv(\"drive/MyDrive/ClassicModels/offices.csv\")\n",
        "\n",
        "# Step 3: Cast amount column\n",
        "payments = payments.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
        "\n",
        "# Step 4: Join customers with payments\n",
        "cust_pay = customers.join(payments, \"customerNumber\")\n",
        "\n",
        "# Step 5: Join with employees\n",
        "cust_pay_emp = cust_pay.join(employees, cust_pay.salesRepEmployeeNumber == employees.employeeNumber)\n",
        "\n",
        "# Step 6: Join with offices and select required columns only\n",
        "cust_pay_emp_office = cust_pay_emp.join(broadcast(offices), \"officeCode\") \\\n",
        "    .select(\"officeCode\",\n",
        "            offices[\"city\"].alias(\"officeCity\"),\n",
        "            \"amount\")\n",
        "\n",
        "# Step 7: Group by and compute total revenue\n",
        "result = (cust_pay_emp_office\n",
        "          .groupBy(\"officeCode\", \"officeCity\")\n",
        "          .agg(sum_(\"amount\").alias(\"total_revenue\"))\n",
        "          .orderBy(col(\"total_revenue\").desc()))\n",
        "\n",
        "# Step 8: Show result\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wagzJ2q1pYr8",
        "outputId": "86b5b3b1-ded7-46ac-bae3-fb3fc5020c99"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------+\n",
            "|officeCode|officeCity|total_revenue|\n",
            "+----------+----------+-------------+\n",
            "+----------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "payments.select(\"amount\").show(5)\n",
        "payments.filter(col(\"amount\").isNull()).count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg871zdjp3y0",
        "outputId": "b6b57567-3cd2-4660-d4a2-26dfb0e9b717"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|  amount|\n",
            "+--------+\n",
            "| 6066.78|\n",
            "|14571.44|\n",
            "| 1676.14|\n",
            "|14191.12|\n",
            "|32641.98|\n",
            "+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cust_pay = customers.join(payments, \"customerNumber\")\n",
        "cust_pay.show(5)\n",
        "cust_pay.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wS-gxRPqP1Z",
        "outputId": "a9bbf992-94ff-43af-9ca3-079f476d7a7d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+-----------+-----------+--------+\n",
            "|customerNumber|      customerName|contactLastName|contactFirstName|     phone|   addressLine1|addressLine2|     city|state|postalCode|country|salesRepEmployeeNumber|creditLimit|checkNumber|paymentDate|  amount|\n",
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+-----------+-----------+--------+\n",
            "|           103| Atelier graphique|        Schmitt|         Carine |40.32.2555| 54, rue Royale|        null|   Nantes| null|     44000| France|                1370.0|    21000.0|   OM314933| 2004-12-18| 1676.14|\n",
            "|           103| Atelier graphique|        Schmitt|         Carine |40.32.2555| 54, rue Royale|        null|   Nantes| null|     44000| France|                1370.0|    21000.0|   JM555205| 2003-06-05|14571.44|\n",
            "|           103| Atelier graphique|        Schmitt|         Carine |40.32.2555| 54, rue Royale|        null|   Nantes| null|     44000| France|                1370.0|    21000.0|   HQ336336| 2004-10-19| 6066.78|\n",
            "|           112|Signal Gift Stores|           King|            Jean|7025551838|8489 Strong St.|        null|Las Vegas|   NV|     83030|    USA|                1166.0|    71800.0|   ND748579| 2004-08-20|33347.88|\n",
            "|           112|Signal Gift Stores|           King|            Jean|7025551838|8489 Strong St.|        null|Las Vegas|   NV|     83030|    USA|                1166.0|    71800.0|    HQ55022| 2003-06-06|32641.98|\n",
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+-----------+-----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "273"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cust_pay_emp = cust_pay.join(employees, cust_pay.salesRepEmployeeNumber == employees.employeeNumber)\n",
        "cust_pay_emp.select(\"salesRepEmployeeNumber\", \"employeeNumber\").show(5)\n",
        "cust_pay_emp.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP-v0QbFqSeU",
        "outputId": "7bf1d158-2065-4b05-8050-96f9e2a2eda0"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+--------------+\n",
            "|salesRepEmployeeNumber|employeeNumber|\n",
            "+----------------------+--------------+\n",
            "+----------------------+--------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers.join(payments, \"customerNumber\", \"inner\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQSWTtUeqWXJ",
        "outputId": "cab130b8-081a-41f7-ff01-2bfb8f6640bd"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+----------+--------+----------+---------+----------------------+-----------+-----------+-----------+--------+\n",
            "|customerNumber|        customerName|contactLastName|contactFirstName|       phone|        addressLine1|addressLine2|      city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|checkNumber|paymentDate|  amount|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+----------+--------+----------+---------+----------------------+-----------+-----------+-----------+--------+\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        null|    Nantes|    null|     44000|   France|                1370.0|    21000.0|   OM314933| 2004-12-18| 1676.14|\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        null|    Nantes|    null|     44000|   France|                1370.0|    21000.0|   JM555205| 2003-06-05|14571.44|\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        null|    Nantes|    null|     44000|   France|                1370.0|    21000.0|   HQ336336| 2004-10-19| 6066.78|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        null| Las Vegas|      NV|     83030|      USA|                1166.0|    71800.0|   ND748579| 2004-08-20|33347.88|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        null| Las Vegas|      NV|     83030|      USA|                1166.0|    71800.0|    HQ55022| 2003-06-06|32641.98|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        null| Las Vegas|      NV|     83030|      USA|                1166.0|    71800.0|   BO864823| 2004-12-17|14191.12|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3| Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|    NR27552| 2004-03-10|44894.74|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3| Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|   NP603840| 2003-05-31| 7565.08|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3| Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|   MA765515| 2004-12-15|82261.22|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3| Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|    GG31455| 2003-05-20|45864.03|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        null|    Nantes|    null|     44000|   France|                1370.0|   118200.0|    NG94694| 2005-02-22|49523.67|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        null|    Nantes|    null|     44000|   France|                1370.0|   118200.0|   LN373447| 2004-08-08|47924.19|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        null|    Nantes|    null|     44000|   France|                1370.0|   118200.0|   DB933704| 2004-11-14|19501.82|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|   Stavern|    null|      4110|   Norway|                1504.0|    81700.0|   MA302151| 2004-11-28|34638.14|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|   Stavern|    null|      4110|   Norway|                1504.0|    81700.0|   KI831359| 2004-11-04|17876.32|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|   Stavern|    null|      4110|   Norway|                1504.0|    81700.0|   FD317790| 2003-10-28| 1491.38|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|   Stavern|    null|      4110|   Norway|                1504.0|    81700.0|   DB889831| 2003-02-16|50218.95|\n",
            "|           124|Mini Gifts Distri...|         Nelson|           Susan|  4155551450|     5677 Strong St.|        null|San Rafael|      CA|     97562|      USA|                1165.0|   210500.0|   NT141748| 2003-11-25|45084.38|\n",
            "|           124|Mini Gifts Distri...|         Nelson|           Susan|  4155551450|     5677 Strong St.|        null|San Rafael|      CA|     97562|      USA|                1165.0|   210500.0|   LF217299| 2004-03-26| 43369.3|\n",
            "|           124|Mini Gifts Distri...|         Nelson|           Susan|  4155551450|     5677 Strong St.|        null|San Rafael|      CA|     97562|      USA|                1165.0|   210500.0|   KI131716| 2003-08-15|111654.4|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+----------+--------+----------+---------+----------------------+-----------+-----------+-----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as sum_, broadcast\n",
        "\n",
        "# Step 1: Cast to correct types\n",
        "cust_pay = cust_pay.withColumn(\"salesRepEmployeeNumber\", col(\"salesRepEmployeeNumber\").cast(\"int\"))\n",
        "employees = employees.withColumn(\"employeeNumber\", col(\"employeeNumber\").cast(\"int\"))\n",
        "offices = offices.withColumn(\"officeCode\", col(\"officeCode\").cast(\"string\"))\n",
        "\n",
        "# Step 2: Join cust_pay with employees\n",
        "cust_pay_emp = cust_pay.join(employees, cust_pay.salesRepEmployeeNumber == employees.employeeNumber)\n",
        "\n",
        "# Step 3: Rename offices.city to officeCity BEFORE joining\n",
        "offices = offices.withColumnRenamed(\"city\", \"officeCity\")\n",
        "\n",
        "# Step 4: Join with offices using broadcast\n",
        "cust_pay_emp_office = cust_pay_emp.join(broadcast(offices), \"officeCode\")\n",
        "\n",
        "# Step 5: DROP any other 'city' column that might still exist\n",
        "columns_to_drop = [colname for colname in cust_pay_emp_office.columns if colname == \"city\"]\n",
        "cust_pay_emp_office = cust_pay_emp_office.drop(*columns_to_drop)\n",
        "\n",
        "# Step 6: Now group and calculate revenue\n",
        "result = (\n",
        "    cust_pay_emp_office\n",
        "    .groupBy(\"officeCode\", \"officeCity\")\n",
        "    .agg(sum_(\"amount\").alias(\"total_revenue\"))\n",
        "    .orderBy(col(\"total_revenue\").desc())\n",
        ")\n",
        "\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdF2hk0wqajT",
        "outputId": "a6340f94-3e9d-4cba-f140-abd425d077f7"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+------------------+\n",
            "|officeCode|   officeCity|     total_revenue|\n",
            "+----------+-------------+------------------+\n",
            "|         4|        Paris|2819168.9000000004|\n",
            "|         1|San Francisco|1337439.5799999998|\n",
            "|         7|       London|1324325.9000000001|\n",
            "|         3|          NYC|        1072619.47|\n",
            "|         6|       Sydney|1007292.9799999997|\n",
            "|         2|       Boston|         835882.33|\n",
            "|         5|        Tokyo|457110.06999999995|\n",
            "+----------+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the DataFrame to reuse in further analysis\n",
        "cust_pay_emp_office.cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrK_Ku3jqv-w",
        "outputId": "0d246bf0-5994-4107-c1ac-3a76e0264f31"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[officeCode: string, customerNumber: string, customerName: string, contactLastName: string, contactFirstName: string, phone: string, addressLine1: string, addressLine2: string, state: string, postalCode: string, country: string, salesRepEmployeeNumber: int, creditLimit: string, checkNumber: string, paymentDate: string, amount: double, employeeNumber: int, lastName: string, firstName: string, extension: string, email: string, reportsTo: string, jobTitle: string, officeCity: string, phone: string, addressLine1: string, addressLine2: string, state: string, country: string, postalCode: string, territory: string]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import StorageLevel\n",
        "\n",
        "# Persist with MEMORY_AND_DISK if it's large\n",
        "cust_pay_emp_office.persist(StorageLevel.MEMORY_AND_DISK)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGC7cD_0rSyQ",
        "outputId": "9c7520ff-c705-468c-eec4-ef9d1bf27351"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[officeCode: string, customerNumber: string, customerName: string, contactLastName: string, contactFirstName: string, phone: string, addressLine1: string, addressLine2: string, state: string, postalCode: string, country: string, salesRepEmployeeNumber: int, creditLimit: string, checkNumber: string, paymentDate: string, amount: double, employeeNumber: int, lastName: string, firstName: string, extension: string, email: string, reportsTo: string, jobTitle: string, officeCity: string, phone: string, addressLine1: string, addressLine2: string, state: string, country: string, postalCode: string, territory: string]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cust_pay_emp_office = cust_pay_emp.join(broadcast(offices), \"officeCode\")\n"
      ],
      "metadata": {
        "id": "1QWl_U9BrVDM"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Count customers using mapPartitions\n",
        "def count_customers(iterator):\n",
        "    count = 0\n",
        "    for _ in iterator:\n",
        "        count += 1\n",
        "    yield count\n",
        "\n",
        "customer_count_rdd = cust_pay.rdd.mapPartitions(count_customers)\n",
        "print(\"Customer count (mapPartitions):\", customer_count_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEkkSGizrXBQ",
        "outputId": "cf942269-a4de-47fa-f93a-b87265530281"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer count (mapPartitions): [273]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ BEFORE calling action (lazy eval check)\")\n",
        "cust_pay_emp_office.groupBy(\"officeCode\").count().show()\n",
        "print(\"✅ AFTER calling action\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWw-sAvtrZ-A",
        "outputId": "ae1a9d1b-7e8a-4016-99af-59a05764325c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BEFORE calling action (lazy eval check)\n",
            "+----------+-----+\n",
            "|officeCode|count|\n",
            "+----------+-----+\n",
            "|         7|   42|\n",
            "|         3|   37|\n",
            "|         5|   14|\n",
            "|         6|   30|\n",
            "|         1|   34|\n",
            "|         4|   87|\n",
            "|         2|   29|\n",
            "+----------+-----+\n",
            "\n",
            "✅ AFTER calling action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRTOEl9-rbzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}