{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD3RJdwj65U6A9vIPZyinX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indresh0007/PySpark-Indresh/blob/main/UseCase_ProductSaleManagement_JAVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbQXO6BHrCEF",
        "outputId": "aea37131-6712-4e89-c60c-9c9b2bd1bc72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs5I_EvurHCg",
        "outputId": "8abf9288-8d42-419b-ee04-5fb83903eb5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Spark Project\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "zo3ErUIKrLdp",
        "outputId": "6894559a-2235-440a-b9c6-4d20fbdc05a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9f12cd1fd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://06c738b394b7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ClassicModels Spark Project</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "YSeMeAxhr6oa",
        "outputId": "f8fbed78-ecb7-42d3-a131-851da94c7e0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40fa3d82-83a5-4a93-ba8c-a2ce424d4be3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40fa3d82-83a5-4a93-ba8c-a2ce424d4be3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving customers.csv to customers.csv\n",
            "Saving employees.csv to employees.csv\n",
            "Saving offices.csv to offices.csv\n",
            "Saving orderdetails.csv to orderdetails.csv\n",
            "Saving orders.csv to orders.csv\n",
            "Saving payments.csv to payments.csv\n",
            "Saving productlines.csv to productlines.csv\n",
            "Saving products.csv to products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "import os\n",
        "\n",
        "# Output directory\n",
        "parquet_output_path = \"/content/data/parquet\"\n",
        "os.makedirs(parquet_output_path, exist_ok=True)\n",
        "\n",
        "# Define schemas for all 8 tables\n",
        "schemas = {\n",
        "    \"productlines\": StructType([\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"textDescription\", StringType())\n",
        "    ]),\n",
        "    \"products\": StructType([\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"productName\", StringType()),\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"productScale\", StringType()),\n",
        "        StructField(\"productVendor\", StringType()),\n",
        "        StructField(\"productDescription\", StringType()),\n",
        "        StructField(\"quantityInStock\", IntegerType()),\n",
        "        StructField(\"buyPrice\", DoubleType()),\n",
        "        StructField(\"MSRP\", DoubleType())\n",
        "    ]),\n",
        "    \"offices\": StructType([\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"territory\", StringType())\n",
        "    ]),\n",
        "    \"employees\": StructType([\n",
        "        StructField(\"employeeNumber\", IntegerType()),\n",
        "        StructField(\"lastName\", StringType()),\n",
        "        StructField(\"firstName\", StringType()),\n",
        "        StructField(\"extension\", StringType()),\n",
        "        StructField(\"email\", StringType()),\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"reportsTo\", IntegerType()),\n",
        "        StructField(\"jobTitle\", StringType())\n",
        "    ]),\n",
        "    \"customers\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"customerName\", StringType()),\n",
        "        StructField(\"contactLastName\", StringType()),\n",
        "        StructField(\"contactFirstName\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"salesRepEmployeeNumber\", IntegerType()),\n",
        "        StructField(\"creditLimit\", DoubleType())\n",
        "    ]),\n",
        "    \"payments\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"checkNumber\", StringType()),\n",
        "        StructField(\"paymentDate\", DateType()),\n",
        "        StructField(\"amount\", DoubleType())\n",
        "    ]),\n",
        "    \"orders\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"orderDate\", DateType()),\n",
        "        StructField(\"requiredDate\", DateType()),\n",
        "        StructField(\"shippedDate\", DateType()),\n",
        "        StructField(\"status\", StringType()),\n",
        "        StructField(\"comments\", StringType()),\n",
        "        StructField(\"customerNumber\", IntegerType())\n",
        "    ]),\n",
        "    \"orderdetails\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"quantityOrdered\", IntegerType()),\n",
        "        StructField(\"priceEach\", DoubleType()),\n",
        "        StructField(\"orderLineNumber\", IntegerType())\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Read and convert CSVs to Parquet\n",
        "for table, schema in schemas.items():\n",
        "    csv_path = f\"/content/{table}.csv\"\n",
        "    parquet_path = f\"{parquet_output_path}/{table}\"\n",
        "\n",
        "    print(f\"Reading {csv_path}\")\n",
        "    df = spark.read.option(\"header\", True).schema(schema).csv(csv_path)\n",
        "\n",
        "    print(f\"Writing to {parquet_path}\")\n",
        "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "\n",
        "    df.show(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bMgSm_sDqD",
        "outputId": "bf663113-cbd9-45bc-d528-a3cf11a431d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/productlines.csv\n",
            "Writing to /content/data/parquet/productlines\n",
            "+------------+--------------------+\n",
            "| productLine|     textDescription|\n",
            "+------------+--------------------+\n",
            "|Classic Cars|Attention car ent...|\n",
            "| Motorcycles|Our motorcycles a...|\n",
            "+------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/products.csv\n",
            "Writing to /content/data/parquet/products\n",
            "+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+--------+-----+\n",
            "|productCode|         productName| productLine|productScale|       productVendor|  productDescription|quantityInStock|buyPrice| MSRP|\n",
            "+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+--------+-----+\n",
            "|   S10_1678|1969 Harley David...| Motorcycles|        1:10|     Min Lin Diecast|This replica feat...|           7933|   48.81| 95.7|\n",
            "|   S10_1949|1952 Alpine Renau...|Classic Cars|        1:10|Classic Metal Cre...|Turnable front wh...|           7305|   98.58|214.3|\n",
            "+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+--------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/offices.csv\n",
            "Writing to /content/data/parquet/offices\n",
            "+----------+-------------+---------------+-----------------+------------+-----+-------+----------+---------+\n",
            "|officeCode|         city|          phone|     addressLine1|addressLine2|state|country|postalCode|territory|\n",
            "+----------+-------------+---------------+-----------------+------------+-----+-------+----------+---------+\n",
            "|         1|San Francisco|+1 650 219 4782|100 Market Street|   Suite 300|   CA|    USA|     94080|       NA|\n",
            "|         2|       Boston|+1 215 837 0825| 1550 Court Place|   Suite 102|   MA|    USA|     02107|       NA|\n",
            "+----------+-------------+---------------+-----------------+------------+-----+-------+----------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/employees.csv\n",
            "Writing to /content/data/parquet/employees\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+---------+\n",
            "|employeeNumber| lastName|firstName|extension|               email|officeCode|reportsTo| jobTitle|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+---------+\n",
            "|          1002|   Murphy|    Diane|    x5800|dmurphy@classicmo...|         1|     null|President|\n",
            "|          1056|Patterson|     Mary|    x4611|mpatterso@classic...|         1|     null| VP Sales|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/customers.csv\n",
            "Writing to /content/data/parquet/customers\n",
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+\n",
            "|customerNumber|      customerName|contactLastName|contactFirstName|     phone|   addressLine1|addressLine2|     city|state|postalCode|country|salesRepEmployeeNumber|creditLimit|\n",
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+\n",
            "|           103| Atelier graphique|        Schmitt|         Carine |40.32.2555| 54, rue Royale|        null|   Nantes| null|     44000| France|                  null|    21000.0|\n",
            "|           112|Signal Gift Stores|           King|            Jean|7025551838|8489 Strong St.|        null|Las Vegas|   NV|     83030|    USA|                  null|    71800.0|\n",
            "+--------------+------------------+---------------+----------------+----------+---------------+------------+---------+-----+----------+-------+----------------------+-----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/payments.csv\n",
            "Writing to /content/data/parquet/payments\n",
            "+--------------+-----------+-----------+--------+\n",
            "|customerNumber|checkNumber|paymentDate|  amount|\n",
            "+--------------+-----------+-----------+--------+\n",
            "|           103|   HQ336336| 2004-10-19| 6066.78|\n",
            "|           103|   JM555205| 2003-06-05|14571.44|\n",
            "+--------------+-----------+-----------+--------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/orders.csv\n",
            "Writing to /content/data/parquet/orders\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                null|           363|\n",
            "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
            "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Reading /content/orderdetails.csv\n",
            "Writing to /content/data/parquet/orderdetails\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|orderNumber|productCode|quantityOrdered|priceEach|orderLineNumber|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|      10100|   S18_1749|             30|    136.0|              3|\n",
            "|      10100|   S18_2248|             50|    55.09|              2|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/output/processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "upBuFZdhtD5U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load orderdetails and products\n",
        "orderdetails_df = spark.read.parquet(\"/content/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/data/parquet/products\")\n",
        "\n",
        "# Join to get product names and calculate total quantity\n",
        "top_products_df = orderdetails_df.join(\n",
        "    products_df,\n",
        "    on=\"productCode\"\n",
        ").groupBy(\"productName\") \\\n",
        " .agg({\"quantityOrdered\": \"sum\"}) \\\n",
        " .withColumnRenamed(\"sum(quantityOrdered)\", \"totalQuantity\") \\\n",
        " .orderBy(\"totalQuantity\", ascending=False) \\\n",
        " .limit(10)\n",
        "\n",
        "top_products_df.show()\n",
        "\n",
        "# Save to Parquet\n",
        "top_products_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/top_10_products_quantity.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdvzJaIAtYPY",
        "outputId": "6c29bd35-5829-41fa-e7f4-bc8893ad213c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|         productName|totalQuantity|\n",
            "+--------------------+-------------+\n",
            "|1992 Ferrari 360 ...|         1808|\n",
            "|1937 Lincoln Berline|         1111|\n",
            "|American Airlines...|         1085|\n",
            "|1941 Chevrolet Sp...|         1076|\n",
            "|1930 Buick Marque...|         1074|\n",
            "|    1940s Ford truck|         1061|\n",
            "|1969 Harley David...|         1057|\n",
            "|   1957 Chevy Pickup|         1056|\n",
            "|1964 Mercedes Tou...|         1053|\n",
            "|1956 Porsche 356A...|         1052|\n",
            "+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = spark.read.parquet(\"/content/data/parquet/orders\")\n",
        "\n",
        "# Join: orders -> orderdetails -> products\n",
        "revenue_df = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "    .join(products_df, \"productCode\") \\\n",
        "    .withColumn(\"revenue\", orderdetails_df[\"quantityOrdered\"] * orderdetails_df[\"priceEach\"]) \\\n",
        "    .groupBy(\"productName\") \\\n",
        "    .agg({\"revenue\": \"sum\"}) \\\n",
        "    .withColumnRenamed(\"sum(revenue)\", \"totalRevenue\") \\\n",
        "    .orderBy(\"totalRevenue\", ascending=False)\n",
        "\n",
        "revenue_df.show()\n",
        "\n",
        "# Save to Parquet\n",
        "revenue_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/product_revenue.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Opiib0MtZmx",
        "outputId": "78d44a23-0813-4f96-de62-8c3f6d5bf7d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+\n",
            "|         productName|      totalRevenue|\n",
            "+--------------------+------------------+\n",
            "|1992 Ferrari 360 ...|         276839.98|\n",
            "|   2001 Ferrari Enzo|         190755.86|\n",
            "|1952 Alpine Renau...|190017.95999999996|\n",
            "|2003 Harley-David...|170685.99999999997|\n",
            "|   1968 Ford Mustang|161531.47999999992|\n",
            "|    1969 Ford Falcon|         152543.02|\n",
            "|1980s Black Hawk ...|144959.90999999997|\n",
            "|1998 Chrysler Ply...|142530.62999999998|\n",
            "|1917 Grand Tourin...|140535.60000000003|\n",
            "|    2002 Suzuki XREO|135767.03000000003|\n",
            "|1956 Porsche 356A...|         134240.71|\n",
            "|  1969 Corvair Monza|132363.78999999998|\n",
            "|1928 Mercedes-Ben...|132275.97999999998|\n",
            "|1957 Corvette Con...|130749.31000000001|\n",
            "| 1972 Alfa Romeo GTA|127924.31999999999|\n",
            "|1962 LanciaA Delt...|123123.00999999998|\n",
            "|1970 Triumph Spit...|         122254.75|\n",
            "|1976 Ford Gran To...|          121890.6|\n",
            "|1948 Porsche Type...|         121653.46|\n",
            "|      1958 Setra Bus|119085.24999999999|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load customers\n",
        "customers_df = spark.read.parquet(\"/content/data/parquet/customers\")\n",
        "\n",
        "# Join orders → orderdetails\n",
        "order_with_details = orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Calculate total per order\n",
        "order_totals_df = order_with_details.withColumn(\n",
        "    \"orderTotal\", orderdetails_df[\"quantityOrdered\"] * orderdetails_df[\"priceEach\"]\n",
        ").groupBy(\"orderNumber\", \"customerNumber\") \\\n",
        " .agg({\"orderTotal\": \"sum\"}) \\\n",
        " .withColumnRenamed(\"sum(orderTotal)\", \"totalOrderValue\")\n",
        "\n",
        "# Average per customer\n",
        "avg_order_value_df = order_totals_df.groupBy(\"customerNumber\") \\\n",
        "    .agg({\"totalOrderValue\": \"avg\"}) \\\n",
        "    .withColumnRenamed(\"avg(totalOrderValue)\", \"avgOrderValue\") \\\n",
        "    .join(customers_df.select(\"customerNumber\", \"customerName\"), \"customerNumber\") \\\n",
        "    .orderBy(\"avgOrderValue\", ascending=False)\n",
        "\n",
        "avg_order_value_df.show()\n",
        "\n",
        "# Save to Parquet\n",
        "avg_order_value_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/avg_order_value_by_customer.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2lCDsoXtdWU",
        "outputId": "5a07fffc-889d-4dcc-ca16-e7f9368c44d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+--------------------+\n",
            "|customerNumber|     avgOrderValue|        customerName|\n",
            "+--------------+------------------+--------------------+\n",
            "|           298|          54388.96|     Vida Sport, Ltd|\n",
            "|           187|          49470.03|      AV Stores, Co.|\n",
            "|           286|         45272.685|Marta's Replicas Co.|\n",
            "|           227|           44954.9|Heintze Collectables|\n",
            "|           259| 44611.56999999999|Toms Spezialitäte...|\n",
            "|           151|        44478.4875|  Muscle Machine Inc|\n",
            "|           146| 43435.11666666667|Saveley & Henriot...|\n",
            "|           278| 42509.89666666667|       Rovelli Gifts|\n",
            "|           386|          41835.19| L'ordine Souveniers|\n",
            "|           249|41111.615000000005|  Amica Models & Co.|\n",
            "|           448|          40314.51|Scandinavian Gift...|\n",
            "|           239|40187.619999999995|Collectable Mini ...|\n",
            "|           119|          39643.28|   La Rochelle Gifts|\n",
            "|           319|          39216.08|       Mini Classics|\n",
            "|           363| 38816.42999999999|Online Diecast Cr...|\n",
            "|           458|          37480.03|Corrida Auto Repl...|\n",
            "|           131|        37271.2875|   Land of Toys Inc.|\n",
            "|           114|36117.013999999996|Australian Collec...|\n",
            "|           240|         35891.875|   giftsbymail.co.uk|\n",
            "|           450|        35884.0675|The Sharp Gifts W...|\n",
            "+--------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output/processed/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swb54Idltgm4",
        "outputId": "fdfa07c6-c6c5-422f-d0fe-1810213e9afb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_order_value_by_customer.parquet  top_10_products_quantity.parquet\n",
            "product_revenue.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the parquet files\n",
        "offices_df = spark.read.parquet(\"/content/data/parquet/offices\").alias(\"o\")\n",
        "customers_df = spark.read.parquet(\"/content/data/parquet/customers\").alias(\"c\")\n",
        "payments_df = spark.read.parquet(\"/content/data/parquet/payments\").alias(\"p\")\n",
        "\n",
        "# Join customers and payments\n",
        "cust_pay_df = customers_df.join(\n",
        "    payments_df,\n",
        "    F.col(\"c.customerNumber\") == F.col(\"p.customerNumber\")\n",
        ")\n",
        "\n",
        "# Join with offices on country\n",
        "region_sales_df = cust_pay_df.join(\n",
        "    offices_df,\n",
        "    F.col(\"c.country\") == F.col(\"o.country\"),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# Group by officeCode, city, country (from offices)\n",
        "customer_sales_summary = region_sales_df.groupBy(\n",
        "    F.col(\"o.officeCode\").alias(\"officeCode\"),\n",
        "    F.col(\"o.city\").alias(\"officeCity\"),\n",
        "    F.col(\"o.country\").alias(\"officeCountry\")\n",
        ").agg(\n",
        "    F.countDistinct(F.col(\"c.customerNumber\")).alias(\"customerCount\"),\n",
        "    F.sum(F.col(\"p.amount\")).alias(\"totalSales\")\n",
        ").orderBy(\"totalSales\", ascending=False)\n",
        "\n",
        "# Show the final result\n",
        "customer_sales_summary.show()\n",
        "\n",
        "# Save to Parquet\n",
        "customer_sales_summary.write.mode(\"overwrite\").parquet(\"/content/output/processed/customer_sales_by_office.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89P6zz1Jt63o",
        "outputId": "4d7bc20b-c112-4477-d5df-10c35900dee8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------+-------------+------------------+\n",
            "|officeCode|   officeCity|officeCountry|customerCount|        totalSales|\n",
            "+----------+-------------+-------------+-------------+------------------+\n",
            "|      null|         null|         null|           39| 3779259.460000001|\n",
            "|         1|San Francisco|          USA|           35|3040029.5200000005|\n",
            "|         3|          NYC|          USA|           35|        3040029.52|\n",
            "|         2|       Boston|          USA|           35|        3040029.52|\n",
            "|         4|        Paris|       France|           12| 965750.5799999998|\n",
            "|         6|       Sydney|    Australia|            5|509385.81999999995|\n",
            "|         7|       London|           UK|            5|          391503.9|\n",
            "|         5|        Tokyo|        Japan|            2|167909.94999999998|\n",
            "+----------+-------------+-------------+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output/processed/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPmeFdC7uJTZ",
        "outputId": "48dbb683-c11c-4278-d0a8-cb458b6858e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_order_value_by_customer.parquet  product_revenue.parquet\n",
            "customer_sales_by_office.parquet     top_10_products_quantity.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load parquet files\n",
        "customers_df = spark.read.parquet(\"/content/data/parquet/customers\")\n",
        "payments_df = spark.read.parquet(\"/content/data/parquet/payments\")\n",
        "\n",
        "# Join customers and payments on customerNumber\n",
        "revenue_by_country_df = customers_df.join(\n",
        "    payments_df, on=\"customerNumber\"\n",
        ").groupBy(\"country\") \\\n",
        " .agg(\n",
        "     F.sum(\"amount\").alias(\"totalRevenue\")\n",
        " ).orderBy(\"totalRevenue\", ascending=False)\n",
        "\n",
        "# Show output\n",
        "revenue_by_country_df.show()\n",
        "\n",
        "# Save to Parquet\n",
        "revenue_by_country_df.write.mode(\"overwrite\").parquet(\"/content/output/processed/country_revenue.parquet\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKJVbEU-vzAP",
        "outputId": "aaa9a093-b017-430b-ce61-bd7c8e071612"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|        USA|3040029.5199999996|\n",
            "|      Spain| 994438.5300000003|\n",
            "|     France| 965750.5800000001|\n",
            "|  Australia|509385.81999999995|\n",
            "|New Zealand|         392486.59|\n",
            "|         UK|391503.89999999997|\n",
            "|      Italy|325254.55000000005|\n",
            "|    Finland|         295149.35|\n",
            "|  Singapore|261671.59999999998|\n",
            "|     Canada|         205911.86|\n",
            "|    Denmark|          197356.3|\n",
            "|    Germany|         196470.99|\n",
            "|      Japan|         167909.95|\n",
            "|   Norway  |         166621.51|\n",
            "|    Austria|         136119.99|\n",
            "|     Sweden|         120457.09|\n",
            "|Switzerland|         108777.92|\n",
            "|     Norway|         104224.79|\n",
            "|    Belgium|          91471.03|\n",
            "|Philippines|           87468.3|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load data with aliases\n",
        "offices_df = spark.read.parquet(\"/content/data/parquet/offices\").alias(\"o\")\n",
        "customers_df = spark.read.parquet(\"/content/data/parquet/customers\").alias(\"c\")\n",
        "payments_df = spark.read.parquet(\"/content/data/parquet/payments\").alias(\"p\")\n",
        "\n",
        "# Join customers + payments\n",
        "cust_pay_df = customers_df.join(\n",
        "    payments_df, F.col(\"c.customerNumber\") == F.col(\"p.customerNumber\")\n",
        ")\n",
        "\n",
        "# Join result with offices on country\n",
        "office_sales_df = cust_pay_df.join(\n",
        "    offices_df, F.col(\"c.country\") == F.col(\"o.country\"), \"left\"\n",
        ")\n",
        "\n",
        "# Group by officeCode and officeCity\n",
        "top_offices_df = office_sales_df.groupBy(\n",
        "    F.col(\"o.officeCode\").alias(\"officeCode\"),\n",
        "    F.col(\"o.city\").alias(\"officeCity\")\n",
        ").agg(\n",
        "    F.sum(F.col(\"p.amount\")).alias(\"totalSales\")\n",
        ").orderBy(\"totalSales\", ascending=False)\n",
        "\n",
        "# Show result\n",
        "top_offices_df.show()\n",
        "\n",
        "# Save to Parquet\n",
        "top_offices_df.write.mode(\"overwrite\").parquet(\"/content/output/processed/top_offices_by_sales.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0pICFjtvhuU",
        "outputId": "9f5b8f69-a975-47f2-ca5f-16d2e27a43fb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+------------------+\n",
            "|officeCode|   officeCity|        totalSales|\n",
            "+----------+-------------+------------------+\n",
            "|      null|         null| 3779259.459999999|\n",
            "|         3|          NYC|3040029.5199999996|\n",
            "|         2|       Boston|3040029.5199999996|\n",
            "|         1|San Francisco|3040029.5199999996|\n",
            "|         4|        Paris| 965750.5800000001|\n",
            "|         6|       Sydney|509385.81999999995|\n",
            "|         7|       London|391503.89999999997|\n",
            "|         5|        Tokyo|         167909.95|\n",
            "+----------+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load data\n",
        "orderdetails_df = spark.read.parquet(\"/content/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/data/parquet/products\")\n",
        "\n",
        "# Join and cache\n",
        "product_sales_df = orderdetails_df.join(products_df, \"productCode\").cache()\n",
        "\n",
        "# Action 1\n",
        "top_products = product_sales_df.groupBy(\"productName\") \\\n",
        "    .agg(F.sum(\"quantityOrdered\").alias(\"totalQuantity\")) \\\n",
        "    .orderBy(\"totalQuantity\", ascending=False)\n",
        "\n",
        "top_products.show()\n",
        "\n",
        "# Action 2\n",
        "revenue_df = product_sales_df.withColumn(\n",
        "    \"revenue\", F.col(\"quantityOrdered\") * F.col(\"priceEach\")\n",
        ").groupBy(\"productName\") \\\n",
        " .agg(F.sum(\"revenue\").alias(\"totalRevenue\")) \\\n",
        " .orderBy(\"totalRevenue\", ascending=False)\n",
        "\n",
        "revenue_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2VvpSIwCsk",
        "outputId": "09751201-8042-4445-c95a-7ac1fe95fe9f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|         productName|totalQuantity|\n",
            "+--------------------+-------------+\n",
            "|1992 Ferrari 360 ...|         1808|\n",
            "|1937 Lincoln Berline|         1111|\n",
            "|American Airlines...|         1085|\n",
            "|1941 Chevrolet Sp...|         1076|\n",
            "|1930 Buick Marque...|         1074|\n",
            "|    1940s Ford truck|         1061|\n",
            "|1969 Harley David...|         1057|\n",
            "|   1957 Chevy Pickup|         1056|\n",
            "|1964 Mercedes Tou...|         1053|\n",
            "|1956 Porsche 356A...|         1052|\n",
            "|Corsair F4U ( Bir...|         1051|\n",
            "|  F/A 18 Hornet 1/72|         1047|\n",
            "|1980s Black Hawk ...|         1040|\n",
            "|1913 Ford Model T...|         1038|\n",
            "|   1997 BMW R 1100 S|         1033|\n",
            "| 1972 Alfa Romeo GTA|         1030|\n",
            "|1962 Volkswagen M...|         1029|\n",
            "|    2002 Suzuki XREO|         1028|\n",
            "|The USS Constitut...|         1020|\n",
            "|   2001 Ferrari Enzo|         1019|\n",
            "+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+------------------+\n",
            "|         productName|      totalRevenue|\n",
            "+--------------------+------------------+\n",
            "|1992 Ferrari 360 ...|         276839.98|\n",
            "|   2001 Ferrari Enzo|         190755.86|\n",
            "|1952 Alpine Renau...|190017.95999999996|\n",
            "|2003 Harley-David...|170685.99999999997|\n",
            "|   1968 Ford Mustang|161531.47999999992|\n",
            "|    1969 Ford Falcon|         152543.02|\n",
            "|1980s Black Hawk ...|144959.90999999997|\n",
            "|1998 Chrysler Ply...|142530.62999999998|\n",
            "|1917 Grand Tourin...|140535.60000000003|\n",
            "|    2002 Suzuki XREO|135767.03000000003|\n",
            "|1956 Porsche 356A...|         134240.71|\n",
            "|  1969 Corvair Monza|132363.78999999998|\n",
            "|1928 Mercedes-Ben...|132275.97999999998|\n",
            "|1957 Corvette Con...|130749.31000000001|\n",
            "| 1972 Alfa Romeo GTA|127924.31999999999|\n",
            "|1962 LanciaA Delt...|123123.00999999998|\n",
            "|1970 Triumph Spit...|         122254.75|\n",
            "|1976 Ford Gran To...|          121890.6|\n",
            "|1948 Porsche Type...|         121653.46|\n",
            "|      1958 Setra Bus|119085.24999999999|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast, col\n",
        "\n",
        "# Load data with aliases\n",
        "customers_df = spark.read.parquet(\"/content/data/parquet/customers\").alias(\"c\")\n",
        "offices_df = spark.read.parquet(\"/content/data/parquet/offices\").alias(\"o\")\n",
        "\n",
        "# Perform broadcast join on country\n",
        "cust_office_join = customers_df.join(\n",
        "    broadcast(offices_df),\n",
        "    col(\"c.country\") == col(\"o.country\"),\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "# Select non-ambiguous columns\n",
        "cust_office_join.select(\n",
        "    col(\"c.customerName\").alias(\"customerName\"),\n",
        "    col(\"o.city\").alias(\"officeCity\"),\n",
        "    col(\"o.country\").alias(\"officeCountry\")\n",
        ").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dcDUb7EwoeK",
        "outputId": "4b7e0e2f-d2fa-41e8-de39-074ca7016702"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+-------------+\n",
            "|        customerName|   officeCity|officeCountry|\n",
            "+--------------------+-------------+-------------+\n",
            "|   Atelier graphique|        Paris|       France|\n",
            "|  Signal Gift Stores|          NYC|          USA|\n",
            "|  Signal Gift Stores|       Boston|          USA|\n",
            "|  Signal Gift Stores|San Francisco|          USA|\n",
            "|Australian Collec...|       Sydney|    Australia|\n",
            "+--------------------+-------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load payments DataFrame\n",
        "payments_df = spark.read.parquet(\"/content/data/parquet/payments\")\n",
        "\n",
        "# Use DataFrame API to group by customerNumber and sum the amount\n",
        "revenue_df = payments_df.groupBy(\"customerNumber\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(\"totalRevenue\", ascending=False)\n",
        "\n",
        "# Show result\n",
        "revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19RuzBKEwrJS",
        "outputId": "5da58923-1bdd-4b01-e064-ef0d75d26ad0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+\n",
            "|customerNumber|      totalRevenue|\n",
            "+--------------+------------------+\n",
            "|           141| 715738.9800000001|\n",
            "|           124| 584188.2400000001|\n",
            "|           114|180585.06999999998|\n",
            "|           151|         177913.95|\n",
            "|           148|         156251.03|\n",
            "|           323|154622.08000000002|\n",
            "|           187|         148410.09|\n",
            "|           276|         137034.22|\n",
            "|           321|         132340.78|\n",
            "|           146|         130305.35|\n",
            "|           278|         127529.69|\n",
            "|           353|         126983.19|\n",
            "|           119|116949.68000000001|\n",
            "|           363|116449.29000000001|\n",
            "|           496|         114497.19|\n",
            "|           458|         112440.09|\n",
            "|           298|         108777.92|\n",
            "|           131|         107639.94|\n",
            "|           145|          107446.5|\n",
            "|           398|         105548.73|\n",
            "+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a DataFrame lazily\n",
        "orders_df = spark.read.parquet(\"/content/data/parquet/orders\")\n",
        "\n",
        "# Transformation only — this does NOT trigger execution\n",
        "transformed_df = orders_df.select(\"orderNumber\", \"orderDate\", \"customerNumber\")\n",
        "\n",
        "# Print execution plan — shows Spark's plan but doesn't run it\n",
        "transformed_df.explain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh5d5RGvw-VA",
        "outputId": "5d31f4d8-87b0-4897-8304-4c1a3f051c82"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) ColumnarToRow\n",
            "+- FileScan parquet [orderNumber#4107,orderDate#4108,customerNumber#4113] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/data/parquet/orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<orderNumber:int,orderDate:date,customerNumber:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an ACTION — it triggers the actual execution\n",
        "row_count = transformed_df.count()\n",
        "\n",
        "print(\"Total Orders:\", row_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXTPxB04x1XT",
        "outputId": "7fc3a445-c8ca-4dbf-c0f9-1512415da18e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Orders: 326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def task1_ingest_and_parquet():\n",
        "    # Read all 8 CSVs, define schema, write to /data/parquet/\n",
        "    pass\n",
        "\n",
        "def task2_product_order_analysis():\n",
        "    # 2.1 Top 10 products by quantity\n",
        "    # 2.2 Product revenue\n",
        "    # 2.3 Avg order value by customer\n",
        "    pass\n",
        "\n",
        "def task3_regional_sales_insights():\n",
        "    # 3.1 Customer sales by office\n",
        "    # 3.2 Revenue by country\n",
        "    # 3.3 Top offices\n",
        "    pass\n",
        "\n",
        "def task4_performance_optimization():\n",
        "    # 4.1 cache()\n",
        "    # 4.2 broadcast()\n",
        "    # 4.3 aggregateByKey (or groupBy)\n",
        "    # 4.4 explain(), count()\n",
        "    pass\n",
        "\n",
        "def main():\n",
        "    task1_ingest_and_parquet()\n",
        "    task2_product_order_analysis()\n",
        "    task3_regional_sales_insights()\n",
        "    task4_performance_optimization()\n",
        "\n",
        "# Run like a main class\n",
        "main()\n"
      ],
      "metadata": {
        "id": "PMBSWpwqyAqo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in the output directory\n",
        "os.listdir(\"/content/output/processed/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfmAI6JwyoC4",
        "outputId": "d468153a-02e4-4b5d-8a1b-541803b7a4be"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customer_sales_by_office.parquet',\n",
              " 'top_offices_by_sales.parquet',\n",
              " 'top_10_products_quantity.parquet',\n",
              " 'product_revenue.parquet',\n",
              " 'country_revenue.parquet',\n",
              " 'avg_order_value_by_customer.parquet']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output_processed.zip /content/output/processed/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwnqfYiEyqW0",
        "outputId": "919b414c-1022-432b-a199-f982877fe908"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/processed/ (stored 0%)\n",
            "  adding: content/output/processed/customer_sales_by_office.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/customer_sales_by_office.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/customer_sales_by_office.parquet/part-00000-7b4303d8-1b3d-44d6-8b9a-f79d3c99c319-c000.snappy.parquet (deflated 45%)\n",
            "  adding: content/output/processed/customer_sales_by_office.parquet/_SUCCESS (stored 0%)\n",
            "  adding: content/output/processed/customer_sales_by_office.parquet/.part-00000-7b4303d8-1b3d-44d6-8b9a-f79d3c99c319-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/top_offices_by_sales.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/top_offices_by_sales.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/top_offices_by_sales.parquet/_SUCCESS (stored 0%)\n",
            "  adding: content/output/processed/top_offices_by_sales.parquet/.part-00000-1c0a5122-1fc8-4763-a63c-694176178f13-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/top_offices_by_sales.parquet/part-00000-1c0a5122-1fc8-4763-a63c-694176178f13-c000.snappy.parquet (deflated 37%)\n",
            "  adding: content/output/processed/top_10_products_quantity.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/top_10_products_quantity.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/top_10_products_quantity.parquet/part-00000-d9956d8b-732b-4916-8da4-46c302fdc846-c000.snappy.parquet (deflated 35%)\n",
            "  adding: content/output/processed/top_10_products_quantity.parquet/_SUCCESS (stored 0%)\n",
            "  adding: content/output/processed/top_10_products_quantity.parquet/.part-00000-d9956d8b-732b-4916-8da4-46c302fdc846-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/product_revenue.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/product_revenue.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/product_revenue.parquet/_SUCCESS (stored 0%)\n",
            "  adding: content/output/processed/product_revenue.parquet/part-00000-606f7257-8580-4907-b65f-602c596632ce-c000.snappy.parquet (deflated 19%)\n",
            "  adding: content/output/processed/product_revenue.parquet/.part-00000-606f7257-8580-4907-b65f-602c596632ce-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/country_revenue.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/country_revenue.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/country_revenue.parquet/.part-00000-f84aef5b-0c12-48a3-b3ec-b6d10eb7be5c-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/country_revenue.parquet/_SUCCESS (stored 0%)\n",
            "  adding: content/output/processed/country_revenue.parquet/part-00000-f84aef5b-0c12-48a3-b3ec-b6d10eb7be5c-c000.snappy.parquet (deflated 27%)\n",
            "  adding: content/output/processed/avg_order_value_by_customer.parquet/ (stored 0%)\n",
            "  adding: content/output/processed/avg_order_value_by_customer.parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/processed/avg_order_value_by_customer.parquet/.part-00000-3fcee8bb-bda5-4d18-8dca-ee4b55c24e37-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/output/processed/avg_order_value_by_customer.parquet/part-00000-3fcee8bb-bda5-4d18-8dca-ee4b55c24e37-c000.snappy.parquet (deflated 23%)\n",
            "  adding: content/output/processed/avg_order_value_by_customer.parquet/_SUCCESS (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnYGYDT5zlgS",
        "outputId": "45a26a87-53fe-4c2c-b241-37f7b504d556"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "drive_output_dir = '/content/drive/MyDrive/spark_output_files'\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "JrC3Gpgzzp8y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "local_output_dir = '/content/output/processed/'\n",
        "\n",
        "# Copy all files from local to Drive folder\n",
        "for file_name in os.listdir(local_output_dir):\n",
        "    full_local_path = os.path.join(local_output_dir, file_name)\n",
        "    if os.path.isfile(full_local_path):\n",
        "        shutil.copy(full_local_path, drive_output_dir)\n"
      ],
      "metadata": {
        "id": "hnXpKK-ezsJx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/output/processed/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k-8Ruybztnj",
        "outputId": "60ac6d28-3156-4164-926c-93902e05ce4a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customer_sales_by_office.parquet',\n",
              " 'top_offices_by_sales.parquet',\n",
              " 'top_10_products_quantity.parquet',\n",
              " 'product_revenue.parquet',\n",
              " 'country_revenue.parquet',\n",
              " 'avg_order_value_by_customer.parquet']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and target directories\n",
        "local_output_dir = '/content/output/processed'\n",
        "drive_output_dir = '/content/drive/MyDrive/spark_output_files'\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "\n",
        "# Copy entire folders (each .parquet is a directory)\n",
        "for item in os.listdir(local_output_dir):\n",
        "    src_path = os.path.join(local_output_dir, item)\n",
        "    dst_path = os.path.join(drive_output_dir, item)\n",
        "\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        print(f\"Copied folder: {item}\")\n",
        "    else:\n",
        "        print(f\"Skipped: {item} (not a directory)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV9z5b-iz-D4",
        "outputId": "baf36392-04df-4359-c173-6616960b7d66"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied folder: customer_sales_by_office.parquet\n",
            "Copied folder: top_offices_by_sales.parquet\n",
            "Copied folder: top_10_products_quantity.parquet\n",
            "Copied folder: product_revenue.parquet\n",
            "Copied folder: country_revenue.parquet\n",
            "Copied folder: avg_order_value_by_customer.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpdnGAwY0B-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}