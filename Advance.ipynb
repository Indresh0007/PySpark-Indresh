{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA9IT3aIFT4sIKZswUHXaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indresh0007/PySpark-Indresh/blob/main/Advance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Windows functions"
      ],
      "metadata": {
        "id": "HNH7YUkDZFnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0y5KSOoY7JE",
        "outputId": "fe253d98-a0f6-4453-b200-6320f50b1c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "window_spec = Window.partitionBy(\"address.city\").orderBy(\"salary\")\n",
        "df.withColumn(\"rank\", rank().over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXIhb98WbCIy",
        "outputId": "a2f8ff5a-c98e-4290-f1e3-cf47f0915f64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+--------------------+--------------------+----+\n",
            "| id|   name|salary|            subjects|             address|rank|\n",
            "+---+-------+------+--------------------+--------------------+----+\n",
            "|  4|  David|  1200|               [art]|{zip -> 90001, ci...|   1|\n",
            "|  1|  Alice|  2000|     [math, science]|{zip -> 10001, ci...|   1|\n",
            "|  3|Charlie|  2200|[math, history, s...|{zip -> 10001, ci...|   2|\n",
            "|  2|    Bob|  1500|           [english]|{zip -> 94105, ci...|   1|\n",
            "+---+-------+------+--------------------+--------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number, rank, dense_rank, max, sum, avg\n",
        "\n",
        "# Employee Data\n",
        "data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"Finance\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Tom\", \"Sales\", 3700),\n",
        "    (7, \"Jerry\", \"Finance\", 4200),\n",
        "    (8, \"Sam\", \"IT\", 4700),\n",
        "    (9, \"Steve\", \"Sales\", 3100),\n",
        "    (10, \"Rachel\", \"IT\", 4600)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hXgIAAIcOSu",
        "outputId": "7b46d891-625b-4bbc-d820-c4fa6d845181"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+\n",
            "|EmpID|  Name|Department|Salary|\n",
            "+-----+------+----------+------+\n",
            "|    1|  John|     Sales|  3000|\n",
            "|    2|  Jane|   Finance|  4000|\n",
            "|    3|  Mike|     Sales|  3500|\n",
            "|    4| Alice|   Finance|  3800|\n",
            "|    5|   Bob|        IT|  4500|\n",
            "|    6|   Tom|     Sales|  3700|\n",
            "|    7| Jerry|   Finance|  4200|\n",
            "|    8|   Sam|        IT|  4700|\n",
            "|    9| Steve|     Sales|  3100|\n",
            "|   10|Rachel|        IT|  4600|\n",
            "+-----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Window Spec\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "\n",
        "# Apply Rank and Show\n",
        "df.withColumn(\"Rank\", rank().over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYMze1F-dF_G",
        "outputId": "ff019c29-90c4-44ae-d697-621d1d40e56c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+----+\n",
            "|EmpID|  Name|Department|Salary|Rank|\n",
            "+-----+------+----------+------+----+\n",
            "|    7| Jerry|   Finance|  4200|   1|\n",
            "|    2|  Jane|   Finance|  4000|   2|\n",
            "|    4| Alice|   Finance|  3800|   3|\n",
            "|    8|   Sam|        IT|  4700|   1|\n",
            "|   10|Rachel|        IT|  4600|   2|\n",
            "|    5|   Bob|        IT|  4500|   3|\n",
            "|    6|   Tom|     Sales|  3700|   1|\n",
            "|    3|  Mike|     Sales|  3500|   2|\n",
            "|    9| Steve|     Sales|  3100|   3|\n",
            "|    1|  John|     Sales|  3000|   4|\n",
            "+-----+------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec=Window.partitionBy(\"Department\")\n",
        "df.withColumn(\"MaxSalaryDept\",max(\"Salary\").over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWx41cJQdsSV",
        "outputId": "8d14544d-6fee-4983-bbff-ea9b3c39d01a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+-------------+\n",
            "|EmpID|  Name|Department|Salary|MaxSalaryDept|\n",
            "+-----+------+----------+------+-------------+\n",
            "|    2|  Jane|   Finance|  4000|         4200|\n",
            "|    4| Alice|   Finance|  3800|         4200|\n",
            "|    7| Jerry|   Finance|  4200|         4200|\n",
            "|    5|   Bob|        IT|  4500|         4700|\n",
            "|    8|   Sam|        IT|  4700|         4700|\n",
            "|   10|Rachel|        IT|  4600|         4700|\n",
            "|    1|  John|     Sales|  3000|         3700|\n",
            "|    3|  Mike|     Sales|  3500|         3700|\n",
            "|    6|   Tom|     Sales|  3700|         3700|\n",
            "|    9| Steve|     Sales|  3100|         3700|\n",
            "+-----+------+----------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"Department\")\n",
        "df.withColumn(\"MaxSalaryDept\", max(\"Salary\").over(window_spec)) \\\n",
        "  .select(\"Department\", \"MaxSalaryDept\") \\.distinct() \\.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-nnJwNTfTdg",
        "outputId": "986108c0-9b6b-40cd-84d6-fe95b5fefce1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|Department|MaxSalaryDept|\n",
            "+----------+-------------+\n",
            "|   Finance|         4200|\n",
            "|        IT|         4700|\n",
            "|     Sales|         3700|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\") \\\n",
        "  .agg(max(\"Salary\").alias(\"MaxSalaryDept\")) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuvx7jtkf9Fe",
        "outputId": "5155b661-0791-403f-b8a6-14c656a14d2e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|Department|MaxSalaryDept|\n",
            "+----------+-------------+\n",
            "|     Sales|         3700|\n",
            "|   Finance|         4200|\n",
            "|        IT|         4700|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec=Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding,0)\n",
        "df.withColumn(\"CumulativeSalary\", sum(\"Salary\").over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk4LFcC3i0od",
        "outputId": "f1c6efba-d117-4c77-df76-f73b1010b54d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+----------------+\n",
            "|EmpID|  Name|Department|Salary|CumulativeSalary|\n",
            "+-----+------+----------+------+----------------+\n",
            "|    4| Alice|   Finance|  3800|            3800|\n",
            "|    2|  Jane|   Finance|  4000|            7800|\n",
            "|    7| Jerry|   Finance|  4200|           12000|\n",
            "|    5|   Bob|        IT|  4500|            4500|\n",
            "|   10|Rachel|        IT|  4600|            9100|\n",
            "|    8|   Sam|        IT|  4700|           13800|\n",
            "|    1|  John|     Sales|  3000|            3000|\n",
            "|    9| Steve|     Sales|  3100|            6100|\n",
            "|    3|  Mike|     Sales|  3500|            9600|\n",
            "|    6|   Tom|     Sales|  3700|           13300|\n",
            "+-----+------+----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_base=Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
        "window_current=window_base.rowsBetween(-1,0)\n",
        "df.withColumn(\"CumulativeSalary\", sum(\"Salary\").over(window_base)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FefKLGEH4z8x",
        "outputId": "172b1bce-03ec-40ae-ba1b-ee1b4d9640df"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+----------------+\n",
            "|EmpID|  Name|Department|Salary|CumulativeSalary|\n",
            "+-----+------+----------+------+----------------+\n",
            "|    4| Alice|   Finance|  3800|            3800|\n",
            "|    2|  Jane|   Finance|  4000|            7800|\n",
            "|    7| Jerry|   Finance|  4200|           12000|\n",
            "|    5|   Bob|        IT|  4500|            4500|\n",
            "|   10|Rachel|        IT|  4600|            9100|\n",
            "|    8|   Sam|        IT|  4700|           13800|\n",
            "|    1|  John|     Sales|  3000|            3000|\n",
            "|    9| Steve|     Sales|  3100|            6100|\n",
            "|    3|  Mike|     Sales|  3500|            9600|\n",
            "|    6|   Tom|     Sales|  3700|           13300|\n",
            "+-----+------+----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF3AIB7T64WY",
        "outputId": "d88a3e73-b8c4-46f4-ac96-dba54e158c37"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(IntegerType())\n",
        "def subject_count(subjects):\n",
        "    return len(subjects)\n",
        "\n",
        "df.withColumn(\"subject_count\", subject_count(col(\"subjects\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWBkeGbb7rAT",
        "outputId": "3a626697-5b39-4e0c-c01d-46cc0a2bc07c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+--------------------+--------------------+-------------+\n",
            "| id|   name|salary|            subjects|             address|subject_count|\n",
            "+---+-------+------+--------------------+--------------------+-------------+\n",
            "|  1|  Alice|  2000|     [math, science]|{zip -> 10001, ci...|            2|\n",
            "|  2|    Bob|  1500|           [english]|{zip -> 94105, ci...|            1|\n",
            "|  3|Charlie|  2200|[math, history, s...|{zip -> 10001, ci...|            3|\n",
            "|  4|  David|  1200|               [art]|{zip -> 90001, ci...|            1|\n",
            "+---+-------+------+--------------------+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "@pandas_udf(DoubleType())\n",
        "def multiply_by_two(s: pd.Series) -> pd.Series:\n",
        "    return s * 2\n",
        "\n",
        "df.withColumn(\"salary_doubled\", multiply_by_two(col(\"salary\"))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnIBrDiy-r2S",
        "outputId": "314c20a3-80c0-4076-eca1-9a5302624caa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+--------------------+--------------------+--------------+\n",
            "| id|   name|salary|            subjects|             address|salary_doubled|\n",
            "+---+-------+------+--------------------+--------------------+--------------+\n",
            "|  1|  Alice|  2000|     [math, science]|{zip -> 10001, ci...|        4000.0|\n",
            "|  2|    Bob|  1500|           [english]|{zip -> 94105, ci...|        3000.0|\n",
            "|  3|Charlie|  2200|[math, history, s...|{zip -> 10001, ci...|        4400.0|\n",
            "|  4|  David|  1200|               [art]|{zip -> 90001, ci...|        2400.0|\n",
            "+---+-------+------+--------------------+--------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Use Case Activities\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col, rank, avg, udf, pandas_udf\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "\n",
        "employees_data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"IT\", 4500),\n",
        "    (5, \"Eve\", \"Finance\", 5000),\n",
        "    (6, \"Frank\", \"Finance\", 4800),\n",
        "]\n",
        "\n",
        "employees_df = spark.createDataFrame(employees_data, [\"id\", \"name\", \"department\", \"salary\"])\n",
        "employees_df.show()\n",
        "\n",
        "departments_data = [\n",
        "    (\"HR\", \"New York\"),\n",
        "    (\"IT\", \"San Francisco\"),\n",
        "    (\"Finance\", \"Chicago\"),\n",
        "]\n",
        "\n",
        "departments_df = spark.createDataFrame(departments_data, [\"department\", \"location\"])\n",
        "departments_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_oxYQ0N_Krn",
        "outputId": "b50a4b03-41dd-4b9d-ea9e-21607274181a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  1|Alice|        HR|  3000|\n",
            "|  2|  Bob|        IT|  4000|\n",
            "|  3|Cathy|        HR|  3500|\n",
            "|  4|David|        IT|  4500|\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+----------+-------------+\n",
            "|department|     location|\n",
            "+----------+-------------+\n",
            "|        HR|     New York|\n",
            "|        IT|San Francisco|\n",
            "|   Finance|      Chicago|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Apply rank\n",
        "ranked_df = employees_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Show results\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A1nQvN_CR--",
        "outputId": "45183b9a-9a2b-49e5-c600-03f2a83055c0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----+\n",
            "| id| name|department|salary|rank|\n",
            "+---+-----+----------+------+----+\n",
            "|  5|  Eve|   Finance|  5000|   1|\n",
            "|  6|Frank|   Finance|  4800|   2|\n",
            "|  3|Cathy|        HR|  3500|   1|\n",
            "|  1|Alice|        HR|  3000|   2|\n",
            "|  4|David|        IT|  4500|   1|\n",
            "|  2|  Bob|        IT|  4000|   2|\n",
            "+---+-----+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec=Window.partitionBy(\"department\")\n",
        "avg_salary_df=employees_df.withColumn(\"avg_salary\",avg(\"Salary\").over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJeC3VW5CiYR",
        "outputId": "5da12b17-129c-4c4e-ef56-5247ceabbf62"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------+\n",
            "| id| name|department|salary|avg_salary|\n",
            "+---+-----+----------+------+----------+\n",
            "|  5|  Eve|   Finance|  5000|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|    4900.0|\n",
            "|  1|Alice|        HR|  3000|    3250.0|\n",
            "|  3|Cathy|        HR|  3500|    3250.0|\n",
            "|  2|  Bob|        IT|  4000|    4250.0|\n",
            "|  4|David|        IT|  4500|    4250.0|\n",
            "+---+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rank, avg, udf, pandas_udf\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySpark Practice\").getOrCreate()\n",
        "\n",
        "# Employees Data\n",
        "employees_data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"IT\", 4500),\n",
        "    (5, \"Eve\", \"Finance\", 5000),\n",
        "    (6, \"Frank\", \"Finance\", 4800),\n",
        "]\n",
        "employees_df = spark.createDataFrame(employees_data, [\"id\", \"name\", \"department\", \"salary\"])\n",
        "\n",
        "# Departments Data\n",
        "departments_data = [\n",
        "    (\"HR\", \"New York\"),\n",
        "    (\"IT\", \"San Francisco\"),\n",
        "    (\"Finance\", \"Chicago\"),\n",
        "]\n",
        "departments_df = spark.createDataFrame(departments_data, [\"department\", \"location\"])\n",
        "employees_df.show()\n",
        "departments_df.show()\n",
        "### 1️⃣ Window Functions\n",
        "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "employees_ranked = employees_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "employees_avg = employees_ranked.withColumn(\"avg_salary\", avg(\"salary\").over(Window.partitionBy(\"department\")))\n",
        "employees_avg.show()\n",
        "\n",
        "### 2️⃣ Joins\n",
        "inner_join_df = employees_df.join(departments_df, \"department\", \"inner\")\n",
        "left_join_df = employees_df.join(departments_df, \"department\", \"left\")\n",
        "inner_join_df.show()\n",
        "left_join_df.show()\n",
        "\n",
        "### 3️⃣ DataFrame Operations\n",
        "employees_df.filter(col(\"salary\") > 4000).show()\n",
        "employees_df.select(\"name\", \"department\", \"salary\").show()\n",
        "employees_df.orderBy(col(\"salary\").desc()).show()\n",
        "employees_df.withColumn(\"bonus_salary\", col(\"salary\") * 1.1).show()\n",
        "\n",
        "### 4️⃣ Spark UDF for Categorization\n",
        "def categorize_salary(salary):\n",
        "    if salary > 4500:\n",
        "        return \"High\"\n",
        "    elif salary >= 3500:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "categorize_udf = udf(categorize_salary, StringType())\n",
        "employees_df.withColumn(\"category\", categorize_udf(\"salary\")).show()\n",
        "\n",
        "### 5️⃣ Pandas UDF for Normalization\n",
        "@pandas_udf(DoubleType())\n",
        "def normalize_salary(s: pd.Series) -> pd.Series:\n",
        "    return (s - s.min()) / (s.max() - s.min())\n",
        "\n",
        "employees_df.withColumn(\"normalized_salary\", normalize_salary(\"salary\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMURXisED4z3",
        "outputId": "58a175af-ef89-47f9-e756-c835bbbdf468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  1|Alice|        HR|  3000|\n",
            "|  2|  Bob|        IT|  4000|\n",
            "|  3|Cathy|        HR|  3500|\n",
            "|  4|David|        IT|  4500|\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+----------+-------------+\n",
            "|department|     location|\n",
            "+----------+-------------+\n",
            "|        HR|     New York|\n",
            "|        IT|San Francisco|\n",
            "|   Finance|      Chicago|\n",
            "+----------+-------------+\n",
            "\n",
            "+---+-----+----------+------+----+----------+\n",
            "| id| name|department|salary|rank|avg_salary|\n",
            "+---+-----+----------+------+----+----------+\n",
            "|  5|  Eve|   Finance|  5000|   1|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|   2|    4900.0|\n",
            "|  3|Cathy|        HR|  3500|   1|    3250.0|\n",
            "|  1|Alice|        HR|  3000|   2|    3250.0|\n",
            "|  4|David|        IT|  4500|   1|    4250.0|\n",
            "|  2|  Bob|        IT|  4000|   2|    4250.0|\n",
            "+---+-----+----------+------+----+----------+\n",
            "\n",
            "+----------+---+-----+------+-------------+\n",
            "|department| id| name|salary|     location|\n",
            "+----------+---+-----+------+-------------+\n",
            "|   Finance|  5|  Eve|  5000|      Chicago|\n",
            "|   Finance|  6|Frank|  4800|      Chicago|\n",
            "|        HR|  1|Alice|  3000|     New York|\n",
            "|        HR|  3|Cathy|  3500|     New York|\n",
            "|        IT|  2|  Bob|  4000|San Francisco|\n",
            "|        IT|  4|David|  4500|San Francisco|\n",
            "+----------+---+-----+------+-------------+\n",
            "\n",
            "+----------+---+-----+------+-------------+\n",
            "|department| id| name|salary|     location|\n",
            "+----------+---+-----+------+-------------+\n",
            "|        HR|  1|Alice|  3000|     New York|\n",
            "|        HR|  3|Cathy|  3500|     New York|\n",
            "|        IT|  2|  Bob|  4000|San Francisco|\n",
            "|   Finance|  5|  Eve|  5000|      Chicago|\n",
            "|   Finance|  6|Frank|  4800|      Chicago|\n",
            "|        IT|  4|David|  4500|San Francisco|\n",
            "+----------+---+-----+------+-------------+\n",
            "\n",
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  4|David|        IT|  4500|\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+-----+----------+------+\n",
            "| name|department|salary|\n",
            "+-----+----------+------+\n",
            "|Alice|        HR|  3000|\n",
            "|  Bob|        IT|  4000|\n",
            "|Cathy|        HR|  3500|\n",
            "|David|        IT|  4500|\n",
            "|  Eve|   Finance|  5000|\n",
            "|Frank|   Finance|  4800|\n",
            "+-----+----------+------+\n",
            "\n",
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "|  4|David|        IT|  4500|\n",
            "|  2|  Bob|        IT|  4000|\n",
            "|  3|Cathy|        HR|  3500|\n",
            "|  1|Alice|        HR|  3000|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+---+-----+----------+------+------------------+\n",
            "| id| name|department|salary|      bonus_salary|\n",
            "+---+-----+----------+------+------------------+\n",
            "|  1|Alice|        HR|  3000|3300.0000000000005|\n",
            "|  2|  Bob|        IT|  4000|            4400.0|\n",
            "|  3|Cathy|        HR|  3500|3850.0000000000005|\n",
            "|  4|David|        IT|  4500|            4950.0|\n",
            "|  5|  Eve|   Finance|  5000|            5500.0|\n",
            "|  6|Frank|   Finance|  4800|            5280.0|\n",
            "+---+-----+----------+------+------------------+\n",
            "\n",
            "+---+-----+----------+------+--------+\n",
            "| id| name|department|salary|category|\n",
            "+---+-----+----------+------+--------+\n",
            "|  1|Alice|        HR|  3000|     Low|\n",
            "|  2|  Bob|        IT|  4000|  Medium|\n",
            "|  3|Cathy|        HR|  3500|  Medium|\n",
            "|  4|David|        IT|  4500|  Medium|\n",
            "|  5|  Eve|   Finance|  5000|    High|\n",
            "|  6|Frank|   Finance|  4800|    High|\n",
            "+---+-----+----------+------+--------+\n",
            "\n",
            "+---+-----+----------+------+-----------------+\n",
            "| id| name|department|salary|normalized_salary|\n",
            "+---+-----+----------+------+-----------------+\n",
            "|  1|Alice|        HR|  3000|              0.0|\n",
            "|  2|  Bob|        IT|  4000|              1.0|\n",
            "|  3|Cathy|        HR|  3500|              0.5|\n",
            "|  4|David|        IT|  4500|              0.0|\n",
            "|  5|  Eve|   Finance|  5000|              1.0|\n",
            "|  6|Frank|   Finance|  4800|              0.6|\n",
            "+---+-----+----------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex Nested Schemas Handling\n"
      ],
      "metadata": {
        "id": "w4xZw1B8jOCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"John\", [\"Python\", \"Java\"]),\n",
        "    (\"Jane\", [\"SQL\", \"R\", \"Scala\"]),\n",
        "    (\"Mike\", [])\n",
        "]\n",
        "columns = [\"Name\", \"Skills\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bEk0WI2jcOu",
        "outputId": "b53312ed-d0ea-4c1a-cb86-bd67fc44dc87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|Name|Skills         |\n",
            "+----+---------------+\n",
            "|John|[Python, Java] |\n",
            "|Jane|[SQL, R, Scala]|\n",
            "|Mike|[]             |\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_explode = df.withColumn(\"skill\", explode(df.Skills))\n",
        "df_explode.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvbkZuibkE6p",
        "outputId": "454f6b93-5511-4388-9986-1e12e9431c30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+------+\n",
            "|Name|         Skills| skill|\n",
            "+----+---------------+------+\n",
            "|John| [Python, Java]|Python|\n",
            "|John| [Python, Java]|  Java|\n",
            "|Jane|[SQL, R, Scala]|   SQL|\n",
            "|Jane|[SQL, R, Scala]|     R|\n",
            "|Jane|[SQL, R, Scala]| Scala|\n",
            "+----+---------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")"
      ],
      "metadata": {
        "id": "aIBCeXSRkWzW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lateral =spark.sql(\"\"\"\n",
        "    SELECT Name, skill\n",
        "    FROM people\n",
        "    LATERAL VIEW EXPLODE(Skills) AS skill\n",
        "\"\"\")\n",
        "df_lateral.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCBZ_qNblbos",
        "outputId": "223339ff-12df-4a8e-b428-519acfdc053c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|Name| skill|\n",
            "+----+------+\n",
            "|John|Python|\n",
            "|John|  Java|\n",
            "|Jane|   SQL|\n",
            "|Jane|     R|\n",
            "|Jane| Scala|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot and Unpiot operations"
      ],
      "metadata": {
        "id": "gWP3b8i2ypmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ProductA\", \"Jan\", 100),\n",
        "    (\"ProductA\", \"Feb\", 150),\n",
        "    (\"ProductA\", \"Mar\", 120),\n",
        "    (\"ProductB\", \"Jan\", 200),\n",
        "    (\"ProductB\", \"Feb\", 230),\n",
        "    (\"ProductB\", \"Mar\", 210),\n",
        "]\n",
        "columns = [\"Product\", \"Month\", \"Sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqsbBGfRyv4R",
        "outputId": "78023541-7bc9-4f5f-d8cf-555e25284dd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df = df.groupBy(\"Product\").pivot(\"Month\").sum(\"Sales\")\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XYVL-vSy7DF",
        "outputId": "f4d8e18c-968d-482e-ed59-c0be6ab4df35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+\n",
            "| Product|Feb|Jan|Mar|\n",
            "+--------+---+---+---+\n",
            "|ProductB|230|200|210|\n",
            "|ProductA|150|100|120|\n",
            "+--------+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, expr\n",
        "\n",
        "wide_df= df.groupBy(\"Product\").pivot(\"Month\").agg(sum(\"Sales\"))\n",
        "unpivot_df = wide_df.select(\"Product\", expr(\"stack(3, 'Jan', Jan, 'Feb', Feb, 'Mar', Mar) as (Month, Sales)\"))\n",
        "unpivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg2uextry93f",
        "outputId": "454368d6-4d42-4a50-ec05-6c7986cc170c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jksm4vw7z_Bg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}